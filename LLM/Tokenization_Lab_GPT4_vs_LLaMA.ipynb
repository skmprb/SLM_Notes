{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Practical Lab: GPT-4 vs LLaMA Tokenization Deep Dive\n",
    "\n",
    "## üéØ Objective\n",
    "Compare GPT-4 (cl100k_base) vs LLaMA (SentencePiece/BPE) tokenizers focusing on **Token Count** and **Cost Analysis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tiktoken transformers matplotlib pandas seaborn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tiktoken  # GPT-4 tokenizer\n",
    "from transformers import AutoTokenizer  # LLaMA tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Initialize Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4 tokenizer (cl100k_base)\n",
    "print(\"Loading GPT-4 tokenizer...\")\n",
    "gpt4_tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(f\"GPT-4 vocab size: {gpt4_tokenizer.n_vocab:,}\")\n",
    "\n",
    "# LLaMA tokenizer (using Llama-2 as example)\n",
    "print(\"\\nLoading LLaMA tokenizer...\")\n",
    "try:\n",
    "    llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "except:\n",
    "    # Fallback to a similar tokenizer if LLaMA not available\n",
    "    llama_tokenizer = AutoTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n",
    "    \n",
    "print(f\"LLaMA vocab size: {len(llama_tokenizer.get_vocab()):,}\")\n",
    "print(\"\\n‚úÖ Tokenizers loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Test Cases Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test cases\n",
    "test_texts = {\n",
    "    \"english_simple\": \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"english_complex\": \"Artificial intelligence and machine learning algorithms are revolutionizing computational linguistics through advanced neural network architectures.\",\n",
    "    \"multilingual\": \"Hello world! Bonjour le monde! Hola mundo! „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïåÔºÅ ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ÿßŸÑÿπÿßŸÑŸÖ!\",\n",
    "    \"code_mixed\": \"def tokenize_text(input_str): return tokenizer.encode(input_str)\",\n",
    "    \"technical\": \"The transformer architecture utilizes self-attention mechanisms with multi-head attention layers for sequence-to-sequence modeling.\",\n",
    "    \"numbers_symbols\": \"Price: $1,234.56 | Date: 2024-01-15 | Email: user@example.com | Phone: +1-555-123-4567\",\n",
    "    \"long_text\": \"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of understanding the contents of documents, including the contextual nuances of the language within them.\"\n",
    "}\n",
    "\n",
    "print(f\"üìä Created {len(test_texts)} test cases:\")\n",
    "for name, text in test_texts.items():\n",
    "    print(f\"  ‚Ä¢ {name}: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tokenization(text: str, text_name: str) -> Dict:\n",
    "    \"\"\"Analyze tokenization for both GPT-4 and LLaMA tokenizers\"\"\"\n",
    "    \n",
    "    # GPT-4 tokenization\n",
    "    gpt4_tokens = gpt4_tokenizer.encode(text)\n",
    "    gpt4_count = len(gpt4_tokens)\n",
    "    gpt4_decoded = [gpt4_tokenizer.decode([token]) for token in gpt4_tokens]\n",
    "    \n",
    "    # LLaMA tokenization\n",
    "    llama_tokens = llama_tokenizer.encode(text, add_special_tokens=False)\n",
    "    llama_count = len(llama_tokens)\n",
    "    llama_decoded = llama_tokenizer.convert_ids_to_tokens(llama_tokens)\n",
    "    \n",
    "    # Cost calculation (example rates - adjust based on actual pricing)\n",
    "    gpt4_cost_per_1k = 0.03  # $0.03 per 1K tokens\n",
    "    llama_cost_per_1k = 0.01  # $0.01 per 1K tokens (hypothetical)\n",
    "    \n",
    "    gpt4_cost = (gpt4_count / 1000) * gpt4_cost_per_1k\n",
    "    llama_cost = (llama_count / 1000) * llama_cost_per_1k\n",
    "    \n",
    "    return {\n",
    "        \"text_name\": text_name,\n",
    "        \"text\": text,\n",
    "        \"char_count\": len(text),\n",
    "        \"gpt4_tokens\": gpt4_count,\n",
    "        \"llama_tokens\": llama_count,\n",
    "        \"gpt4_decoded\": gpt4_decoded,\n",
    "        \"llama_decoded\": llama_decoded,\n",
    "        \"gpt4_cost\": gpt4_cost,\n",
    "        \"llama_cost\": llama_cost,\n",
    "        \"token_ratio\": gpt4_count / llama_count if llama_count > 0 else 0,\n",
    "        \"cost_ratio\": gpt4_cost / llama_cost if llama_cost > 0 else 0,\n",
    "        \"gpt4_chars_per_token\": len(text) / gpt4_count if gpt4_count > 0 else 0,\n",
    "        \"llama_chars_per_token\": len(text) / llama_count if llama_count > 0 else 0\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Analysis function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis on all test cases\n",
    "print(\"üîÑ Running tokenization analysis...\\n\")\n",
    "\n",
    "results = []\n",
    "for name, text in test_texts.items():\n",
    "    result = analyze_tokenization(text, name)\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"üìù {name}:\")\n",
    "    print(f\"   Characters: {result['char_count']}\")\n",
    "    print(f\"   GPT-4 tokens: {result['gpt4_tokens']} (${result['gpt4_cost']:.6f})\")\n",
    "    print(f\"   LLaMA tokens: {result['llama_tokens']} (${result['llama_cost']:.6f})\")\n",
    "    print(f\"   Token ratio (GPT-4/LLaMA): {result['token_ratio']:.2f}\")\n",
    "    print(f\"   Cost ratio (GPT-4/LLaMA): {result['cost_ratio']:.2f}\")\n",
    "    print()\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "print(\"‚úÖ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_df = df[['text_name', 'char_count', 'gpt4_tokens', 'llama_tokens', \n",
    "                 'token_ratio', 'cost_ratio', 'gpt4_chars_per_token', 'llama_chars_per_token']].copy()\n",
    "\n",
    "summary_df.columns = ['Text Type', 'Characters', 'GPT-4 Tokens', 'LLaMA Tokens', \n",
    "                      'Token Ratio', 'Cost Ratio', 'GPT-4 Chars/Token', 'LLaMA Chars/Token']\n",
    "\n",
    "# Round numerical columns\n",
    "summary_df = summary_df.round(2)\n",
    "\n",
    "print(\"üìä TOKENIZATION COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualization: Token Count Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token count comparison chart\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(df))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, df['gpt4_tokens'], width, label='GPT-4 (cl100k_base)', alpha=0.8, color='#FF6B6B')\n",
    "plt.bar(x + width/2, df['llama_tokens'], width, label='LLaMA (SentencePiece)', alpha=0.8, color='#4ECDC4')\n",
    "\n",
    "plt.xlabel('Text Types', fontsize=12)\n",
    "plt.ylabel('Token Count', fontsize=12)\n",
    "plt.title('Token Count Comparison: GPT-4 vs LLaMA', fontsize=14, fontweight='bold')\n",
    "plt.xticks(x, df['text_name'], rotation=45, ha='right')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (gpt4, llama) in enumerate(zip(df['gpt4_tokens'], df['llama_tokens'])):\n",
    "    plt.text(i - width/2, gpt4 + 0.5, str(gpt4), ha='center', va='bottom', fontsize=9)\n",
    "    plt.text(i + width/2, llama + 0.5, str(llama), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí∞ Cost Analysis Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost comparison visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Cost comparison bar chart\n",
    "x = np.arange(len(df))\n",
    "ax1.bar(x - width/2, df['gpt4_cost'] * 1000, width, label='GPT-4', alpha=0.8, color='#FF6B6B')\n",
    "ax1.bar(x + width/2, df['llama_cost'] * 1000, width, label='LLaMA', alpha=0.8, color='#4ECDC4')\n",
    "ax1.set_xlabel('Text Types')\n",
    "ax1.set_ylabel('Cost ($ per 1000 chars)')\n",
    "ax1.set_title('Cost Comparison by Text Type')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(df['text_name'], rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency scatter plot\n",
    "ax2.scatter(df['gpt4_tokens'], df['gpt4_cost'] * 1000, label='GPT-4', alpha=0.7, s=100, color='#FF6B6B')\n",
    "ax2.scatter(df['llama_tokens'], df['llama_cost'] * 1000, label='LLaMA', alpha=0.7, s=100, color='#4ECDC4')\n",
    "ax2.set_xlabel('Token Count')\n",
    "ax2.set_ylabel('Cost ($ per 1000 chars)')\n",
    "ax2.set_title('Token Count vs Cost Efficiency')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall efficiency metrics\n",
    "total_chars = df['char_count'].sum()\n",
    "total_gpt4_tokens = df['gpt4_tokens'].sum()\n",
    "total_llama_tokens = df['llama_tokens'].sum()\n",
    "total_gpt4_cost = df['gpt4_cost'].sum()\n",
    "total_llama_cost = df['llama_cost'].sum()\n",
    "\n",
    "print(\"üéØ OVERALL EFFICIENCY METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total characters processed: {total_chars:,}\")\n",
    "print()\n",
    "print(\"üìä TOKEN EFFICIENCY:\")\n",
    "print(f\"  GPT-4 average: {total_chars/total_gpt4_tokens:.2f} chars/token\")\n",
    "print(f\"  LLaMA average: {total_chars/total_llama_tokens:.2f} chars/token\")\n",
    "print(f\"  GPT-4 is {((total_chars/total_gpt4_tokens)/(total_chars/total_llama_tokens)-1)*100:.1f}% more efficient\")\n",
    "print()\n",
    "print(\"üí∞ COST ANALYSIS:\")\n",
    "print(f\"  GPT-4 total cost: ${total_gpt4_cost:.6f}\")\n",
    "print(f\"  LLaMA total cost: ${total_llama_cost:.6f}\")\n",
    "print(f\"  GPT-4 costs {(total_gpt4_cost/total_llama_cost):.1f}x more than LLaMA\")\n",
    "print(f\"  Cost difference: ${(total_gpt4_cost-total_llama_cost):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Deep Dive: Token Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect tokenization for a specific example\n",
    "example_text = \"The transformer architecture utilizes self-attention mechanisms.\"\n",
    "\n",
    "gpt4_tokens = gpt4_tokenizer.encode(example_text)\n",
    "gpt4_decoded = [gpt4_tokenizer.decode([token]) for token in gpt4_tokens]\n",
    "\n",
    "llama_tokens = llama_tokenizer.encode(example_text, add_special_tokens=False)\n",
    "llama_decoded = llama_tokenizer.convert_ids_to_tokens(llama_tokens)\n",
    "\n",
    "print(\"üîç TOKEN BREAKDOWN ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Text: '{example_text}'\")\n",
    "print(f\"Length: {len(example_text)} characters\")\n",
    "print()\n",
    "print(\"GPT-4 Tokenization:\")\n",
    "for i, (token_id, token_str) in enumerate(zip(gpt4_tokens, gpt4_decoded)):\n",
    "    print(f\"  {i+1:2d}. ID:{token_id:5d} ‚Üí '{token_str}'\")\n",
    "print(f\"Total GPT-4 tokens: {len(gpt4_tokens)}\")\n",
    "print()\n",
    "print(\"LLaMA Tokenization:\")\n",
    "for i, (token_id, token_str) in enumerate(zip(llama_tokens, llama_decoded)):\n",
    "    print(f\"  {i+1:2d}. ID:{token_id:5d} ‚Üí '{token_str}'\")\n",
    "print(f\"Total LLaMA tokens: {len(llama_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç Character Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different character types\n",
    "char_test_cases = {\n",
    "    'ascii_basic': 'Hello world 123',\n",
    "    'unicode_accents': 'caf√© na√Øve r√©sum√©',\n",
    "    'symbols': '!@#$%^&*()_+-=[]{}|;:,.<>?',\n",
    "    'mixed_unicode': 'Hello ‰∏ñÁïå! Price: $1,234.56',\n",
    "    'code_syntax': 'function(x) { return x * 2; }',\n",
    "    'emojis': 'üöÄ Hello! üòä How are you? üåü'\n",
    "}\n",
    "\n",
    "print(\"üåç CHARACTER TYPE EFFICIENCY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "char_results = []\n",
    "for char_type, text in char_test_cases.items():\n",
    "    gpt4_tokens = len(gpt4_tokenizer.encode(text))\n",
    "    llama_tokens = len(llama_tokenizer.encode(text, add_special_tokens=False))\n",
    "    \n",
    "    char_results.append({\n",
    "        'type': char_type,\n",
    "        'text': text,\n",
    "        'chars': len(text),\n",
    "        'gpt4_tokens': gpt4_tokens,\n",
    "        'llama_tokens': llama_tokens,\n",
    "        'gpt4_efficiency': len(text) / gpt4_tokens,\n",
    "        'llama_efficiency': len(text) / llama_tokens\n",
    "    })\n",
    "    \n",
    "    print(f\"{char_type:15s}: '{text[:30]}{'...' if len(text) > 30 else ''}'\")\n",
    "    print(f\"                 GPT-4: {gpt4_tokens:2d} tokens ({len(text)/gpt4_tokens:.1f} chars/token)\")\n",
    "    print(f\"                 LLaMA: {llama_tokens:2d} tokens ({len(text)/llama_tokens:.1f} chars/token)\")\n",
    "    print()\n",
    "\n",
    "char_df = pd.DataFrame(char_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Character Type Efficiency Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize character type efficiency\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(char_df))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, char_df['gpt4_efficiency'], width, label='GPT-4', alpha=0.8, color='#FF6B6B')\n",
    "plt.bar(x + width/2, char_df['llama_efficiency'], width, label='LLaMA', alpha=0.8, color='#4ECDC4')\n",
    "\n",
    "plt.xlabel('Character Types')\n",
    "plt.ylabel('Characters per Token')\n",
    "plt.title('Tokenization Efficiency by Character Type')\n",
    "plt.xticks(x, char_df['type'], rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (gpt4_eff, llama_eff) in enumerate(zip(char_df['gpt4_efficiency'], char_df['llama_efficiency'])):\n",
    "    plt.text(i - width/2, gpt4_eff + 0.05, f'{gpt4_eff:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "    plt.text(i + width/2, llama_eff + 0.05, f'{llama_eff:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Cost Optimization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_cost_choice(text: str, gpt4_price_per_1k: float = 0.03, llama_price_per_1k: float = 0.01) -> Dict:\n",
    "    \"\"\"Determine which tokenizer is more cost-effective for given text\"\"\"\n",
    "    \n",
    "    gpt4_tokens = len(gpt4_tokenizer.encode(text))\n",
    "    llama_tokens = len(llama_tokenizer.encode(text, add_special_tokens=False))\n",
    "    \n",
    "    gpt4_cost = (gpt4_tokens / 1000) * gpt4_price_per_1k\n",
    "    llama_cost = (llama_tokens / 1000) * llama_price_per_1k\n",
    "    \n",
    "    savings = abs(gpt4_cost - llama_cost)\n",
    "    savings_percent = (savings / max(gpt4_cost, llama_cost)) * 100\n",
    "    \n",
    "    recommendation = \"LLaMA\" if llama_cost < gpt4_cost else \"GPT-4\"\n",
    "    \n",
    "    return {\n",
    "        \"text_length\": len(text),\n",
    "        \"gpt4_tokens\": gpt4_tokens,\n",
    "        \"llama_tokens\": llama_tokens,\n",
    "        \"gpt4_cost\": gpt4_cost,\n",
    "        \"llama_cost\": llama_cost,\n",
    "        \"recommendation\": recommendation,\n",
    "        \"savings\": savings,\n",
    "        \"savings_percent\": savings_percent\n",
    "    }\n",
    "\n",
    "# Test the optimization function\n",
    "test_optimization = \"Write a Python function that implements a binary search algorithm with proper error handling and documentation.\"\n",
    "\n",
    "result = optimize_cost_choice(test_optimization)\n",
    "\n",
    "print(\"üí° COST OPTIMIZATION RECOMMENDATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Text: '{test_optimization}'\")\n",
    "print(f\"Length: {result['text_length']} characters\")\n",
    "print()\n",
    "print(f\"GPT-4: {result['gpt4_tokens']} tokens ‚Üí ${result['gpt4_cost']:.6f}\")\n",
    "print(f\"LLaMA: {result['llama_tokens']} tokens ‚Üí ${result['llama_cost']:.6f}\")\n",
    "print()\n",
    "print(f\"üí∞ Recommendation: Use {result['recommendation']}\")\n",
    "print(f\"üíµ Potential savings: ${result['savings']:.6f} ({result['savings_percent']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Interactive Testing Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing - modify this cell to test your own text\n",
    "YOUR_TEST_TEXT = \"Enter your own text here to see how different tokenizers handle it!\"\n",
    "\n",
    "# Analyze your text\n",
    "your_result = analyze_tokenization(YOUR_TEST_TEXT, \"your_test\")\n",
    "\n",
    "print(\"üß™ YOUR TEXT ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Text: '{YOUR_TEST_TEXT}'\")\n",
    "print(f\"Characters: {your_result['char_count']}\")\n",
    "print()\n",
    "print(f\"GPT-4 tokenization:\")\n",
    "print(f\"  Tokens: {your_result['gpt4_tokens']}\")\n",
    "print(f\"  Cost: ${your_result['gpt4_cost']:.6f}\")\n",
    "print(f\"  Efficiency: {your_result['gpt4_chars_per_token']:.2f} chars/token\")\n",
    "print()\n",
    "print(f\"LLaMA tokenization:\")\n",
    "print(f\"  Tokens: {your_result['llama_tokens']}\")\n",
    "print(f\"  Cost: ${your_result['llama_cost']:.6f}\")\n",
    "print(f\"  Efficiency: {your_result['llama_chars_per_token']:.2f} chars/token\")\n",
    "print()\n",
    "print(f\"Token ratio (GPT-4/LLaMA): {your_result['token_ratio']:.2f}\")\n",
    "print(f\"Cost ratio (GPT-4/LLaMA): {your_result['cost_ratio']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ KEY FINDINGS & RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"üìä TOKENIZATION EFFICIENCY:\")\n",
    "print(f\"  ‚Ä¢ GPT-4 (cl100k_base): {total_chars/total_gpt4_tokens:.2f} chars/token average\")\n",
    "print(f\"  ‚Ä¢ LLaMA (SentencePiece): {total_chars/total_llama_tokens:.2f} chars/token average\")\n",
    "print(f\"  ‚Ä¢ GPT-4 is ~{((total_chars/total_gpt4_tokens)/(total_chars/total_llama_tokens)-1)*100:.0f}% more token-efficient\")\n",
    "print()\n",
    "print(\"üí∞ COST IMPLICATIONS:\")\n",
    "print(f\"  ‚Ä¢ GPT-4 typically costs {(total_gpt4_cost/total_llama_cost):.1f}x more per text\")\n",
    "print(f\"  ‚Ä¢ Higher per-token pricing + larger vocabulary = higher costs\")\n",
    "print(f\"  ‚Ä¢ Token efficiency doesn't always translate to cost savings\")\n",
    "print()\n",
    "print(\"üéØ WHEN TO USE EACH:\")\n",
    "print(\"  GPT-4 Tokenizer:\")\n",
    "print(\"    ‚úì Code-heavy applications\")\n",
    "print(\"    ‚úì Technical/scientific text\")\n",
    "print(\"    ‚úì Mixed Unicode content\")\n",
    "print(\"    ‚úì Quality over cost scenarios\")\n",
    "print()\n",
    "print(\"  LLaMA Tokenizer:\")\n",
    "print(\"    ‚úì Cost-sensitive applications\")\n",
    "print(\"    ‚úì Consistent multilingual text\")\n",
    "print(\"    ‚úì Memory-constrained environments\")\n",
    "print(\"    ‚úì Research/academic use\")\n",
    "print()\n",
    "print(\"üîç WHY SAME TEXT COSTS DIFFERENT:\")\n",
    "print(\"  1. Different vocabulary sizes (100K vs 32K tokens)\")\n",
    "print(\"  2. Different tokenization algorithms (BPE variants)\")\n",
    "print(\"  3. Different training data and optimization goals\")\n",
    "print(\"  4. Different pricing models per token\")\n",
    "print(\"  5. Different handling of special characters and Unicode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps & Exercises\n",
    "\n",
    "### Try These Experiments:\n",
    "1. **Test with your domain-specific text** (legal, medical, technical)\n",
    "2. **Compare with other tokenizers** (BERT, T5, etc.)\n",
    "3. **Analyze different languages** (Chinese, Arabic, etc.)\n",
    "4. **Test with code in different programming languages**\n",
    "5. **Measure actual inference speed differences**\n",
    "\n",
    "### Questions to Explore:\n",
    "- How do tokenizers handle out-of-vocabulary words?\n",
    "- What's the impact on model performance vs efficiency?\n",
    "- How do different tokenizers affect multilingual capabilities?\n",
    "- What are the memory implications during training vs inference?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
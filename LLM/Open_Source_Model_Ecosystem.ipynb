{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open-Source Model Ecosystem Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "The open-source LLM ecosystem is rapidly evolving with diverse models, licensing schemes, and deployment strategies. This notebook covers:\n",
    "\n",
    "- **Model Checkpoints**: Management, versioning, and distribution systems\n",
    "- **License Analysis**: Compatibility checking and compliance frameworks\n",
    "- **Ecosystem Metrics**: Performance tracking and adoption analysis\n",
    "- **Model Selection**: Decision frameworks for choosing appropriate models\n",
    "\n",
    "Let's build practical tools for navigating the open-source model landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import hashlib\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import re\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Checkpoint Management System\n",
    "\n",
    "Let's implement a comprehensive system for managing model checkpoints with versioning and metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LicenseType(Enum):\n",
    "    MIT = \"MIT\"\n",
    "    APACHE_2 = \"Apache-2.0\"\n",
    "    GPL_3 = \"GPL-3.0\"\n",
    "    BSD_3 = \"BSD-3-Clause\"\n",
    "    CC_BY_4 = \"CC-BY-4.0\"\n",
    "    CC_BY_SA_4 = \"CC-BY-SA-4.0\"\n",
    "    LLAMA_2 = \"Llama-2-Custom\"\n",
    "    OPENRAIL = \"OpenRAIL-M\"\n",
    "    PROPRIETARY = \"Proprietary\"\n",
    "\n",
    "@dataclass\n",
    "class ModelCheckpoint:\n",
    "    model_id: str\n",
    "    name: str\n",
    "    version: str\n",
    "    organization: str\n",
    "    license_type: LicenseType\n",
    "    parameter_count: int\n",
    "    model_size_gb: float\n",
    "    architecture: str\n",
    "    training_data: str\n",
    "    release_date: datetime\n",
    "    download_url: str = \"\"\n",
    "    paper_url: str = \"\"\n",
    "    code_url: str = \"\"\n",
    "    performance_metrics: Dict[str, float] = field(default_factory=dict)\n",
    "    tags: List[str] = field(default_factory=list)\n",
    "    download_count: int = 0\n",
    "    rating: float = 0.0\n",
    "    commercial_use: bool = True\n",
    "    modification_allowed: bool = True\n",
    "    redistribution_allowed: bool = True\n",
    "\n",
    "class ModelRegistry:\n",
    "    \"\"\"Comprehensive model registry with search and analysis capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.license_compatibility = self._init_license_compatibility()\n",
    "        self.search_index = defaultdict(set)\n",
    "        self.analytics = {\n",
    "            'total_models': 0,\n",
    "            'total_downloads': 0,\n",
    "            'license_distribution': Counter(),\n",
    "            'organization_distribution': Counter(),\n",
    "            'architecture_distribution': Counter()\n",
    "        }\n",
    "        self._populate_sample_models()\n",
    "    \n",
    "    def _init_license_compatibility(self):\n",
    "        \"\"\"Initialize license compatibility matrix\"\"\"\n",
    "        # Simplified compatibility matrix (True = compatible)\n",
    "        compatibility = {\n",
    "            (LicenseType.MIT, LicenseType.MIT): True,\n",
    "            (LicenseType.MIT, LicenseType.APACHE_2): True,\n",
    "            (LicenseType.MIT, LicenseType.BSD_3): True,\n",
    "            (LicenseType.APACHE_2, LicenseType.APACHE_2): True,\n",
    "            (LicenseType.APACHE_2, LicenseType.MIT): True,\n",
    "            (LicenseType.GPL_3, LicenseType.GPL_3): True,\n",
    "            (LicenseType.GPL_3, LicenseType.MIT): False,  # GPL is more restrictive\n",
    "            (LicenseType.CC_BY_4, LicenseType.CC_BY_4): True,\n",
    "            (LicenseType.CC_BY_SA_4, LicenseType.CC_BY_SA_4): True,\n",
    "            (LicenseType.LLAMA_2, LicenseType.LLAMA_2): True,\n",
    "            (LicenseType.PROPRIETARY, LicenseType.PROPRIETARY): False\n",
    "        }\n",
    "        return compatibility\n",
    "    \n",
    "    def _populate_sample_models(self):\n",
    "        \"\"\"Populate registry with sample models\"\"\"\n",
    "        sample_models = [\n",
    "            ModelCheckpoint(\n",
    "                model_id=\"llama-2-7b\",\n",
    "                name=\"Llama 2 7B\",\n",
    "                version=\"1.0\",\n",
    "                organization=\"Meta\",\n",
    "                license_type=LicenseType.LLAMA_2,\n",
    "                parameter_count=7_000_000_000,\n",
    "                model_size_gb=13.5,\n",
    "                architecture=\"Transformer\",\n",
    "                training_data=\"Custom dataset (2T tokens)\",\n",
    "                release_date=datetime(2023, 7, 18),\n",
    "                performance_metrics={\"hellaswag\": 0.776, \"mmlu\": 0.459, \"truthfulqa\": 0.389},\n",
    "                tags=[\"chat\", \"instruct\", \"general\"],\n",
    "                download_count=150000,\n",
    "                rating=4.2,\n",
    "                commercial_use=True\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                model_id=\"mistral-7b\",\n",
    "                name=\"Mistral 7B\",\n",
    "                version=\"0.1\",\n",
    "                organization=\"Mistral AI\",\n",
    "                license_type=LicenseType.APACHE_2,\n",
    "                parameter_count=7_300_000_000,\n",
    "                model_size_gb=14.2,\n",
    "                architecture=\"Transformer\",\n",
    "                training_data=\"Web crawl + curated datasets\",\n",
    "                release_date=datetime(2023, 9, 27),\n",
    "                performance_metrics={\"hellaswag\": 0.813, \"mmlu\": 0.624, \"truthfulqa\": 0.425},\n",
    "                tags=[\"general\", \"efficient\", \"open\"],\n",
    "                download_count=89000,\n",
    "                rating=4.5,\n",
    "                commercial_use=True\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                model_id=\"falcon-7b\",\n",
    "                name=\"Falcon 7B\",\n",
    "                version=\"1.0\",\n",
    "                organization=\"TII\",\n",
    "                license_type=LicenseType.APACHE_2,\n",
    "                parameter_count=7_000_000_000,\n",
    "                model_size_gb=14.0,\n",
    "                architecture=\"Transformer\",\n",
    "                training_data=\"RefinedWeb (1.5T tokens)\",\n",
    "                release_date=datetime(2023, 6, 5),\n",
    "                performance_metrics={\"hellaswag\": 0.743, \"mmlu\": 0.353, \"truthfulqa\": 0.344},\n",
    "                tags=[\"general\", \"multilingual\"],\n",
    "                download_count=67000,\n",
    "                rating=3.9,\n",
    "                commercial_use=True\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                model_id=\"vicuna-7b\",\n",
    "                name=\"Vicuna 7B\",\n",
    "                version=\"1.5\",\n",
    "                organization=\"LMSYS\",\n",
    "                license_type=LicenseType.LLAMA_2,\n",
    "                parameter_count=7_000_000_000,\n",
    "                model_size_gb=13.5,\n",
    "                architecture=\"Transformer\",\n",
    "                training_data=\"ShareGPT conversations\",\n",
    "                release_date=datetime(2023, 7, 20),\n",
    "                performance_metrics={\"hellaswag\": 0.766, \"mmlu\": 0.471, \"truthfulqa\": 0.392},\n",
    "                tags=[\"chat\", \"conversation\", \"fine-tuned\"],\n",
    "                download_count=45000,\n",
    "                rating=4.1,\n",
    "                commercial_use=False  # Due to training data restrictions\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                model_id=\"code-llama-7b\",\n",
    "                name=\"Code Llama 7B\",\n",
    "                version=\"1.0\",\n",
    "                organization=\"Meta\",\n",
    "                license_type=LicenseType.LLAMA_2,\n",
    "                parameter_count=7_000_000_000,\n",
    "                model_size_gb=13.5,\n",
    "                architecture=\"Transformer\",\n",
    "                training_data=\"Code datasets + Llama 2\",\n",
    "                release_date=datetime(2023, 8, 24),\n",
    "                performance_metrics={\"humaneval\": 0.299, \"mbpp\": 0.374},\n",
    "                tags=[\"code\", \"programming\", \"specialized\"],\n",
    "                download_count=78000,\n",
    "                rating=4.3,\n",
    "                commercial_use=True\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        for model in sample_models:\n",
    "            self.register_model(model)\n",
    "    \n",
    "    def register_model(self, model: ModelCheckpoint):\n",
    "        \"\"\"Register a new model in the registry\"\"\"\n",
    "        self.models[model.model_id] = model\n",
    "        \n",
    "        # Update search index\n",
    "        self._update_search_index(model)\n",
    "        \n",
    "        # Update analytics\n",
    "        self._update_analytics(model)\n",
    "    \n",
    "    def _update_search_index(self, model: ModelCheckpoint):\n",
    "        \"\"\"Update search index for efficient querying\"\"\"\n",
    "        # Index by various attributes\n",
    "        self.search_index['license'][model.license_type].add(model.model_id)\n",
    "        self.search_index['organization'][model.organization.lower()].add(model.model_id)\n",
    "        self.search_index['architecture'][model.architecture.lower()].add(model.model_id)\n",
    "        \n",
    "        # Index by tags\n",
    "        for tag in model.tags:\n",
    "            self.search_index['tag'][tag.lower()].add(model.model_id)\n",
    "        \n",
    "        # Index by parameter size ranges\n",
    "        param_range = self._get_parameter_range(model.parameter_count)\n",
    "        self.search_index['param_range'][param_range].add(model.model_id)\n",
    "    \n",
    "    def _update_analytics(self, model: ModelCheckpoint):\n",
    "        \"\"\"Update registry analytics\"\"\"\n",
    "        self.analytics['total_models'] += 1\n",
    "        self.analytics['total_downloads'] += model.download_count\n",
    "        self.analytics['license_distribution'][model.license_type] += 1\n",
    "        self.analytics['organization_distribution'][model.organization] += 1\n",
    "        self.analytics['architecture_distribution'][model.architecture] += 1\n",
    "    \n",
    "    def _get_parameter_range(self, param_count):\n",
    "        \"\"\"Categorize parameter count into ranges\"\"\"\n",
    "        if param_count < 1e9:\n",
    "            return \"<1B\"\n",
    "        elif param_count < 10e9:\n",
    "            return \"1B-10B\"\n",
    "        elif param_count < 100e9:\n",
    "            return \"10B-100B\"\n",
    "        else:\n",
    "            return \">100B\"\n",
    "    \n",
    "    def search_models(self, query_params: Dict[str, Any]) -> List[ModelCheckpoint]:\n",
    "        \"\"\"Search models based on various criteria\"\"\"\n",
    "        candidate_ids = set(self.models.keys())\n",
    "        \n",
    "        # Filter by license\n",
    "        if 'license' in query_params:\n",
    "            license_type = query_params['license']\n",
    "            if isinstance(license_type, str):\n",
    "                license_type = LicenseType(license_type)\n",
    "            candidate_ids &= self.search_index['license'][license_type]\n",
    "        \n",
    "        # Filter by organization\n",
    "        if 'organization' in query_params:\n",
    "            org = query_params['organization'].lower()\n",
    "            candidate_ids &= self.search_index['organization'][org]\n",
    "        \n",
    "        # Filter by tags\n",
    "        if 'tags' in query_params:\n",
    "            for tag in query_params['tags']:\n",
    "                candidate_ids &= self.search_index['tag'][tag.lower()]\n",
    "        \n",
    "        # Filter by parameter range\n",
    "        if 'param_range' in query_params:\n",
    "            param_range = query_params['param_range']\n",
    "            candidate_ids &= self.search_index['param_range'][param_range]\n",
    "        \n",
    "        # Filter by commercial use\n",
    "        if 'commercial_use' in query_params:\n",
    "            commercial_required = query_params['commercial_use']\n",
    "            candidate_ids = {\n",
    "                mid for mid in candidate_ids \n",
    "                if self.models[mid].commercial_use >= commercial_required\n",
    "            }\n",
    "        \n",
    "        # Filter by minimum rating\n",
    "        if 'min_rating' in query_params:\n",
    "            min_rating = query_params['min_rating']\n",
    "            candidate_ids = {\n",
    "                mid for mid in candidate_ids \n",
    "                if self.models[mid].rating >= min_rating\n",
    "            }\n",
    "        \n",
    "        # Return matching models\n",
    "        results = [self.models[mid] for mid in candidate_ids]\n",
    "        \n",
    "        # Sort by relevance (download count * rating)\n",
    "        results.sort(key=lambda m: m.download_count * m.rating, reverse=True)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def check_license_compatibility(self, model_ids: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Check license compatibility between multiple models\"\"\"\n",
    "        models = [self.models[mid] for mid in model_ids if mid in self.models]\n",
    "        \n",
    "        if len(models) < 2:\n",
    "            return {'compatible': True, 'issues': []}\n",
    "        \n",
    "        compatibility_issues = []\n",
    "        \n",
    "        # Check pairwise compatibility\n",
    "        for i, model1 in enumerate(models):\n",
    "            for model2 in models[i+1:]:\n",
    "                license_pair = (model1.license_type, model2.license_type)\n",
    "                reverse_pair = (model2.license_type, model1.license_type)\n",
    "                \n",
    "                compatible = (\n",
    "                    self.license_compatibility.get(license_pair, False) or\n",
    "                    self.license_compatibility.get(reverse_pair, False)\n",
    "                )\n",
    "                \n",
    "                if not compatible:\n",
    "                    compatibility_issues.append({\n",
    "                        'model1': model1.name,\n",
    "                        'model2': model2.name,\n",
    "                        'license1': model1.license_type.value,\n",
    "                        'license2': model2.license_type.value,\n",
    "                        'issue': 'License incompatibility'\n",
    "                    })\n",
    "        \n",
    "        # Check commercial use restrictions\n",
    "        commercial_restricted = [m for m in models if not m.commercial_use]\n",
    "        if commercial_restricted:\n",
    "            compatibility_issues.append({\n",
    "                'models': [m.name for m in commercial_restricted],\n",
    "                'issue': 'Commercial use restricted'\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'compatible': len(compatibility_issues) == 0,\n",
    "            'issues': compatibility_issues,\n",
    "            'models_checked': [m.name for m in models]\n",
    "        }\n",
    "    \n",
    "    def get_model_recommendations(self, requirements: Dict[str, Any]) -> List[Tuple[ModelCheckpoint, float]]:\n",
    "        \"\"\"Get model recommendations based on requirements\"\"\"\n",
    "        all_models = list(self.models.values())\n",
    "        recommendations = []\n",
    "        \n",
    "        for model in all_models:\n",
    "            score = self._calculate_recommendation_score(model, requirements)\n",
    "            if score > 0:\n",
    "                recommendations.append((model, score))\n",
    "        \n",
    "        # Sort by score\n",
    "        recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return recommendations[:10]  # Top 10 recommendations\n",
    "    \n",
    "    def _calculate_recommendation_score(self, model: ModelCheckpoint, requirements: Dict[str, Any]) -> float:\n",
    "        \"\"\"Calculate recommendation score for a model\"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # Base score from rating and popularity\n",
    "        score += model.rating * 0.3\n",
    "        score += min(model.download_count / 100000, 1.0) * 0.2\n",
    "        \n",
    "        # License compatibility\n",
    "        if 'license_preference' in requirements:\n",
    "            preferred_license = requirements['license_preference']\n",
    "            if model.license_type == preferred_license:\n",
    "                score += 0.3\n",
    "            elif self.license_compatibility.get((model.license_type, preferred_license), False):\n",
    "                score += 0.15\n",
    "        \n",
    "        # Commercial use requirement\n",
    "        if requirements.get('commercial_use', False) and not model.commercial_use:\n",
    "            return 0.0  # Disqualify if commercial use required but not allowed\n",
    "        \n",
    "        # Task-specific requirements\n",
    "        if 'task_type' in requirements:\n",
    "            task_type = requirements['task_type'].lower()\n",
    "            if task_type in [tag.lower() for tag in model.tags]:\n",
    "                score += 0.4\n",
    "        \n",
    "        # Performance requirements\n",
    "        if 'min_performance' in requirements:\n",
    "            benchmark = requirements.get('benchmark', 'mmlu')\n",
    "            min_perf = requirements['min_performance']\n",
    "            \n",
    "            if benchmark in model.performance_metrics:\n",
    "                actual_perf = model.performance_metrics[benchmark]\n",
    "                if actual_perf >= min_perf:\n",
    "                    score += 0.3 * (actual_perf - min_perf)\n",
    "                else:\n",
    "                    score *= 0.5  # Penalize for not meeting requirements\n",
    "        \n",
    "        # Size constraints\n",
    "        if 'max_size_gb' in requirements:\n",
    "            max_size = requirements['max_size_gb']\n",
    "            if model.model_size_gb <= max_size:\n",
    "                score += 0.1\n",
    "            else:\n",
    "                score *= 0.7  # Penalize for being too large\n",
    "        \n",
    "        return max(0.0, min(5.0, score))  # Clamp between 0 and 5\n",
    "    \n",
    "    def get_ecosystem_analytics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive ecosystem analytics\"\"\"\n",
    "        models = list(self.models.values())\n",
    "        \n",
    "        analytics = {\n",
    "            'overview': self.analytics.copy(),\n",
    "            'performance_analysis': self._analyze_performance_trends(models),\n",
    "            'license_analysis': self._analyze_license_landscape(models),\n",
    "            'temporal_analysis': self._analyze_release_timeline(models),\n",
    "            'size_analysis': self._analyze_model_sizes(models)\n",
    "        }\n",
    "        \n",
    "        return analytics\n",
    "    \n",
    "    def _analyze_performance_trends(self, models: List[ModelCheckpoint]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze performance trends across models\"\"\"\n",
    "        benchmarks = set()\n",
    "        for model in models:\n",
    "            benchmarks.update(model.performance_metrics.keys())\n",
    "        \n",
    "        performance_data = {}\n",
    "        for benchmark in benchmarks:\n",
    "            scores = [m.performance_metrics.get(benchmark, 0) for m in models if benchmark in m.performance_metrics]\n",
    "            if scores:\n",
    "                performance_data[benchmark] = {\n",
    "                    'mean': np.mean(scores),\n",
    "                    'std': np.std(scores),\n",
    "                    'min': np.min(scores),\n",
    "                    'max': np.max(scores),\n",
    "                    'count': len(scores)\n",
    "                }\n",
    "        \n",
    "        return performance_data\n",
    "    \n",
    "    def _analyze_license_landscape(self, models: List[ModelCheckpoint]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze license distribution and implications\"\"\"\n",
    "        license_stats = Counter(m.license_type for m in models)\n",
    "        commercial_friendly = sum(1 for m in models if m.commercial_use)\n",
    "        \n",
    "        return {\n",
    "            'distribution': dict(license_stats),\n",
    "            'commercial_friendly_ratio': commercial_friendly / len(models),\n",
    "            'most_common_license': license_stats.most_common(1)[0] if license_stats else None\n",
    "        }\n",
    "    \n",
    "    def _analyze_release_timeline(self, models: List[ModelCheckpoint]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze model release timeline\"\"\"\n",
    "        release_dates = [m.release_date for m in models]\n",
    "        \n",
    "        if not release_dates:\n",
    "            return {}\n",
    "        \n",
    "        earliest = min(release_dates)\n",
    "        latest = max(release_dates)\n",
    "        \n",
    "        # Group by month\n",
    "        monthly_releases = defaultdict(int)\n",
    "        for date in release_dates:\n",
    "            month_key = f\"{date.year}-{date.month:02d}\"\n",
    "            monthly_releases[month_key] += 1\n",
    "        \n",
    "        return {\n",
    "            'earliest_release': earliest,\n",
    "            'latest_release': latest,\n",
    "            'total_timespan_days': (latest - earliest).days,\n",
    "            'monthly_distribution': dict(monthly_releases),\n",
    "            'release_velocity': len(models) / max(1, (latest - earliest).days / 30)  # Models per month\n",
    "        }\n",
    "    \n",
    "    def _analyze_model_sizes(self, models: List[ModelCheckpoint]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze model size distribution\"\"\"\n",
    "        param_counts = [m.parameter_count for m in models]\n",
    "        model_sizes = [m.model_size_gb for m in models]\n",
    "        \n",
    "        return {\n",
    "            'parameter_stats': {\n",
    "                'mean': np.mean(param_counts),\n",
    "                'median': np.median(param_counts),\n",
    "                'min': np.min(param_counts),\n",
    "                'max': np.max(param_counts)\n",
    "            },\n",
    "            'size_stats': {\n",
    "                'mean_gb': np.mean(model_sizes),\n",
    "                'median_gb': np.median(model_sizes),\n",
    "                'min_gb': np.min(model_sizes),\n",
    "                'max_gb': np.max(model_sizes)\n",
    "            },\n",
    "            'size_distribution': dict(Counter(self._get_parameter_range(pc) for pc in param_counts))\n",
    "        }\n",
    "\n",
    "# Initialize model registry\n",
    "model_registry = ModelRegistry()\n",
    "print(\"Model registry initialized with sample models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model Registry and Search\n",
    "\n",
    "Let's test our model registry with various search queries and analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test various search scenarios\n",
    "search_scenarios = [\n",
    "    {\n",
    "        'name': 'Commercial Use Models',\n",
    "        'query': {'commercial_use': True, 'min_rating': 4.0},\n",
    "        'description': 'Models suitable for commercial use with good ratings'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Apache Licensed Models',\n",
    "        'query': {'license': LicenseType.APACHE_2},\n",
    "        'description': 'Models with Apache 2.0 license'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Code-Focused Models',\n",
    "        'query': {'tags': ['code']},\n",
    "        'description': 'Models specialized for code generation'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Chat Models',\n",
    "        'query': {'tags': ['chat'], 'param_range': '1B-10B'},\n",
    "        'description': 'Chat models in the 1B-10B parameter range'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Meta Models',\n",
    "        'query': {'organization': 'Meta'},\n",
    "        'description': 'Models released by Meta'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Testing model search functionality...\\n\")\n",
    "\n",
    "search_results = []\n",
    "\n",
    "for scenario in search_scenarios:\n",
    "    print(f\"Search: {scenario['name']}\")\n",
    "    print(f\"Description: {scenario['description']}\")\n",
    "    print(f\"Query: {scenario['query']}\")\n",
    "    \n",
    "    results = model_registry.search_models(scenario['query'])\n",
    "    \n",
    "    print(f\"Found {len(results)} models:\")\n",
    "    for model in results:\n",
    "        print(f\"  - {model.name} ({model.organization}) - Rating: {model.rating}, Downloads: {model.download_count:,}\")\n",
    "    \n",
    "    search_results.append({\n",
    "        'scenario': scenario['name'],\n",
    "        'query_complexity': len(scenario['query']),\n",
    "        'results_count': len(results),\n",
    "        'avg_rating': np.mean([m.rating for m in results]) if results else 0\n",
    "    })\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Test license compatibility\n",
    "print(\"\\nTesting license compatibility...\")\n",
    "\n",
    "compatibility_tests = [\n",
    "    ['llama-2-7b', 'vicuna-7b'],  # Same license family\n",
    "    ['mistral-7b', 'falcon-7b'],  # Both Apache 2.0\n",
    "    ['llama-2-7b', 'mistral-7b']  # Different licenses\n",
    "]\n",
    "\n",
    "for test_models in compatibility_tests:\n",
    "    compatibility = model_registry.check_license_compatibility(test_models)\n",
    "    model_names = [model_registry.models[mid].name for mid in test_models if mid in model_registry.models]\n",
    "    \n",
    "    print(f\"\\nCompatibility check: {' + '.join(model_names)}\")\n",
    "    print(f\"Compatible: {compatibility['compatible']}\")\n",
    "    \n",
    "    if compatibility['issues']:\n",
    "        print(\"Issues found:\")\n",
    "        for issue in compatibility['issues']:\n",
    "            print(f\"  - {issue}\")\n",
    "\n",
    "# Test model recommendations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING MODEL RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "recommendation_scenarios = [\n",
    "    {\n",
    "        'name': 'Commercial Chat Application',\n",
    "        'requirements': {\n",
    "            'commercial_use': True,\n",
    "            'task_type': 'chat',\n",
    "            'min_performance': 0.4,\n",
    "            'benchmark': 'mmlu',\n",
    "            'max_size_gb': 15\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Open Source Code Assistant',\n",
    "        'requirements': {\n",
    "            'license_preference': LicenseType.APACHE_2,\n",
    "            'task_type': 'code',\n",
    "            'commercial_use': True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Research Project',\n",
    "        'requirements': {\n",
    "            'commercial_use': False,\n",
    "            'min_performance': 0.45,\n",
    "            'benchmark': 'mmlu'\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "recommendation_results = []\n",
    "\n",
    "for scenario in recommendation_scenarios:\n",
    "    print(f\"\\nScenario: {scenario['name']}\")\n",
    "    print(f\"Requirements: {scenario['requirements']}\")\n",
    "    \n",
    "    recommendations = model_registry.get_model_recommendations(scenario['requirements'])\n",
    "    \n",
    "    print(f\"\\nTop recommendations:\")\n",
    "    for i, (model, score) in enumerate(recommendations[:3], 1):\n",
    "        print(f\"  {i}. {model.name} (Score: {score:.2f})\")\n",
    "        print(f\"     License: {model.license_type.value}, Commercial: {model.commercial_use}\")\n",
    "        print(f\"     Rating: {model.rating}, Size: {model.model_size_gb:.1f}GB\")\n",
    "    \n",
    "    recommendation_results.append({\n",
    "        'scenario': scenario['name'],\n",
    "        'recommendations_count': len(recommendations),\n",
    "        'top_score': recommendations[0][1] if recommendations else 0,\n",
    "        'avg_score': np.mean([score for _, score in recommendations]) if recommendations else 0\n",
    "    })\n",
    "\n",
    "# Visualize search and recommendation results\n",
    "df_search = pd.DataFrame(search_results)\n",
    "df_recommendations = pd.DataFrame(recommendation_results)\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Search results distribution\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.bar(df_search['scenario'], df_search['results_count'], color='skyblue')\n",
    "plt.xlabel('Search Scenario')\n",
    "plt.ylabel('Results Count')\n",
    "plt.title('Search Results by Scenario')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Average rating of search results\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.bar(df_search['scenario'], df_search['avg_rating'], color='lightgreen')\n",
    "plt.xlabel('Search Scenario')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Quality of Search Results')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 5)\n",
    "\n",
    "# Recommendation scores\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.bar(df_recommendations['scenario'], df_recommendations['top_score'], color='coral')\n",
    "plt.xlabel('Recommendation Scenario')\n",
    "plt.ylabel('Top Recommendation Score')\n",
    "plt.title('Recommendation Quality')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Model registry overview\n",
    "analytics = model_registry.get_ecosystem_analytics()\n",
    "\n",
    "# License distribution\n",
    "plt.subplot(3, 3, 4)\n",
    "license_dist = analytics['license_analysis']['distribution']\n",
    "license_names = [license.value for license in license_dist.keys()]\n",
    "license_counts = list(license_dist.values())\n",
    "plt.pie(license_counts, labels=license_names, autopct='%1.1f%%')\n",
    "plt.title('License Distribution')\n",
    "\n",
    "# Organization distribution\n",
    "plt.subplot(3, 3, 5)\n",
    "org_dist = analytics['overview']['organization_distribution']\n",
    "plt.bar(org_dist.keys(), org_dist.values(), color='lightblue')\n",
    "plt.xlabel('Organization')\n",
    "plt.ylabel('Model Count')\n",
    "plt.title('Models by Organization')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Parameter size distribution\n",
    "plt.subplot(3, 3, 6)\n",
    "size_dist = analytics['size_analysis']['size_distribution']\n",
    "plt.bar(size_dist.keys(), size_dist.values(), color='gold')\n",
    "plt.xlabel('Parameter Range')\n",
    "plt.ylabel('Model Count')\n",
    "plt.title('Model Size Distribution')\n",
    "\n",
    "# Performance comparison (MMLU scores)\n",
    "plt.subplot(3, 3, 7)\n",
    "models_with_mmlu = [m for m in model_registry.models.values() if 'mmlu' in m.performance_metrics]\n",
    "model_names = [m.name.split()[0] for m in models_with_mmlu]  # Shortened names\n",
    "mmlu_scores = [m.performance_metrics['mmlu'] for m in models_with_mmlu]\n",
    "plt.bar(model_names, mmlu_scores, color='lightcoral')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('MMLU Score')\n",
    "plt.title('MMLU Performance Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Download popularity\n",
    "plt.subplot(3, 3, 8)\n",
    "all_models = list(model_registry.models.values())\n",
    "model_names_short = [m.name.split()[0] for m in all_models]\n",
    "download_counts = [m.download_count for m in all_models]\n",
    "plt.bar(model_names_short, download_counts, color='mediumpurple')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Download Count')\n",
    "plt.title('Model Popularity (Downloads)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Commercial use vs Rating\n",
    "plt.subplot(3, 3, 9)\n",
    "commercial_models = [m for m in all_models if m.commercial_use]\n",
    "non_commercial_models = [m for m in all_models if not m.commercial_use]\n",
    "\n",
    "commercial_ratings = [m.rating for m in commercial_models]\n",
    "non_commercial_ratings = [m.rating for m in non_commercial_models]\n",
    "\n",
    "plt.boxplot([commercial_ratings, non_commercial_ratings], \n",
    "           labels=['Commercial OK', 'Non-Commercial'])\n",
    "plt.ylabel('Rating')\n",
    "plt.title('Rating Distribution by Commercial Use')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== ECOSYSTEM ANALYTICS SUMMARY ===\")\n",
    "print(f\"Total models in registry: {analytics['overview']['total_models']}\")\n",
    "print(f\"Total downloads: {analytics['overview']['total_downloads']:,}\")\n",
    "print(f\"Commercial-friendly models: {analytics['license_analysis']['commercial_friendly_ratio']:.1%}\")\n",
    "print(f\"Most common license: {analytics['license_analysis']['most_common_license'][0].value if analytics['license_analysis']['most_common_license'] else 'N/A'}\")\n",
    "print(f\"Average model size: {analytics['size_analysis']['size_stats']['mean_gb']:.1f} GB\")\n",
    "print(f\"Release velocity: {analytics['temporal_analysis']['release_velocity']:.1f} models/month\")\n",
    "\n",
    "if 'mmlu' in analytics['performance_analysis']:\n",
    "    mmlu_stats = analytics['performance_analysis']['mmlu']\n",
    "    print(f\"Average MMLU performance: {mmlu_stats['mean']:.3f} (Â±{mmlu_stats['std']:.3f})\")\n",
    "    print(f\"Best MMLU performance: {mmlu_stats['max']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
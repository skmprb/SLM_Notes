{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Inference and Decoding Strategies\n",
    "\n",
    "This notebook covers various strategies for generating text from language models.\n",
    "\n",
    "## Topics Covered:\n",
    "- Greedy decoding\n",
    "- Beam search\n",
    "- Top-k sampling\n",
    "- Top-p (nucleus) sampling\n",
    "- Temperature scaling\n",
    "- Repetition penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decoding Strategies Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DecodingStrategies:\n",
    "    \"\"\"Implementation of various decoding strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.eos_token = 0\n",
    "    \n",
    "    def softmax(self, logits: np.ndarray, temperature: float = 1.0) -> np.ndarray:\n",
    "        \"\"\"Apply softmax with temperature scaling.\"\"\"\n",
    "        scaled_logits = logits / temperature\n",
    "        exp_logits = np.exp(scaled_logits - np.max(scaled_logits))\n",
    "        return exp_logits / np.sum(exp_logits)\n",
    "    \n",
    "    def greedy_decode(self, logits: np.ndarray) -> int:\n",
    "        \"\"\"Greedy decoding - select most probable token.\"\"\"\n",
    "        return np.argmax(logits)\n",
    "    \n",
    "    def top_k_sampling(self, logits: np.ndarray, k: int, temperature: float = 1.0) -> int:\n",
    "        \"\"\"Top-k sampling.\"\"\"\n",
    "        # Get top-k indices\n",
    "        top_k_indices = np.argpartition(logits, -k)[-k:]\n",
    "        top_k_logits = logits[top_k_indices]\n",
    "        \n",
    "        # Apply temperature and softmax\n",
    "        probs = self.softmax(top_k_logits, temperature)\n",
    "        \n",
    "        # Sample from top-k\n",
    "        sampled_idx = np.random.choice(len(top_k_indices), p=probs)\n",
    "        return top_k_indices[sampled_idx]\n",
    "    \n",
    "    def top_p_sampling(self, logits: np.ndarray, p: float, temperature: float = 1.0) -> int:\n",
    "        \"\"\"Top-p (nucleus) sampling.\"\"\"\n",
    "        # Apply temperature and get probabilities\n",
    "        probs = self.softmax(logits, temperature)\n",
    "        \n",
    "        # Sort probabilities in descending order\n",
    "        sorted_indices = np.argsort(probs)[::-1]\n",
    "        sorted_probs = probs[sorted_indices]\n",
    "        \n",
    "        # Find nucleus (top-p)\n",
    "        cumsum_probs = np.cumsum(sorted_probs)\n",
    "        nucleus_size = np.searchsorted(cumsum_probs, p) + 1\n",
    "        \n",
    "        # Sample from nucleus\n",
    "        nucleus_indices = sorted_indices[:nucleus_size]\n",
    "        nucleus_probs = sorted_probs[:nucleus_size]\n",
    "        nucleus_probs = nucleus_probs / np.sum(nucleus_probs)  # Renormalize\n",
    "        \n",
    "        sampled_idx = np.random.choice(len(nucleus_indices), p=nucleus_probs)\n",
    "        return nucleus_indices[sampled_idx]\n",
    "    \n",
    "    def beam_search(self, get_logits_fn, start_token: int, max_length: int, \n",
    "                   beam_width: int, length_penalty: float = 1.0) -> List[Tuple[List[int], float]]:\n",
    "        \"\"\"Beam search decoding.\"\"\"\n",
    "        # Initialize beams: (sequence, log_prob)\n",
    "        beams = [([start_token], 0.0)]\n",
    "        completed_beams = []\n",
    "        \n",
    "        for step in range(max_length):\n",
    "            candidates = []\n",
    "            \n",
    "            for sequence, log_prob in beams:\n",
    "                if sequence[-1] == self.eos_token:\n",
    "                    completed_beams.append((sequence, log_prob))\n",
    "                    continue\n",
    "                \n",
    "                # Get logits for next token\n",
    "                logits = get_logits_fn(sequence)\n",
    "                probs = self.softmax(logits)\n",
    "                \n",
    "                # Get top beam_width candidates\n",
    "                top_indices = np.argpartition(probs, -beam_width)[-beam_width:]\n",
    "                \n",
    "                for token_id in top_indices:\n",
    "                    new_sequence = sequence + [token_id]\n",
    "                    new_log_prob = log_prob + np.log(probs[token_id] + 1e-10)\n",
    "                    candidates.append((new_sequence, new_log_prob))\n",
    "            \n",
    "            # Select top beam_width candidates\n",
    "            candidates.sort(key=lambda x: x[1] / (len(x[0]) ** length_penalty), reverse=True)\n",
    "            beams = candidates[:beam_width]\n",
    "            \n",
    "            if not beams:\n",
    "                break\n",
    "        \n",
    "        # Add remaining beams to completed\n",
    "        completed_beams.extend(beams)\n",
    "        \n",
    "        # Sort by score\n",
    "        completed_beams.sort(key=lambda x: x[1] / (len(x[0]) ** length_penalty), reverse=True)\n",
    "        \n",
    "        return completed_beams\n",
    "    \n",
    "    def apply_repetition_penalty(self, logits: np.ndarray, generated_tokens: List[int], \n",
    "                               penalty: float = 1.2) -> np.ndarray:\n",
    "        \"\"\"Apply repetition penalty to logits.\"\"\"\n",
    "        penalized_logits = logits.copy()\n",
    "        \n",
    "        for token in set(generated_tokens):\n",
    "            if token < len(penalized_logits):\n",
    "                if penalized_logits[token] > 0:\n",
    "                    penalized_logits[token] /= penalty\n",
    "                else:\n",
    "                    penalized_logits[token] *= penalty\n",
    "        \n",
    "        return penalized_logits\n",
    "\n",
    "def demonstrate_decoding_strategies():\n",
    "    \"\"\"Demonstrate different decoding strategies.\"\"\"\n",
    "    \n",
    "    # Create a simple mock language model\n",
    "    vocab_size = 20\n",
    "    decoder = DecodingStrategies(vocab_size)\n",
    "    \n",
    "    # Mock function to get logits (simplified)\n",
    "    def get_logits(sequence):\n",
    "        # Simple pattern: higher probability for tokens 1-5\n",
    "        logits = np.random.randn(vocab_size) * 0.5\n",
    "        logits[1:6] += 2.0  # Boost certain tokens\n",
    "        \n",
    "        # Add some context dependency\n",
    "        if len(sequence) > 1:\n",
    "            last_token = sequence[-1]\n",
    "            if last_token < vocab_size:\n",
    "                logits[last_token] -= 1.0  # Reduce repetition\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    # Generate sample logits\n",
    "    sample_logits = get_logits([1])\n",
    "    \n",
    "    # Test different strategies\n",
    "    print(\"Decoding Strategy Comparison:\")\n",
    "    print(f\"Sample logits shape: {sample_logits.shape}\")\n",
    "    \n",
    "    # Greedy decoding\n",
    "    greedy_token = decoder.greedy_decode(sample_logits)\n",
    "    print(f\"\\nGreedy decoding: token {greedy_token}\")\n",
    "    \n",
    "    # Top-k sampling\n",
    "    top_k_tokens = [decoder.top_k_sampling(sample_logits, k=5) for _ in range(5)]\n",
    "    print(f\"Top-k sampling (k=5): {top_k_tokens}\")\n",
    "    \n",
    "    # Top-p sampling\n",
    "    top_p_tokens = [decoder.top_p_sampling(sample_logits, p=0.9) for _ in range(5)]\n",
    "    print(f\"Top-p sampling (p=0.9): {top_p_tokens}\")\n",
    "    \n",
    "    # Beam search\n",
    "    beam_results = decoder.beam_search(get_logits, start_token=1, max_length=5, beam_width=3)\n",
    "    print(f\"\\nBeam search results:\")\n",
    "    for i, (sequence, score) in enumerate(beam_results[:3]):\n",
    "        print(f\"  Beam {i+1}: {sequence} (score: {score:.3f})\")\n",
    "    \n",
    "    # Visualize decoding strategies\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Original probability distribution\n",
    "    plt.subplot(3, 3, 1)\n",
    "    probs = decoder.softmax(sample_logits)\n",
    "    plt.bar(range(vocab_size), probs, alpha=0.7)\n",
    "    plt.title('Original Probability Distribution')\n",
    "    plt.xlabel('Token ID')\n",
    "    plt.ylabel('Probability')\n",
    "    \n",
    "    # Temperature effects\n",
    "    plt.subplot(3, 3, 2)\n",
    "    temperatures = [0.5, 1.0, 2.0]\n",
    "    for temp in temperatures:\n",
    "        temp_probs = decoder.softmax(sample_logits, temp)\n",
    "        plt.plot(temp_probs, label=f'T={temp}', alpha=0.7)\n",
    "    \n",
    "    plt.title('Temperature Effects')\n",
    "    plt.xlabel('Token ID')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Top-k sampling visualization\n",
    "    plt.subplot(3, 3, 3)\n",
    "    k_values = [3, 5, 10]\n",
    "    \n",
    "    for k in k_values:\n",
    "        top_k_indices = np.argpartition(sample_logits, -k)[-k:]\n",
    "        top_k_probs = np.zeros(vocab_size)\n",
    "        top_k_probs[top_k_indices] = decoder.softmax(sample_logits[top_k_indices])\n",
    "        \n",
    "        plt.bar(range(vocab_size), top_k_probs, alpha=0.5, label=f'k={k}')\n",
    "    \n",
    "    plt.title('Top-k Sampling')\n",
    "    plt.xlabel('Token ID')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Top-p sampling visualization\n",
    "    plt.subplot(3, 3, 4)\n",
    "    p_values = [0.7, 0.9, 0.95]\n",
    "    \n",
    "    for p in p_values:\n",
    "        probs = decoder.softmax(sample_logits)\n",
    "        sorted_indices = np.argsort(probs)[::-1]\n",
    "        sorted_probs = probs[sorted_indices]\n",
    "        cumsum_probs = np.cumsum(sorted_probs)\n",
    "        nucleus_size = np.searchsorted(cumsum_probs, p) + 1\n",
    "        \n",
    "        nucleus_probs = np.zeros(vocab_size)\n",
    "        nucleus_indices = sorted_indices[:nucleus_size]\n",
    "        nucleus_probs[nucleus_indices] = sorted_probs[:nucleus_size]\n",
    "        nucleus_probs = nucleus_probs / np.sum(nucleus_probs)\n",
    "        \n",
    "        plt.bar(range(vocab_size), nucleus_probs, alpha=0.5, label=f'p={p}')\n",
    "    \n",
    "    plt.title('Top-p (Nucleus) Sampling')\n",
    "    plt.xlabel('Token ID')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Repetition penalty effect\n",
    "    plt.subplot(3, 3, 5)\n",
    "    generated_tokens = [2, 3, 2, 4]  # Some repeated tokens\n",
    "    \n",
    "    original_probs = decoder.softmax(sample_logits)\n",
    "    penalized_logits = decoder.apply_repetition_penalty(sample_logits, generated_tokens, penalty=1.5)\n",
    "    penalized_probs = decoder.softmax(penalized_logits)\n",
    "    \n",
    "    x = np.arange(vocab_size)\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, original_probs, width, label='Original', alpha=0.7)\n",
    "    plt.bar(x + width/2, penalized_probs, width, label='With Penalty', alpha=0.7)\n",
    "    \n",
    "    # Highlight repeated tokens\n",
    "    for token in set(generated_tokens):\n",
    "        plt.axvline(x=token, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.title('Repetition Penalty Effect')\n",
    "    plt.xlabel('Token ID')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Beam search tree visualization (simplified)\n",
    "    plt.subplot(3, 3, 6)\n",
    "    \n",
    "    # Simulate beam search steps\n",
    "    beam_data = {\n",
    "        'Step 0': [1.0],\n",
    "        'Step 1': [0.8, 0.6, 0.4],\n",
    "        'Step 2': [0.7, 0.5, 0.4, 0.3, 0.2],\n",
    "        'Step 3': [0.6, 0.4, 0.3]\n",
    "    }\n",
    "    \n",
    "    for i, (step, scores) in enumerate(beam_data.items()):\n",
    "        y_positions = np.linspace(-1, 1, len(scores))\n",
    "        plt.scatter([i] * len(scores), y_positions, s=[s*100 for s in scores], alpha=0.7)\n",
    "        \n",
    "        # Connect to previous step (simplified)\n",
    "        if i > 0:\n",
    "            prev_step, prev_scores = list(beam_data.items())[i-1]\n",
    "            prev_y = np.linspace(-1, 1, len(prev_scores))\n",
    "            for j, y in enumerate(y_positions[:len(prev_y)]):\n",
    "                if j < len(prev_y):\n",
    "                    plt.plot([i-1, i], [prev_y[j], y], 'k-', alpha=0.3)\n",
    "    \n",
    "    plt.title('Beam Search Tree')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Beam Position')\n",
    "    plt.xticks(range(len(beam_data)), beam_data.keys())\n",
    "    \n",
    "    # Strategy comparison metrics\n",
    "    plt.subplot(3, 3, 7)\n",
    "    \n",
    "    # Simulate diversity and quality metrics\n",
    "    strategies = ['Greedy', 'Top-k', 'Top-p', 'Beam Search']\n",
    "    diversity = [0.1, 0.7, 0.8, 0.4]  # Higher = more diverse\n",
    "    quality = [0.9, 0.7, 0.6, 0.8]   # Higher = better quality\n",
    "    \n",
    "    x = np.arange(len(strategies))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, diversity, width, label='Diversity', alpha=0.7)\n",
    "    plt.bar(x + width/2, quality, width, label='Quality', alpha=0.7)\n",
    "    \n",
    "    plt.title('Strategy Trade-offs')\n",
    "    plt.xlabel('Strategy')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(x, strategies, rotation=45)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Computational cost comparison\n",
    "    plt.subplot(3, 3, 8)\n",
    "    \n",
    "    # Relative computational costs\n",
    "    costs = [1, 2, 2.5, 8]  # Relative to greedy\n",
    "    \n",
    "    bars = plt.bar(strategies, costs, alpha=0.7)\n",
    "    plt.title('Computational Cost')\n",
    "    plt.xlabel('Strategy')\n",
    "    plt.ylabel('Relative Cost')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, cost in zip(bars, costs):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                f'{cost}x', ha='center')\n",
    "    \n",
    "    # Parameter sensitivity analysis\n",
    "    plt.subplot(3, 3, 9)\n",
    "    \n",
    "    # Show how different parameters affect output diversity\n",
    "    param_ranges = {\n",
    "        'Temperature': np.linspace(0.1, 2.0, 10),\n",
    "        'Top-k': np.arange(1, 11),\n",
    "        'Top-p': np.linspace(0.1, 1.0, 10)\n",
    "    }\n",
    "    \n",
    "    # Simulate diversity scores for different parameter values\n",
    "    for param_name, param_values in param_ranges.items():\n",
    "        if param_name == 'Temperature':\n",
    "            diversity_scores = 1 - np.exp(-param_values)  # Increases with temperature\n",
    "        elif param_name == 'Top-k':\n",
    "            diversity_scores = np.log(param_values + 1) / np.log(11)  # Logarithmic increase\n",
    "        else:  # Top-p\n",
    "            diversity_scores = param_values  # Linear increase\n",
    "        \n",
    "        plt.plot(param_values, diversity_scores, 'o-', label=param_name, alpha=0.7)\n",
    "    \n",
    "    plt.title('Parameter Sensitivity')\n",
    "    plt.xlabel('Parameter Value')\n",
    "    plt.ylabel('Output Diversity')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nDecoding Strategy Characteristics:\")\n",
    "    \n",
    "    print(\"\\nGreedy Decoding:\")\n",
    "    print(\"  + Fast and deterministic\")\n",
    "    print(\"  + High quality for well-trained models\")\n",
    "    print(\"  - No diversity, can get stuck in loops\")\n",
    "    \n",
    "    print(\"\\nBeam Search:\")\n",
    "    print(\"  + Finds high-probability sequences\")\n",
    "    print(\"  + Good for tasks requiring accuracy\")\n",
    "    print(\"  - Computationally expensive\")\n",
    "    print(\"  - Can produce generic outputs\")\n",
    "    \n",
    "    print(\"\\nTop-k Sampling:\")\n",
    "    print(\"  + Good balance of quality and diversity\")\n",
    "    print(\"  + Controllable via k parameter\")\n",
    "    print(\"  - Fixed vocabulary size regardless of distribution\")\n",
    "    \n",
    "    print(\"\\nTop-p Sampling:\")\n",
    "    print(\"  + Adaptive vocabulary size\")\n",
    "    print(\"  + Good for creative tasks\")\n",
    "    print(\"  + Handles varying confidence levels\")\n",
    "    print(\"  - Can be unstable with poor models\")\n",
    "\n",
    "demonstrate_decoding_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Decoding Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class AdvancedDecoding:\n",
    "    \"\"\"Advanced decoding techniques.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.eos_token = 0\n",
    "    \n",
    "    def diverse_beam_search(self, get_logits_fn, start_token: int, max_length: int,\n",
    "                          beam_width: int, num_groups: int, diversity_penalty: float = 0.5):\n",
    "        \"\"\"Diverse beam search for more varied outputs.\"\"\"\n",
    "        group_size = beam_width // num_groups\n",
    "        all_beams = []\n",
    "        \n",
    "        for group in range(num_groups):\n",
    "            # Initialize group beams\n",
    "            beams = [([start_token], 0.0)]\n",
    "            \n",
    "            for step in range(max_length):\n",
    "                candidates = []\n",
    "                \n",
    "                for sequence, log_prob in beams:\n",
    "                    if sequence[-1] == self.eos_token:\n",
    "                        candidates.append((sequence, log_prob))\n",
    "                        continue\n",
    "                    \n",
    "                    logits = get_logits_fn(sequence)\n",
    "                    \n",
    "                    # Apply diversity penalty based on other groups\n",
    "                    if step > 0 and all_beams:\n",
    "                        for other_group_beams in all_beams:\n",
    "                            for other_seq, _ in other_group_beams:\n",
    "                                if step < len(other_seq):\n",
    "                                    other_token = other_seq[step]\n",
    "                                    if other_token < len(logits):\n",
    "                                        logits[other_token] -= diversity_penalty\n",
    "                    \n",
    "                    probs = self._softmax(logits)\n",
    "                    top_indices = np.argpartition(probs, -group_size)[-group_size:]\n",
    "                    \n",
    "                    for token_id in top_indices:\n",
    "                        new_sequence = sequence + [token_id]\n",
    "                        new_log_prob = log_prob + np.log(probs[token_id] + 1e-10)\n",
    "                        candidates.append((new_sequence, new_log_prob))\n",
    "                \n",
    "                candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "                beams = candidates[:group_size]\n",
    "                \n",
    "                if not beams:\n",
    "                    break\n",
    "            \n",
    "            all_beams.append(beams)\n",
    "        \n",
    "        # Flatten and return all beams\n",
    "        result = []\n",
    "        for group_beams in all_beams:\n",
    "            result.extend(group_beams)\n",
    "        \n",
    "        result.sort(key=lambda x: x[1], reverse=True)\n",
    "        return result\n",
    "    \n",
    "    def contrastive_search(self, get_logits_fn, start_token: int, max_length: int,\n",
    "                         alpha: float = 0.6, k: int = 4):\n",
    "        \"\"\"Contrastive search for coherent and diverse generation.\"\"\"\n",
    "        sequence = [start_token]\n",
    "        \n",
    "        for step in range(max_length):\n",
    "            if sequence[-1] == self.eos_token:\n",
    "                break\n",
    "            \n",
    "            logits = get_logits_fn(sequence)\n",
    "            probs = self._softmax(logits)\n",
    "            \n",
    "            # Get top-k candidates\n",
    "            top_k_indices = np.argpartition(probs, -k)[-k:]\n",
    "            \n",
    "            best_score = float('-inf')\n",
    "            best_token = top_k_indices[0]\n",
    "            \n",
    "            for token_id in top_k_indices:\n",
    "                # Model confidence\n",
    "                model_prob = probs[token_id]\n",
    "                \n",
    "                # Degeneration penalty (simplified)\n",
    "                degeneration_penalty = 0\n",
    "                if len(sequence) > 1:\n",
    "                    # Penalize repetition\n",
    "                    recent_tokens = sequence[-min(5, len(sequence)):]\n",
    "                    if token_id in recent_tokens:\n",
    "                        degeneration_penalty = 0.5\n",
    "                \n",
    "                # Combined score\n",
    "                score = alpha * np.log(model_prob) - (1 - alpha) * degeneration_penalty\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_token = token_id\n",
    "            \n",
    "            sequence.append(best_token)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def _softmax(self, logits: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Helper softmax function.\"\"\"\n",
    "        exp_logits = np.exp(logits - np.max(logits))\n",
    "        return exp_logits / np.sum(exp_logits)\n",
    "\n",
    "def demonstrate_advanced_decoding():\n",
    "    \"\"\"Demonstrate advanced decoding techniques.\"\"\"\n",
    "    \n",
    "    vocab_size = 15\n",
    "    decoder = AdvancedDecoding()\n",
    "    \n",
    "    # Mock logits function with some patterns\n",
    "    def get_logits(sequence):\n",
    "        logits = np.random.randn(vocab_size) * 0.5\n",
    "        \n",
    "        # Create some patterns\n",
    "        if len(sequence) % 2 == 0:\n",
    "            logits[1:4] += 1.5  # Boost tokens 1-3 on even steps\n",
    "        else:\n",
    "            logits[5:8] += 1.5  # Boost tokens 5-7 on odd steps\n",
    "        \n",
    "        # Reduce probability of recent tokens\n",
    "        for token in sequence[-3:]:\n",
    "            if token < vocab_size:\n",
    "                logits[token] -= 0.5\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    # Test advanced techniques\n",
    "    print(\"Advanced Decoding Techniques:\")\n",
    "    \n",
    "    # Diverse beam search\n",
    "    diverse_results = decoder.diverse_beam_search(\n",
    "        get_logits, start_token=1, max_length=8, \n",
    "        beam_width=6, num_groups=2, diversity_penalty=0.5\n",
    "    )\n",
    "    \n",
    "    print(\"\\nDiverse Beam Search Results:\")\n",
    "    for i, (sequence, score) in enumerate(diverse_results[:4]):\n",
    "        print(f\"  {i+1}: {sequence} (score: {score:.3f})\")\n",
    "    \n",
    "    # Contrastive search\n",
    "    contrastive_result = decoder.contrastive_search(\n",
    "        get_logits, start_token=1, max_length=8, alpha=0.6, k=4\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nContrastive Search Result: {contrastive_result}\")\n",
    "    \n",
    "    # Compare diversity metrics\n",
    "    def calculate_diversity(sequences):\n",
    "        \"\"\"Calculate diversity metrics for a set of sequences.\"\"\"\n",
    "        if not sequences:\n",
    "            return 0, 0\n",
    "        \n",
    "        # Unique tokens\n",
    "        all_tokens = []\n",
    "        for seq, _ in sequences:\n",
    "            all_tokens.extend(seq)\n",
    "        \n",
    "        unique_tokens = len(set(all_tokens))\n",
    "        total_tokens = len(all_tokens)\n",
    "        \n",
    "        # Sequence diversity (Jaccard distance)\n",
    "        if len(sequences) < 2:\n",
    "            return unique_tokens / total_tokens, 0\n",
    "        \n",
    "        diversity_sum = 0\n",
    "        count = 0\n",
    "        \n",
    "        for i in range(len(sequences)):\n",
    "            for j in range(i + 1, len(sequences)):\n",
    "                seq1_set = set(sequences[i][0])\n",
    "                seq2_set = set(sequences[j][0])\n",
    "                \n",
    "                intersection = len(seq1_set.intersection(seq2_set))\n",
    "                union = len(seq1_set.union(seq2_set))\n",
    "                \n",
    "                if union > 0:\n",
    "                    diversity_sum += 1 - (intersection / union)\n",
    "                    count += 1\n",
    "        \n",
    "        avg_diversity = diversity_sum / count if count > 0 else 0\n",
    "        token_diversity = unique_tokens / total_tokens\n",
    "        \n",
    "        return token_diversity, avg_diversity\n",
    "    \n",
    "    # Calculate metrics\n",
    "    token_div, seq_div = calculate_diversity(diverse_results[:4])\n",
    "    \n",
    "    print(f\"\\nDiversity Metrics:\")\n",
    "    print(f\"  Token diversity: {token_div:.3f}\")\n",
    "    print(f\"  Sequence diversity: {seq_div:.3f}\")\n",
    "    \n",
    "    # Visualize advanced techniques\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Diverse beam search visualization\n",
    "    plt.subplot(2, 3, 1)\n",
    "    \n",
    "    # Show token distribution across diverse beams\n",
    "    token_counts = defaultdict(int)\n",
    "    for sequence, _ in diverse_results[:4]:\n",
    "        for token in sequence:\n",
    "            token_counts[token] += 1\n",
    "    \n",
    "    tokens = list(token_counts.keys())\n",
    "    counts = list(token_counts.values())\n",
    "    \n",
    "    plt.bar(tokens, counts, alpha=0.7)\n",
    "    plt.title('Diverse Beam Search\\nToken Distribution')\n",
    "    plt.xlabel('Token ID')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Contrastive search token distribution\n",
    "    plt.subplot(2, 3, 2)\n",
    "    \n",
    "    contrastive_counts = defaultdict(int)\n",
    "    for token in contrastive_result:\n",
    "        contrastive_counts[token] += 1\n",
    "    \n",
    "    c_tokens = list(contrastive_counts.keys())\n",
    "    c_counts = list(contrastive_counts.values())\n",
    "    \n",
    "    plt.bar(c_tokens, c_counts, alpha=0.7, color='orange')\n",
    "    plt.title('Contrastive Search\\nToken Distribution')\n",
    "    plt.xlabel('Token ID')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Diversity comparison\n",
    "    plt.subplot(2, 3, 3)\n",
    "    \n",
    "    techniques = ['Standard\\nBeam', 'Diverse\\nBeam', 'Contrastive\\nSearch']\n",
    "    \n",
    "    # Simulate diversity scores\n",
    "    diversity_scores = [0.3, 0.7, 0.6]\n",
    "    quality_scores = [0.8, 0.6, 0.7]\n",
    "    \n",
    "    x = np.arange(len(techniques))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, diversity_scores, width, label='Diversity', alpha=0.7)\n",
    "    plt.bar(x + width/2, quality_scores, width, label='Quality', alpha=0.7)\n",
    "    \n",
    "    plt.title('Technique Comparison')\n",
    "    plt.xlabel('Technique')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(x, techniques)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Sequence length analysis\n",
    "    plt.subplot(2, 3, 4)\n",
    "    \n",
    "    diverse_lengths = [len(seq) for seq, _ in diverse_results[:6]]\n",
    "    \n",
    "    plt.hist(diverse_lengths, bins=5, alpha=0.7, label='Diverse Beam')\n",
    "    plt.axvline(len(contrastive_result), color='orange', linestyle='--', \n",
    "               label='Contrastive', linewidth=2)\n",
    "    \n",
    "    plt.title('Sequence Length Distribution')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Parameter sensitivity for contrastive search\n",
    "    plt.subplot(2, 3, 5)\n",
    "    \n",
    "    alpha_values = np.linspace(0.1, 0.9, 9)\n",
    "    \n",
    "    # Simulate how alpha affects diversity vs quality\n",
    "    diversity_trend = alpha_values  # Higher alpha = more diverse\n",
    "    quality_trend = 1 - alpha_values  # Lower alpha = higher quality\n",
    "    \n",
    "    plt.plot(alpha_values, diversity_trend, 'o-', label='Diversity', alpha=0.7)\n",
    "    plt.plot(alpha_values, quality_trend, 's-', label='Quality', alpha=0.7)\n",
    "    \n",
    "    plt.title('Contrastive Search\\nAlpha Parameter Effect')\n",
    "    plt.xlabel('Alpha Value')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Computational complexity comparison\n",
    "    plt.subplot(2, 3, 6)\n",
    "    \n",
    "    complexities = {\n",
    "        'Greedy': 1,\n",
    "        'Beam Search': 5,\n",
    "        'Diverse Beam': 7,\n",
    "        'Contrastive': 3\n",
    "    }\n",
    "    \n",
    "    plt.bar(complexities.keys(), complexities.values(), alpha=0.7)\n",
    "    plt.title('Computational Complexity')\n",
    "    plt.xlabel('Method')\n",
    "    plt.ylabel('Relative Cost')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nAdvanced Decoding Insights:\")\n",
    "    \n",
    "    print(\"\\nDiverse Beam Search:\")\n",
    "    print(\"  + Generates more varied outputs\")\n",
    "    print(\"  + Good for creative applications\")\n",
    "    print(\"  - More computationally expensive\")\n",
    "    print(\"  - May sacrifice some quality for diversity\")\n",
    "    \n",
    "    print(\"\\nContrastive Search:\")\n",
    "    print(\"  + Balances coherence and diversity\")\n",
    "    print(\"  + Reduces repetition effectively\")\n",
    "    print(\"  + Computationally efficient\")\n",
    "    print(\"  - Requires tuning of alpha parameter\")\n",
    "    \n",
    "    print(\"\\nKey Considerations:\")\n",
    "    print(\"  - Task requirements (creativity vs accuracy)\")\n",
    "    print(\"  - Computational budget\")\n",
    "    print(\"  - Model quality and training\")\n",
    "    print(\"  - User preferences and application context\")\n",
    "\n",
    "demonstrate_advanced_decoding()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Sequence Modeling\n",
    "\n",
    "This notebook covers sequence modeling techniques that led to modern transformers.\n",
    "\n",
    "## Topics Covered:\n",
    "- Recurrent Neural Networks (RNNs)\n",
    "- Vanishing and exploding gradients\n",
    "- Long Short-Term Memory (LSTM)\n",
    "- Gated Recurrent Units (GRU)\n",
    "- Sequence-to-sequence models\n",
    "- Encoder-decoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"Basic RNN implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.Wxh = np.random.randn(input_size, hidden_size) * 0.1\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.1\n",
    "        self.Why = np.random.randn(hidden_size, output_size) * 0.1\n",
    "        \n",
    "        # Initialize biases\n",
    "        self.bh = np.zeros((1, hidden_size))\n",
    "        self.by = np.zeros((1, output_size))\n",
    "    \n",
    "    def forward(self, inputs: np.ndarray, h_prev: Optional[np.ndarray] = None) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        \"\"\"Forward pass through RNN.\"\"\"\n",
    "        seq_len, batch_size, input_size = inputs.shape\n",
    "        \n",
    "        if h_prev is None:\n",
    "            h_prev = np.zeros((batch_size, self.hidden_size))\n",
    "        \n",
    "        hidden_states = []\n",
    "        outputs = []\n",
    "        \n",
    "        h = h_prev\n",
    "        for t in range(seq_len):\n",
    "            # RNN cell computation\n",
    "            h = np.tanh(inputs[t] @ self.Wxh + h @ self.Whh + self.bh)\n",
    "            y = h @ self.Why + self.by\n",
    "            \n",
    "            hidden_states.append(h)\n",
    "            outputs.append(y)\n",
    "        \n",
    "        return outputs, hidden_states\n",
    "    \n",
    "    def backward(self, inputs: np.ndarray, targets: np.ndarray, outputs: List[np.ndarray], \n",
    "                hidden_states: List[np.ndarray], learning_rate: float = 0.01):\n",
    "        \"\"\"Backpropagation through time.\"\"\"\n",
    "        seq_len = len(outputs)\n",
    "        \n",
    "        # Initialize gradients\n",
    "        dWxh = np.zeros_like(self.Wxh)\n",
    "        dWhh = np.zeros_like(self.Whh)\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "        dby = np.zeros_like(self.by)\n",
    "        \n",
    "        dh_next = np.zeros_like(hidden_states[0])\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        # Backward pass\n",
    "        for t in reversed(range(seq_len)):\n",
    "            # Output layer gradients\n",
    "            dy = outputs[t] - targets[t]\n",
    "            loss += 0.5 * np.sum(dy ** 2)\n",
    "            \n",
    "            dWhy += hidden_states[t].T @ dy\n",
    "            dby += np.sum(dy, axis=0, keepdims=True)\n",
    "            \n",
    "            # Hidden layer gradients\n",
    "            dh = dy @ self.Why.T + dh_next\n",
    "            dh_raw = (1 - hidden_states[t] ** 2) * dh  # tanh derivative\n",
    "            \n",
    "            dbh += np.sum(dh_raw, axis=0, keepdims=True)\n",
    "            dWxh += inputs[t].T @ dh_raw\n",
    "            \n",
    "            if t > 0:\n",
    "                dWhh += hidden_states[t-1].T @ dh_raw\n",
    "                dh_next = dh_raw @ self.Whh.T\n",
    "        \n",
    "        # Update weights\n",
    "        self.Wxh -= learning_rate * dWxh\n",
    "        self.Whh -= learning_rate * dWhh\n",
    "        self.Why -= learning_rate * dWhy\n",
    "        self.bh -= learning_rate * dbh\n",
    "        self.by -= learning_rate * dby\n",
    "        \n",
    "        return loss / seq_len\n",
    "\n",
    "# Test RNN on simple sequence prediction\n",
    "def generate_sine_data(seq_len: int = 50, num_sequences: int = 100) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate sine wave sequences for testing.\"\"\"\n",
    "    X = np.zeros((num_sequences, seq_len, 1))\n",
    "    y = np.zeros((num_sequences, seq_len, 1))\n",
    "    \n",
    "    for i in range(num_sequences):\n",
    "        start = np.random.uniform(0, 2*np.pi)\n",
    "        t = np.linspace(start, start + 4*np.pi, seq_len + 1)\n",
    "        sequence = np.sin(t)\n",
    "        \n",
    "        X[i, :, 0] = sequence[:-1]\n",
    "        y[i, :, 0] = sequence[1:]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data and train RNN\n",
    "X_train, y_train = generate_sine_data(seq_len=20, num_sequences=50)\n",
    "rnn = SimpleRNN(input_size=1, hidden_size=10, output_size=1)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for i in range(len(X_train)):\n",
    "        # Transpose to (seq_len, batch_size, input_size)\n",
    "        inputs = X_train[i:i+1].transpose(1, 0, 2)\n",
    "        targets = y_train[i:i+1].transpose(1, 0, 2)\n",
    "        \n",
    "        outputs, hidden_states = rnn.forward(inputs)\n",
    "        loss = rnn.backward(inputs, targets, outputs, hidden_states, learning_rate=0.01)\n",
    "        total_loss += loss\n",
    "    \n",
    "    losses.append(total_loss / len(X_train))\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {losses[-1]:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.title('RNN Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Test prediction\n",
    "test_input = X_train[0:1].transpose(1, 0, 2)\n",
    "outputs, _ = rnn.forward(test_input)\n",
    "predictions = np.array([out[0, 0] for out in outputs])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(X_train[0, :, 0], label='Input', alpha=0.7)\n",
    "plt.plot(y_train[0, :, 0], label='Target', alpha=0.7)\n",
    "plt.plot(predictions, label='Predicted', alpha=0.7)\n",
    "plt.title('RNN Prediction')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vanishing and Exploding Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradient_flow(seq_len: int = 50):\n",
    "    \"\"\"Analyze gradient magnitudes in RNN.\"\"\"\n",
    "    \n",
    "    # Create simple RNN weights\n",
    "    hidden_size = 5\n",
    "    Whh_values = [0.5, 1.0, 1.5, 2.0]  # Different weight magnitudes\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for i, whh_val in enumerate(Whh_values):\n",
    "        Whh = np.full((hidden_size, hidden_size), whh_val / hidden_size)\n",
    "        \n",
    "        # Simulate gradient backpropagation\n",
    "        gradient_magnitudes = []\n",
    "        gradient = np.ones(hidden_size)\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Simplified gradient computation (ignoring tanh derivative)\n",
    "            gradient = gradient @ Whh.T\n",
    "            gradient_magnitudes.append(np.linalg.norm(gradient))\n",
    "        \n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        plt.plot(gradient_magnitudes)\n",
    "        plt.title(f'Weight magnitude: {whh_val}')\n",
    "        plt.xlabel('Time steps back')\n",
    "        plt.ylabel('Gradient magnitude')\n",
    "        plt.yscale('log')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        if whh_val < 1.0:\n",
    "            plt.text(0.5, 0.8, 'Vanishing\\nGradients', transform=plt.gca().transAxes, \n",
    "                    bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))\n",
    "        elif whh_val > 1.0:\n",
    "            plt.text(0.5, 0.8, 'Exploding\\nGradients', transform=plt.gca().transAxes,\n",
    "                    bbox=dict(boxstyle='round', facecolor='orange', alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Gradient Flow Analysis:\")\n",
    "    print(\"- Weight < 1.0: Gradients vanish (become very small)\")\n",
    "    print(\"- Weight > 1.0: Gradients explode (become very large)\")\n",
    "    print(\"- This makes learning long-term dependencies difficult\")\n",
    "\n",
    "analyze_gradient_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    \"\"\"LSTM implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Forget gate weights\n",
    "        self.Wf = np.random.randn(input_size + hidden_size, hidden_size) * 0.1\n",
    "        self.bf = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Input gate weights\n",
    "        self.Wi = np.random.randn(input_size + hidden_size, hidden_size) * 0.1\n",
    "        self.bi = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Candidate values weights\n",
    "        self.Wc = np.random.randn(input_size + hidden_size, hidden_size) * 0.1\n",
    "        self.bc = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Output gate weights\n",
    "        self.Wo = np.random.randn(input_size + hidden_size, hidden_size) * 0.1\n",
    "        self.bo = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Output layer weights\n",
    "        self.Wy = np.random.randn(hidden_size, output_size) * 0.1\n",
    "        self.by = np.zeros((1, output_size))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def forward_step(self, x, h_prev, c_prev):\n",
    "        \"\"\"Single LSTM forward step.\"\"\"\n",
    "        # Concatenate input and previous hidden state\n",
    "        combined = np.concatenate([x, h_prev], axis=1)\n",
    "        \n",
    "        # Forget gate\n",
    "        f = self.sigmoid(combined @ self.Wf + self.bf)\n",
    "        \n",
    "        # Input gate\n",
    "        i = self.sigmoid(combined @ self.Wi + self.bi)\n",
    "        \n",
    "        # Candidate values\n",
    "        c_tilde = np.tanh(combined @ self.Wc + self.bc)\n",
    "        \n",
    "        # Update cell state\n",
    "        c = f * c_prev + i * c_tilde\n",
    "        \n",
    "        # Output gate\n",
    "        o = self.sigmoid(combined @ self.Wo + self.bo)\n",
    "        \n",
    "        # Update hidden state\n",
    "        h = o * np.tanh(c)\n",
    "        \n",
    "        # Output\n",
    "        y = h @ self.Wy + self.by\n",
    "        \n",
    "        return y, h, c, (f, i, c_tilde, o)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass through LSTM.\"\"\"\n",
    "        seq_len, batch_size, input_size = inputs.shape\n",
    "        \n",
    "        h = np.zeros((batch_size, self.hidden_size))\n",
    "        c = np.zeros((batch_size, self.hidden_size))\n",
    "        \n",
    "        outputs = []\n",
    "        states = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            y, h, c, gates = self.forward_step(inputs[t], h, c)\n",
    "            outputs.append(y)\n",
    "            states.append((h.copy(), c.copy(), gates))\n",
    "        \n",
    "        return outputs, states\n",
    "\n",
    "# Compare RNN vs LSTM on longer sequences\n",
    "def compare_rnn_lstm():\n",
    "    \"\"\"Compare RNN and LSTM on sequence learning.\"\"\"\n",
    "    \n",
    "    # Generate longer sequences\n",
    "    X_long, y_long = generate_sine_data(seq_len=50, num_sequences=30)\n",
    "    \n",
    "    # Initialize models\n",
    "    rnn = SimpleRNN(input_size=1, hidden_size=20, output_size=1)\n",
    "    lstm = LSTM(input_size=1, hidden_size=20, output_size=1)\n",
    "    \n",
    "    rnn_losses = []\n",
    "    lstm_losses = []\n",
    "    \n",
    "    # Train both models\n",
    "    for epoch in range(50):\n",
    "        rnn_loss = 0\n",
    "        lstm_loss = 0\n",
    "        \n",
    "        for i in range(len(X_long)):\n",
    "            inputs = X_long[i:i+1].transpose(1, 0, 2)\n",
    "            targets = y_long[i:i+1].transpose(1, 0, 2)\n",
    "            \n",
    "            # RNN training\n",
    "            rnn_outputs, rnn_states = rnn.forward(inputs)\n",
    "            rnn_loss += rnn.backward(inputs, targets, rnn_outputs, rnn_states, 0.001)\n",
    "            \n",
    "            # LSTM training (simplified - just compute loss)\n",
    "            lstm_outputs, _ = lstm.forward(inputs)\n",
    "            for t in range(len(lstm_outputs)):\n",
    "                lstm_loss += 0.5 * np.sum((lstm_outputs[t] - targets[t]) ** 2)\n",
    "        \n",
    "        rnn_losses.append(rnn_loss / len(X_long))\n",
    "        lstm_losses.append(lstm_loss / len(X_long) / len(lstm_outputs))\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(rnn_losses, label='RNN', alpha=0.7)\n",
    "    plt.plot(lstm_losses, label='LSTM', alpha=0.7)\n",
    "    plt.title('RNN vs LSTM Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"LSTM advantages:\")\n",
    "    print(\"- Cell state provides long-term memory\")\n",
    "    print(\"- Gates control information flow\")\n",
    "    print(\"- Mitigates vanishing gradient problem\")\n",
    "\n",
    "compare_rnn_lstm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gated Recurrent Units (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    \"\"\"GRU implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Reset gate weights\n",
    "        self.Wr = np.random.randn(input_size + hidden_size, hidden_size) * 0.1\n",
    "        self.br = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Update gate weights\n",
    "        self.Wz = np.random.randn(input_size + hidden_size, hidden_size) * 0.1\n",
    "        self.bz = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # New gate weights\n",
    "        self.Wh = np.random.randn(input_size + hidden_size, hidden_size) * 0.1\n",
    "        self.bh = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Output weights\n",
    "        self.Wy = np.random.randn(hidden_size, output_size) * 0.1\n",
    "        self.by = np.zeros((1, output_size))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def forward_step(self, x, h_prev):\n",
    "        \"\"\"Single GRU forward step.\"\"\"\n",
    "        # Concatenate input and previous hidden state\n",
    "        combined = np.concatenate([x, h_prev], axis=1)\n",
    "        \n",
    "        # Reset gate\n",
    "        r = self.sigmoid(combined @ self.Wr + self.br)\n",
    "        \n",
    "        # Update gate\n",
    "        z = self.sigmoid(combined @ self.Wz + self.bz)\n",
    "        \n",
    "        # New gate (candidate hidden state)\n",
    "        combined_reset = np.concatenate([x, r * h_prev], axis=1)\n",
    "        h_tilde = np.tanh(combined_reset @ self.Wh + self.bh)\n",
    "        \n",
    "        # Update hidden state\n",
    "        h = (1 - z) * h_prev + z * h_tilde\n",
    "        \n",
    "        # Output\n",
    "        y = h @ self.Wy + self.by\n",
    "        \n",
    "        return y, h, (r, z, h_tilde)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass through GRU.\"\"\"\n",
    "        seq_len, batch_size, input_size = inputs.shape\n",
    "        \n",
    "        h = np.zeros((batch_size, self.hidden_size))\n",
    "        \n",
    "        outputs = []\n",
    "        states = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            y, h, gates = self.forward_step(inputs[t], h)\n",
    "            outputs.append(y)\n",
    "            states.append((h.copy(), gates))\n",
    "        \n",
    "        return outputs, states\n",
    "\n",
    "# Visualize gate activations\n",
    "def visualize_gates():\n",
    "    \"\"\"Visualize how gates work in LSTM and GRU.\"\"\"\n",
    "    \n",
    "    # Create sample input sequence\n",
    "    seq_len = 20\n",
    "    x = np.sin(np.linspace(0, 4*np.pi, seq_len)).reshape(seq_len, 1, 1)\n",
    "    \n",
    "    # Initialize models\n",
    "    lstm = LSTM(input_size=1, hidden_size=5, output_size=1)\n",
    "    gru = GRU(input_size=1, hidden_size=5, output_size=1)\n",
    "    \n",
    "    # Forward pass\n",
    "    lstm_outputs, lstm_states = lstm.forward(x)\n",
    "    gru_outputs, gru_states = gru.forward(x)\n",
    "    \n",
    "    # Extract gate values\n",
    "    lstm_forget = [state[2][0].mean() for state in lstm_states]\n",
    "    lstm_input = [state[2][1].mean() for state in lstm_states]\n",
    "    lstm_output = [state[2][3].mean() for state in lstm_states]\n",
    "    \n",
    "    gru_reset = [state[1][0].mean() for state in gru_states]\n",
    "    gru_update = [state[1][1].mean() for state in gru_states]\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # LSTM gates\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(lstm_forget, label='Forget Gate')\n",
    "    plt.title('LSTM Forget Gate')\n",
    "    plt.ylabel('Gate Value')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(lstm_input, label='Input Gate')\n",
    "    plt.title('LSTM Input Gate')\n",
    "    plt.ylabel('Gate Value')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(lstm_output, label='Output Gate')\n",
    "    plt.title('LSTM Output Gate')\n",
    "    plt.ylabel('Gate Value')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # GRU gates\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(gru_reset, label='Reset Gate')\n",
    "    plt.title('GRU Reset Gate')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Gate Value')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(gru_update, label='Update Gate')\n",
    "    plt.title('GRU Update Gate')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Gate Value')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Input sequence\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.plot(x[:, 0, 0], label='Input')\n",
    "    plt.title('Input Sequence')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Gate Functions:\")\n",
    "    print(\"LSTM:\")\n",
    "    print(\"  - Forget gate: What to forget from cell state\")\n",
    "    print(\"  - Input gate: What new information to store\")\n",
    "    print(\"  - Output gate: What parts of cell state to output\")\n",
    "    print(\"GRU:\")\n",
    "    print(\"  - Reset gate: How much past information to forget\")\n",
    "    print(\"  - Update gate: How much new information to add\")\n",
    "\n",
    "visualize_gates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sequence-to-Sequence Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq:\n",
    "    \"\"\"Simple sequence-to-sequence model.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_vocab_size: int, output_vocab_size: int, hidden_size: int):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Encoder (simplified LSTM)\n",
    "        self.encoder = LSTM(input_vocab_size, hidden_size, hidden_size)\n",
    "        \n",
    "        # Decoder (simplified LSTM)\n",
    "        self.decoder = LSTM(output_vocab_size, hidden_size, output_vocab_size)\n",
    "    \n",
    "    def encode(self, input_sequence):\n",
    "        \"\"\"Encode input sequence to context vector.\"\"\"\n",
    "        outputs, states = self.encoder.forward(input_sequence)\n",
    "        \n",
    "        # Use final hidden state as context vector\n",
    "        context = states[-1][0]  # Final hidden state\n",
    "        return context\n",
    "    \n",
    "    def decode(self, context, target_length, start_token=None):\n",
    "        \"\"\"Decode context vector to output sequence.\"\"\"\n",
    "        batch_size = context.shape[0]\n",
    "        \n",
    "        # Initialize decoder state with context\n",
    "        h = context\n",
    "        c = np.zeros_like(context)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        # Start with start token or zeros\n",
    "        if start_token is not None:\n",
    "            decoder_input = start_token\n",
    "        else:\n",
    "            decoder_input = np.zeros((batch_size, self.decoder.Wy.shape[1]))\n",
    "        \n",
    "        for t in range(target_length):\n",
    "            # Decoder step\n",
    "            output, h, c, _ = self.decoder.forward_step(decoder_input, h, c)\n",
    "            outputs.append(output)\n",
    "            \n",
    "            # Use output as next input (teacher forcing disabled)\n",
    "            decoder_input = output\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Demonstrate sequence reversal task\n",
    "def sequence_reversal_demo():\n",
    "    \"\"\"Demonstrate seq2seq on sequence reversal.\"\"\"\n",
    "    \n",
    "    # Generate simple sequences (numbers 0-9)\n",
    "    vocab_size = 10\n",
    "    seq_len = 5\n",
    "    num_samples = 100\n",
    "    \n",
    "    # Create training data\n",
    "    X = np.random.randint(0, vocab_size, (num_samples, seq_len))\n",
    "    y = X[:, ::-1]  # Reversed sequences\n",
    "    \n",
    "    # Convert to one-hot encoding\n",
    "    X_onehot = np.eye(vocab_size)[X].transpose(1, 0, 2)\n",
    "    y_onehot = np.eye(vocab_size)[y].transpose(1, 0, 2)\n",
    "    \n",
    "    print(\"Sequence Reversal Task:\")\n",
    "    print(f\"Input:  {X[0]}\")\n",
    "    print(f\"Target: {y[0]}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = Seq2Seq(vocab_size, vocab_size, hidden_size=20)\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    print(\"Encoder: Input → Hidden State (Context Vector)\")\n",
    "    print(\"Decoder: Context Vector → Output Sequence\")\n",
    "    \n",
    "    # Simple forward pass demonstration\n",
    "    sample_input = X_onehot[:, 0:1, :]  # First sample\n",
    "    context = model.encode(sample_input)\n",
    "    outputs = model.decode(context, seq_len)\n",
    "    \n",
    "    print(f\"\\nContext vector shape: {context.shape}\")\n",
    "    print(f\"Number of output steps: {len(outputs)}\")\n",
    "    print(\"\\nSeq2Seq captures the entire input in a fixed-size context vector\")\n",
    "\n",
    "sequence_reversal_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder:\n",
    "    \"\"\"Encoder-Decoder with attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        self.encoder = LSTM(input_size, hidden_size, hidden_size)\n",
    "        self.decoder = LSTM(output_size + hidden_size, hidden_size, output_size)  # +hidden_size for attention\n",
    "        \n",
    "        # Attention weights\n",
    "        self.W_attention = np.random.randn(hidden_size, hidden_size) * 0.1\n",
    "        self.v_attention = np.random.randn(hidden_size, 1) * 0.1\n",
    "    \n",
    "    def attention(self, decoder_hidden, encoder_outputs):\n",
    "        \"\"\"Compute attention weights and context vector.\"\"\"\n",
    "        seq_len = len(encoder_outputs)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = []\n",
    "        for encoder_output in encoder_outputs:\n",
    "            # Simple additive attention\n",
    "            score = np.tanh(decoder_hidden @ self.W_attention + encoder_output) @ self.v_attention\n",
    "            scores.append(score)\n",
    "        \n",
    "        # Softmax to get attention weights\n",
    "        scores = np.array(scores).reshape(-1)\n",
    "        exp_scores = np.exp(scores - np.max(scores))\n",
    "        attention_weights = exp_scores / np.sum(exp_scores)\n",
    "        \n",
    "        # Compute context vector\n",
    "        context = np.zeros_like(encoder_outputs[0])\n",
    "        for i, encoder_output in enumerate(encoder_outputs):\n",
    "            context += attention_weights[i] * encoder_output\n",
    "        \n",
    "        return context, attention_weights\n",
    "    \n",
    "    def forward(self, input_sequence, target_length):\n",
    "        \"\"\"Forward pass with attention.\"\"\"\n",
    "        # Encode\n",
    "        encoder_outputs, encoder_states = self.encoder.forward(input_sequence)\n",
    "        \n",
    "        # Initialize decoder\n",
    "        decoder_hidden = encoder_states[-1][0]\n",
    "        decoder_cell = encoder_states[-1][1]\n",
    "        \n",
    "        outputs = []\n",
    "        attention_weights_history = []\n",
    "        \n",
    "        # Decode with attention\n",
    "        decoder_input = np.zeros((1, self.decoder.Wy.shape[1]))\n",
    "        \n",
    "        for t in range(target_length):\n",
    "            # Compute attention\n",
    "            context, attention_weights = self.attention(decoder_hidden, \n",
    "                                                      [state[0] for state in encoder_states])\n",
    "            \n",
    "            # Concatenate decoder input with context\n",
    "            decoder_input_with_context = np.concatenate([decoder_input, context], axis=1)\n",
    "            \n",
    "            # Decoder step\n",
    "            output, decoder_hidden, decoder_cell, _ = self.decoder.forward_step(\n",
    "                decoder_input_with_context, decoder_hidden, decoder_cell)\n",
    "            \n",
    "            outputs.append(output)\n",
    "            attention_weights_history.append(attention_weights)\n",
    "            \n",
    "            decoder_input = output\n",
    "        \n",
    "        return outputs, attention_weights_history\n",
    "\n",
    "# Visualize attention mechanism\n",
    "def visualize_attention():\n",
    "    \"\"\"Visualize attention weights.\"\"\"\n",
    "    \n",
    "    # Create sample sequence\n",
    "    input_len = 8\n",
    "    output_len = 6\n",
    "    \n",
    "    # Generate sample input\n",
    "    input_seq = np.random.randn(input_len, 1, 5)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = EncoderDecoder(input_size=5, hidden_size=10, output_size=3)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs, attention_weights = model.forward(input_seq, output_len)\n",
    "    \n",
    "    # Convert attention weights to matrix\n",
    "    attention_matrix = np.array(attention_weights)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(attention_matrix, cmap='Blues', aspect='auto')\n",
    "    plt.colorbar(label='Attention Weight')\n",
    "    plt.xlabel('Encoder Position')\n",
    "    plt.ylabel('Decoder Position')\n",
    "    plt.title('Attention Weights Visualization')\n",
    "    \n",
    "    # Add grid\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Attention Mechanism:\")\n",
    "    print(\"- Decoder can focus on different encoder positions\")\n",
    "    print(\"- Solves the bottleneck problem of fixed context vector\")\n",
    "    print(\"- Each decoder step gets a different context vector\")\n",
    "    print(f\"- Attention matrix shape: {attention_matrix.shape}\")\n",
    "\n",
    "visualize_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered the evolution of sequence modeling:\n",
    "\n",
    "### Key Concepts:\n",
    "1. **RNNs**: Basic sequential processing with hidden states\n",
    "2. **Gradient Problems**: Vanishing/exploding gradients limit learning\n",
    "3. **LSTM**: Gates control information flow, enabling long-term memory\n",
    "4. **GRU**: Simplified gating mechanism with fewer parameters\n",
    "5. **Seq2Seq**: Encoder-decoder framework for sequence transformation\n",
    "6. **Attention**: Dynamic focus mechanism solving bottleneck problems\n",
    "\n",
    "### Evolution Path:\n",
    "RNN → LSTM/GRU → Seq2Seq → Attention → **Transformers**\n",
    "\n",
    "These architectures laid the foundation for modern transformer-based language models by:\n",
    "- Introducing the concept of attention\n",
    "- Solving long-range dependency problems\n",
    "- Establishing encoder-decoder patterns\n",
    "- Demonstrating the power of gating mechanisms\n",
    "\n",
    "The next notebook will explore how transformers revolutionized this field by using attention as the primary mechanism."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

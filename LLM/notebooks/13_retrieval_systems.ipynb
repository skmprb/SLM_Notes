{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 - Retrieval-Augmented Systems\n",
    "\n",
    "This notebook covers retrieval-augmented generation and hybrid systems.\n",
    "\n",
    "## Topics Covered:\n",
    "- Embedding models\n",
    "- Vector databases\n",
    "- Similarity search\n",
    "- Retrieval pipelines\n",
    "- RAG architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Embedding Models and Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class EmbeddingModel:\n",
    "    \"\"\"Simple embedding model for text.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, d_model: int):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.token_embeddings = np.random.randn(vocab_size, d_model) * 0.1\n",
    "        \n",
    "        # Simple vocabulary\n",
    "        self.vocab = {f'token_{i}': i for i in range(vocab_size)}\n",
    "        self.vocab.update({\n",
    "            'the': 0, 'cat': 1, 'dog': 2, 'sat': 3, 'ran': 4,\n",
    "            'on': 5, 'in': 6, 'park': 7, 'mat': 8, 'quick': 9\n",
    "        })\n",
    "    \n",
    "    def encode_text(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Encode text to embedding vector.\"\"\"\n",
    "        tokens = text.lower().split()\n",
    "        \n",
    "        # Get token embeddings\n",
    "        embeddings = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                embeddings.append(self.token_embeddings[self.vocab[token]])\n",
    "            else:\n",
    "                embeddings.append(np.random.randn(self.d_model) * 0.1)\n",
    "        \n",
    "        if not embeddings:\n",
    "            return np.zeros(self.d_model)\n",
    "        \n",
    "        # Mean pooling\n",
    "        sentence_embedding = np.mean(embeddings, axis=0)\n",
    "        \n",
    "        # Normalize\n",
    "        norm = np.linalg.norm(sentence_embedding)\n",
    "        if norm > 0:\n",
    "            sentence_embedding = sentence_embedding / norm\n",
    "        \n",
    "        return sentence_embedding\n",
    "\n",
    "class VectorDatabase:\n",
    "    \"\"\"Simple vector database for similarity search.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int):\n",
    "        self.d_model = d_model\n",
    "        self.vectors = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_vector(self, vector: np.ndarray, metadata: Dict):\n",
    "        \"\"\"Add vector with metadata.\"\"\"\n",
    "        self.vectors.append(vector)\n",
    "        self.metadata.append(metadata)\n",
    "    \n",
    "    def similarity_search(self, query_vector: np.ndarray, top_k: int = 5, \n",
    "                         metric: str = 'cosine') -> List[Tuple[int, float, Dict]]:\n",
    "        \"\"\"Search for similar vectors.\"\"\"\n",
    "        similarities = []\n",
    "        \n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            if metric == 'cosine':\n",
    "                similarity = np.dot(query_vector, vector)\n",
    "            elif metric == 'euclidean':\n",
    "                similarity = -np.linalg.norm(query_vector - vector)\n",
    "            else:\n",
    "                similarity = np.dot(query_vector, vector)\n",
    "            \n",
    "            similarities.append((i, similarity, self.metadata[i]))\n",
    "        \n",
    "        # Sort by similarity\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return similarities[:top_k]\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Get database statistics.\"\"\"\n",
    "        if not self.vectors:\n",
    "            return {'size': 0, 'avg_norm': 0, 'dimension': self.d_model}\n",
    "        \n",
    "        vectors_array = np.array(self.vectors)\n",
    "        \n",
    "        return {\n",
    "            'size': len(self.vectors),\n",
    "            'avg_norm': np.mean(np.linalg.norm(vectors_array, axis=1)),\n",
    "            'dimension': self.d_model,\n",
    "            'memory_mb': vectors_array.nbytes / (1024 ** 2)\n",
    "        }\n",
    "\n",
    "class RAGPipeline:\n",
    "    \"\"\"Complete RAG pipeline implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: EmbeddingModel, vector_db: VectorDatabase):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.vector_db = vector_db\n",
    "    \n",
    "    def index_documents(self, documents: List[str]):\n",
    "        \"\"\"Index documents in the vector database.\"\"\"\n",
    "        for i, doc in enumerate(documents):\n",
    "            embedding = self.embedding_model.encode_text(doc)\n",
    "            metadata = {'doc_id': i, 'text': doc, 'length': len(doc.split())}\n",
    "            self.vector_db.add_vector(embedding, metadata)\n",
    "    \n",
    "    def retrieve_and_generate(self, query: str, top_k: int = 3) -> Dict:\n",
    "        \"\"\"Retrieve relevant documents and generate response.\"\"\"\n",
    "        # Encode query\n",
    "        query_embedding = self.embedding_model.encode_text(query)\n",
    "        \n",
    "        # Retrieve similar documents\n",
    "        results = self.vector_db.similarity_search(query_embedding, top_k)\n",
    "        \n",
    "        # Extract retrieved texts\n",
    "        retrieved_texts = [metadata['text'] for _, _, metadata in results]\n",
    "        similarities = [score for _, score, _ in results]\n",
    "        \n",
    "        # Combine context\n",
    "        context = \" \".join(retrieved_texts)\n",
    "        \n",
    "        # Simulate generation (simplified)\n",
    "        response = f\"Based on retrieved context, {query.lower()} can be answered as: [Generated response using context]\"\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'retrieved_docs': retrieved_texts,\n",
    "            'similarities': similarities,\n",
    "            'context': context,\n",
    "            'response': response\n",
    "        }\n",
    "\n",
    "def demonstrate_rag_systems():\n",
    "    \"\"\"Demonstrate retrieval-augmented generation systems.\"\"\"\n",
    "    \n",
    "    print(\"Retrieval-Augmented Generation Systems:\")\n",
    "    \n",
    "    # Initialize components\n",
    "    embedding_model = EmbeddingModel(vocab_size=1000, d_model=128)\n",
    "    vector_db = VectorDatabase(d_model=128)\n",
    "    rag_pipeline = RAGPipeline(embedding_model, vector_db)\n",
    "    \n",
    "    # Sample knowledge base\n",
    "    knowledge_base = [\n",
    "        \"Transformers use self-attention to process sequences in parallel.\",\n",
    "        \"BERT is an encoder-only model trained with masked language modeling.\",\n",
    "        \"GPT models are decoder-only and generate text autoregressively.\",\n",
    "        \"Attention mechanisms allow models to focus on relevant input parts.\",\n",
    "        \"Large language models are trained on billions of text tokens.\",\n",
    "        \"Fine-tuning adapts pre-trained models to specific tasks.\",\n",
    "        \"RAG combines retrieval with generation for factual accuracy.\",\n",
    "        \"Vector databases enable efficient similarity search at scale.\",\n",
    "        \"Embeddings represent text as dense numerical vectors.\",\n",
    "        \"Context windows limit the amount of text models can process.\"\n",
    "    ]\n",
    "    \n",
    "    # Index documents\n",
    "    rag_pipeline.index_documents(knowledge_base)\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"How do transformers work?\",\n",
    "        \"What is the difference between BERT and GPT?\",\n",
    "        \"How does RAG improve language models?\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nKnowledge Base: {len(knowledge_base)} documents indexed\")\n",
    "    print(f\"Vector Database Stats: {vector_db.get_statistics()}\")\n",
    "    \n",
    "    # Process queries\n",
    "    results = []\n",
    "    for query in test_queries:\n",
    "        result = rag_pipeline.retrieve_and_generate(query, top_k=3)\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(f\"Top retrieved documents:\")\n",
    "        for i, (doc, sim) in enumerate(zip(result['retrieved_docs'], result['similarities'])):\n",
    "            print(f\"  {i+1}. {doc} (similarity: {sim:.3f})\")\n",
    "    \n",
    "    # Visualizations\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Embedding similarity matrix\n",
    "    plt.subplot(2, 3, 1)\n",
    "    \n",
    "    # Create similarity matrix for knowledge base\n",
    "    embeddings_matrix = np.array(vector_db.vectors)\n",
    "    similarity_matrix = embeddings_matrix @ embeddings_matrix.T\n",
    "    \n",
    "    plt.imshow(similarity_matrix, cmap='Blues', aspect='auto')\n",
    "    plt.title('Document Similarity Matrix')\n",
    "    plt.xlabel('Document Index')\n",
    "    plt.ylabel('Document Index')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Retrieval scores for different queries\n",
    "    plt.subplot(2, 3, 2)\n",
    "    \n",
    "    query_names = [f'Q{i+1}' for i in range(len(test_queries))]\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        similarities = result['similarities']\n",
    "        plt.plot(range(len(similarities)), similarities, 'o-', \n",
    "                label=query_names[i], alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Retrieved Document Rank')\n",
    "    plt.ylabel('Similarity Score')\n",
    "    plt.title('Retrieval Quality by Query')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Vector database performance\n",
    "    plt.subplot(2, 3, 3)\n",
    "    \n",
    "    db_sizes = [100, 1000, 10000, 100000, 1000000]\n",
    "    \n",
    "    # Simulate search times (linear for brute force, log for indexed)\n",
    "    brute_force_time = np.array(db_sizes) / 1000  # Linear\n",
    "    indexed_time = np.log(db_sizes) * 0.1         # Logarithmic\n",
    "    \n",
    "    plt.loglog(db_sizes, brute_force_time, 'r-', label='Brute Force', linewidth=2)\n",
    "    plt.loglog(db_sizes, indexed_time, 'b-', label='Indexed Search', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Database Size')\n",
    "    plt.ylabel('Search Time (ms)')\n",
    "    plt.title('Vector Search Scalability')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # RAG pipeline components\n",
    "    plt.subplot(2, 3, 4)\n",
    "    \n",
    "    pipeline_steps = ['Query\\nEncoding', 'Vector\\nSearch', 'Context\\nRanking', 'Response\\nGeneration']\n",
    "    latencies = [5, 15, 8, 50]  # Milliseconds\n",
    "    \n",
    "    bars = plt.bar(pipeline_steps, latencies, alpha=0.7)\n",
    "    plt.ylabel('Latency (ms)')\n",
    "    plt.title('RAG Pipeline Latency Breakdown')\n",
    "    \n",
    "    # Add cumulative latency\n",
    "    cumulative = np.cumsum(latencies)\n",
    "    for i, (bar, cum_lat) in enumerate(zip(bars, cumulative)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                f'{cum_lat}ms', ha='center', fontsize=8)\n",
    "    \n",
    "    # Embedding quality analysis\n",
    "    plt.subplot(2, 3, 5)\n",
    "    \n",
    "    # Simulate embedding quality metrics\n",
    "    embedding_dims = [64, 128, 256, 512, 1024]\n",
    "    retrieval_accuracy = [0.65, 0.72, 0.78, 0.82, 0.84]  # Accuracy improves with dimension\n",
    "    search_speed = [100, 80, 60, 40, 20]  # Speed decreases with dimension\n",
    "    \n",
    "    ax1 = plt.gca()\n",
    "    ax1.plot(embedding_dims, retrieval_accuracy, 'g-o', label='Accuracy', linewidth=2)\n",
    "    ax1.set_xlabel('Embedding Dimension')\n",
    "    ax1.set_ylabel('Retrieval Accuracy', color='g')\n",
    "    ax1.tick_params(axis='y', labelcolor='g')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(embedding_dims, search_speed, 'r-s', label='Speed', linewidth=2)\n",
    "    ax2.set_ylabel('Search Speed (relative)', color='r')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    \n",
    "    plt.title('Embedding Dimension Trade-offs')\n",
    "    \n",
    "    # RAG vs standard LM comparison\n",
    "    plt.subplot(2, 3, 6)\n",
    "    \n",
    "    metrics = ['Factual\\nAccuracy', 'Response\\nLatency', 'Knowledge\\nCoverage', 'Consistency']\n",
    "    standard_lm = [60, 90, 70, 85]\n",
    "    rag_system = [85, 60, 95, 80]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, standard_lm, width, label='Standard LM', alpha=0.7)\n",
    "    plt.bar(x + width/2, rag_system, width, label='RAG System', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('RAG vs Standard LM')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nRAG System Components:\")\n",
    "    \n",
    "    print(\"\\n1. Embedding Models:\")\n",
    "    print(\"   - Convert text to dense vectors\")\n",
    "    print(\"   - Capture semantic similarity\")\n",
    "    print(\"   - Enable efficient similarity search\")\n",
    "    \n",
    "    print(\"\\n2. Vector Databases:\")\n",
    "    print(\"   - Store and index embeddings\")\n",
    "    print(\"   - Fast approximate nearest neighbor search\")\n",
    "    print(\"   - Scalable to millions/billions of vectors\")\n",
    "    \n",
    "    print(\"\\n3. Retrieval Pipeline:\")\n",
    "    print(\"   - Query encoding\")\n",
    "    print(\"   - Similarity search\")\n",
    "    print(\"   - Result ranking and filtering\")\n",
    "    print(\"   - Context preparation\")\n",
    "    \n",
    "    print(\"\\n4. RAG Advantages:\")\n",
    "    print(\"   + Access to external knowledge\")\n",
    "    print(\"   + Better factual accuracy\")\n",
    "    print(\"   + Updatable knowledge without retraining\")\n",
    "    print(\"   + Reduced hallucination\")\n",
    "    \n",
    "    print(\"\\n5. RAG Challenges:\")\n",
    "    print(\"   - Additional latency from retrieval\")\n",
    "    print(\"   - Quality depends on retrieval accuracy\")\n",
    "    print(\"   - Context integration complexity\")\n",
    "    print(\"   - Maintaining embedding quality\")\n",
    "\n",
    "demonstrate_rag_systems()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
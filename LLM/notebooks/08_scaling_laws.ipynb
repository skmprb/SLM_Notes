{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 - Scaling Laws and Model Size\n",
    "\n",
    "This notebook explores the relationship between model size, compute, data, and performance in large language models.\n",
    "\n",
    "## Topics Covered:\n",
    "- Parameter count analysis\n",
    "- Model depth and width trade-offs\n",
    "- Context window considerations\n",
    "- Compute scaling relationships\n",
    "- Data scaling laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple\n",
    "import math\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameter Count Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ModelSizeAnalyzer:\n",
    "    \"\"\"Analyze parameter counts and model sizes.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_configs = {\n",
    "            'GPT-Small': {'layers': 12, 'd_model': 768, 'heads': 12, 'vocab': 50257},\n",
    "            'GPT-Medium': {'layers': 24, 'd_model': 1024, 'heads': 16, 'vocab': 50257},\n",
    "            'GPT-Large': {'layers': 36, 'd_model': 1280, 'heads': 20, 'vocab': 50257},\n",
    "            'GPT-XL': {'layers': 48, 'd_model': 1600, 'heads': 25, 'vocab': 50257},\n",
    "            'GPT-2': {'layers': 48, 'd_model': 1600, 'heads': 25, 'vocab': 50257},\n",
    "            'GPT-3': {'layers': 96, 'd_model': 12288, 'heads': 96, 'vocab': 50257}\n",
    "        }\n",
    "    \n",
    "    def calculate_parameters(self, layers: int, d_model: int, heads: int, \n",
    "                           vocab_size: int, context_length: int = 2048) -> Dict[str, int]:\n",
    "        \"\"\"Calculate parameter count breakdown for a transformer model.\"\"\"\n",
    "        \n",
    "        # Token embeddings\n",
    "        token_embeddings = vocab_size * d_model\n",
    "        \n",
    "        # Position embeddings (if learned)\n",
    "        position_embeddings = context_length * d_model\n",
    "        \n",
    "        # Per-layer parameters\n",
    "        # Multi-head attention: Q, K, V projections + output projection\n",
    "        attention_params = 4 * d_model * d_model\n",
    "        \n",
    "        # Feed-forward network (typically 4x expansion)\n",
    "        d_ff = 4 * d_model\n",
    "        ffn_params = d_model * d_ff + d_ff * d_model  # Two linear layers\n",
    "        \n",
    "        # Layer normalization (2 per layer: pre-attention and pre-ffn)\n",
    "        layernorm_params = 2 * 2 * d_model  # 2 layers * 2 params (scale, bias)\n",
    "        \n",
    "        # Total per layer\n",
    "        per_layer_params = attention_params + ffn_params + layernorm_params\n",
    "        \n",
    "        # Total transformer layers\n",
    "        transformer_params = layers * per_layer_params\n",
    "        \n",
    "        # Output layer (language modeling head)\n",
    "        output_params = d_model * vocab_size\n",
    "        \n",
    "        # Final layer norm\n",
    "        final_layernorm = 2 * d_model\n",
    "        \n",
    "        total_params = (\n",
    "            token_embeddings + position_embeddings + transformer_params + \n",
    "            output_params + final_layernorm\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'token_embeddings': token_embeddings,\n",
    "            'position_embeddings': position_embeddings,\n",
    "            'attention_params': layers * attention_params,\n",
    "            'ffn_params': layers * ffn_params,\n",
    "            'layernorm_params': layers * layernorm_params + final_layernorm,\n",
    "            'output_params': output_params,\n",
    "            'total_params': total_params\n",
    "        }\n",
    "    \n",
    "    def analyze_scaling_trends(self) -> Dict[str, List]:\n",
    "        \"\"\"Analyze parameter scaling trends across model sizes.\"\"\"\n",
    "        results = {\n",
    "            'model_names': [],\n",
    "            'total_params': [],\n",
    "            'layers': [],\n",
    "            'd_model': [],\n",
    "            'attention_ratio': [],\n",
    "            'ffn_ratio': []\n",
    "        }\n",
    "        \n",
    "        for name, config in self.model_configs.items():\n",
    "            params = self.calculate_parameters(**config)\n",
    "            \n",
    "            results['model_names'].append(name)\n",
    "            results['total_params'].append(params['total_params'])\n",
    "            results['layers'].append(config['layers'])\n",
    "            results['d_model'].append(config['d_model'])\n",
    "            \n",
    "            # Calculate ratios\n",
    "            total = params['total_params']\n",
    "            results['attention_ratio'].append(params['attention_params'] / total)\n",
    "            results['ffn_ratio'].append(params['ffn_params'] / total)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def memory_requirements(self, total_params: int, precision: str = 'fp16') -> Dict[str, float]:\n",
    "        \"\"\"Calculate memory requirements for different scenarios.\"\"\"\n",
    "        \n",
    "        # Bytes per parameter based on precision\n",
    "        bytes_per_param = {\n",
    "            'fp32': 4,\n",
    "            'fp16': 2,\n",
    "            'int8': 1,\n",
    "            'int4': 0.5\n",
    "        }\n",
    "        \n",
    "        param_bytes = bytes_per_param[precision]\n",
    "        \n",
    "        # Model weights\n",
    "        model_memory = total_params * param_bytes\n",
    "        \n",
    "        # Training memory (approximate)\n",
    "        # Gradients + optimizer states (Adam: 2x params) + activations\n",
    "        training_memory = model_memory * 4  # Rough estimate\n",
    "        \n",
    "        # Inference memory (model + activations)\n",
    "        inference_memory = model_memory * 1.2  # Rough estimate\n",
    "        \n",
    "        return {\n",
    "            'model_gb': model_memory / (1024**3),\n",
    "            'training_gb': training_memory / (1024**3),\n",
    "            'inference_gb': inference_memory / (1024**3)\n",
    "        }\n",
    "\n",
    "def demonstrate_parameter_analysis():\n",
    "    \"\"\"Demonstrate parameter count analysis.\"\"\"\n",
    "    \n",
    "    analyzer = ModelSizeAnalyzer()\n",
    "    \n",
    "    # Analyze a specific model configuration\n",
    "    config = {'layers': 24, 'd_model': 1024, 'heads': 16, 'vocab': 50257}\n",
    "    params = analyzer.calculate_parameters(**config)\n",
    "    \n",
    "    print(\"Parameter Breakdown for GPT-Medium:\")\n",
    "    for component, count in params.items():\n",
    "        percentage = (count / params['total_params']) * 100\n",
    "        print(f\"  {component}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Analyze scaling trends\n",
    "    trends = analyzer.analyze_scaling_trends()\n",
    "    \n",
    "    print(f\"\\nModel Scaling Analysis:\")\n",
    "    for i, name in enumerate(trends['model_names']):\n",
    "        params_m = trends['total_params'][i] / 1e6\n",
    "        print(f\"  {name}: {params_m:.1f}M parameters\")\n",
    "    \n",
    "    # Memory analysis\n",
    "    gpt3_params = trends['total_params'][-1]  # GPT-3\n",
    "    memory_fp16 = analyzer.memory_requirements(gpt3_params, 'fp16')\n",
    "    memory_fp32 = analyzer.memory_requirements(gpt3_params, 'fp32')\n",
    "    \n",
    "    print(f\"\\nGPT-3 Memory Requirements:\")\n",
    "    print(f\"  FP16 - Model: {memory_fp16['model_gb']:.1f}GB, Training: {memory_fp16['training_gb']:.1f}GB\")\n",
    "    print(f\"  FP32 - Model: {memory_fp32['model_gb']:.1f}GB, Training: {memory_fp32['training_gb']:.1f}GB\")\n",
    "    \n",
    "    # Visualize parameter scaling\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Parameter count scaling\n",
    "    plt.subplot(3, 3, 1)\n",
    "    params_millions = [p / 1e6 for p in trends['total_params']]\n",
    "    plt.semilogy(trends['model_names'], params_millions, 'o-', linewidth=2, markersize=8)\n",
    "    plt.title('Parameter Count Scaling')\n",
    "    plt.ylabel('Parameters (Millions)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Parameter breakdown for GPT-Medium\n",
    "    plt.subplot(3, 3, 2)\n",
    "    components = ['Token Emb', 'Position Emb', 'Attention', 'FFN', 'LayerNorm', 'Output']\n",
    "    values = [\n",
    "        params['token_embeddings'],\n",
    "        params['position_embeddings'],\n",
    "        params['attention_params'],\n",
    "        params['ffn_params'],\n",
    "        params['layernorm_params'],\n",
    "        params['output_params']\n",
    "    ]\n",
    "    \n",
    "    plt.pie(values, labels=components, autopct='%1.1f%%')\n",
    "    plt.title('Parameter Distribution\\n(GPT-Medium)')\n",
    "    \n",
    "    # Layers vs d_model scaling\n",
    "    plt.subplot(3, 3, 3)\n",
    "    plt.scatter(trends['layers'], trends['d_model'], s=[p/1e6 for p in trends['total_params']], \n",
    "               alpha=0.7, c=range(len(trends['model_names'])), cmap='viridis')\n",
    "    \n",
    "    for i, name in enumerate(trends['model_names']):\n",
    "        plt.annotate(name, (trends['layers'][i], trends['d_model'][i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    plt.xlabel('Number of Layers')\n",
    "    plt.ylabel('Model Dimension')\n",
    "    plt.title('Architecture Scaling\\n(Bubble size = Parameters)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Memory requirements comparison\n",
    "    plt.subplot(3, 3, 4)\n",
    "    precisions = ['fp32', 'fp16', 'int8', 'int4']\n",
    "    model_memory = []\n",
    "    training_memory = []\n",
    "    \n",
    "    for precision in precisions:\n",
    "        mem = analyzer.memory_requirements(gpt3_params, precision)\n",
    "        model_memory.append(mem['model_gb'])\n",
    "        training_memory.append(mem['training_gb'])\n",
    "    \n",
    "    x = np.arange(len(precisions))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, model_memory, width, label='Model', alpha=0.7)\n",
    "    plt.bar(x + width/2, training_memory, width, label='Training', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Memory (GB)')\n",
    "    plt.title('Memory vs Precision\\n(GPT-3 Scale)')\n",
    "    plt.xticks(x, precisions)\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # Attention vs FFN parameter ratio\n",
    "    plt.subplot(3, 3, 5)\n",
    "    plt.plot(trends['model_names'], trends['attention_ratio'], 'o-', label='Attention', alpha=0.7)\n",
    "    plt.plot(trends['model_names'], trends['ffn_ratio'], 's-', label='FFN', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Parameter Ratio')\n",
    "    plt.title('Component Parameter Ratios')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Parameter efficiency analysis\n",
    "    plt.subplot(3, 3, 6)\n",
    "    \n",
    "    # Calculate parameters per layer\n",
    "    params_per_layer = [p / l for p, l in zip(trends['total_params'], trends['layers'])]\n",
    "    \n",
    "    plt.scatter(trends['layers'], params_per_layer, alpha=0.7)\n",
    "    \n",
    "    for i, name in enumerate(trends['model_names']):\n",
    "        plt.annotate(name, (trends['layers'][i], params_per_layer[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    plt.xlabel('Number of Layers')\n",
    "    plt.ylabel('Parameters per Layer')\n",
    "    plt.title('Parameter Efficiency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Context length impact\n",
    "    plt.subplot(3, 3, 7)\n",
    "    \n",
    "    context_lengths = [512, 1024, 2048, 4096, 8192]\n",
    "    base_config = {'layers': 24, 'd_model': 1024, 'heads': 16, 'vocab': 50257}\n",
    "    \n",
    "    total_params = []\n",
    "    pos_emb_params = []\n",
    "    \n",
    "    for ctx_len in context_lengths:\n",
    "        params = analyzer.calculate_parameters(**base_config, context_length=ctx_len)\n",
    "        total_params.append(params['total_params'] / 1e6)\n",
    "        pos_emb_params.append(params['position_embeddings'] / 1e6)\n",
    "    \n",
    "    plt.plot(context_lengths, total_params, 'o-', label='Total Parameters', alpha=0.7)\n",
    "    plt.plot(context_lengths, pos_emb_params, 's-', label='Position Embeddings', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Context Length')\n",
    "    plt.ylabel('Parameters (Millions)')\n",
    "    plt.title('Context Length Impact')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Model size vs compute relationship\n",
    "    plt.subplot(3, 3, 8)\n",
    "    \n",
    "    # Approximate FLOPs for forward pass (simplified)\n",
    "    def estimate_flops(params, seq_len=2048):\n",
    "        # Very rough estimate: 2 * params * seq_len\n",
    "        return 2 * params * seq_len\n",
    "    \n",
    "    flops = [estimate_flops(p) / 1e12 for p in trends['total_params']]  # TFLOPs\n",
    "    \n",
    "    plt.loglog(params_millions, flops, 'o-', alpha=0.7)\n",
    "    \n",
    "    for i, name in enumerate(trends['model_names']):\n",
    "        plt.annotate(name, (params_millions[i], flops[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    plt.xlabel('Parameters (Millions)')\n",
    "    plt.ylabel('FLOPs per Forward Pass (TFLOPs)')\n",
    "    plt.title('Parameters vs Compute')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Architecture design space\n",
    "    plt.subplot(3, 3, 9)\n",
    "    \n",
    "    # Show different ways to achieve similar parameter counts\n",
    "    target_params = 350e6  # 350M parameters\n",
    "    \n",
    "    layer_options = np.arange(12, 49, 4)\n",
    "    d_model_options = []\n",
    "    \n",
    "    for layers in layer_options:\n",
    "        # Approximate d_model needed for target parameters\n",
    "        # Simplified: total_params ≈ layers * 12 * d_model^2\n",
    "        d_model = int(np.sqrt(target_params / (layers * 12)))\n",
    "        d_model_options.append(d_model)\n",
    "    \n",
    "    plt.plot(layer_options, d_model_options, 'o-', alpha=0.7)\n",
    "    plt.xlabel('Number of Layers')\n",
    "    plt.ylabel('Model Dimension')\n",
    "    plt.title(f'Design Space for\\n{target_params/1e6:.0f}M Parameters')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nKey Insights:\")\n",
    "    print(\"\\n1. Parameter Distribution:\")\n",
    "    print(\"   - FFN parameters dominate (typically 60-70%)\")\n",
    "    print(\"   - Attention parameters are significant (20-30%)\")\n",
    "    print(\"   - Embeddings become less significant as models grow\")\n",
    "    \n",
    "    print(\"\\n2. Scaling Patterns:\")\n",
    "    print(\"   - Both depth and width contribute to capacity\")\n",
    "    print(\"   - Deeper models may be more parameter-efficient\")\n",
    "    print(\"   - Context length has linear impact on position embeddings\")\n",
    "    \n",
    "    print(\"\\n3. Memory Considerations:\")\n",
    "    print(\"   - Training requires ~4x model memory\")\n",
    "    print(\"   - Precision choice significantly impacts memory\")\n",
    "    print(\"   - Quantization can reduce memory by 2-8x\")\n",
    "\n",
    "demonstrate_parameter_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scaling Laws Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ScalingLaws:\n",
    "    \"\"\"Implementation of neural scaling laws.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Empirical constants from scaling law research\n",
    "        self.constants = {\n",
    "            'A': 1.0,      # Scale factor\n",
    "            'alpha': 0.076, # Parameter scaling exponent\n",
    "            'beta': 0.095,  # Data scaling exponent\n",
    "            'gamma': 0.5,   # Compute scaling exponent\n",
    "            'E': 1.69       # Irreducible loss\n",
    "        }\n",
    "    \n",
    "    def loss_vs_parameters(self, N: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Loss as a function of parameter count (with fixed data).\"\"\"\n",
    "        A, alpha, E = self.constants['A'], self.constants['alpha'], self.constants['E']\n",
    "        return A * (N ** -alpha) + E\n",
    "    \n",
    "    def loss_vs_data(self, D: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Loss as a function of dataset size (with fixed parameters).\"\"\"\n",
    "        A, beta, E = self.constants['A'], self.constants['beta'], self.constants['E']\n",
    "        return A * (D ** -beta) + E\n",
    "    \n",
    "    def loss_vs_compute(self, C: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Loss as a function of compute (FLOPs).\"\"\"\n",
    "        A, gamma, E = self.constants['A'], self.constants['gamma'], self.constants['E']\n",
    "        return A * (C ** -gamma) + E\n",
    "    \n",
    "    def optimal_allocation(self, compute_budget: float) -> Tuple[float, float]:\n",
    "        \"\"\"Find optimal parameter count and data size for given compute budget.\"\"\"\n",
    "        # Chinchilla scaling: optimal ratio between parameters and data\n",
    "        # Simplified relationship: D_optimal ≈ 20 * N_optimal\n",
    "        \n",
    "        # Approximate relationship between compute, parameters, and data\n",
    "        # C ≈ 6 * N * D (forward pass) + training overhead\n",
    "        \n",
    "        # For optimal allocation: N_optimal ≈ (C / 120)^0.5\n",
    "        N_optimal = (compute_budget / 120) ** 0.5\n",
    "        D_optimal = 20 * N_optimal\n",
    "        \n",
    "        return N_optimal, D_optimal\n",
    "    \n",
    "    def compute_requirements(self, N: float, D: float, epochs: int = 1) -> float:\n",
    "        \"\"\"Estimate compute requirements for training.\"\"\"\n",
    "        # Simplified: 6 FLOPs per parameter per token (forward + backward)\n",
    "        flops_per_token = 6 * N\n",
    "        total_compute = flops_per_token * D * epochs\n",
    "        return total_compute\n",
    "    \n",
    "    def performance_prediction(self, N: float, D: float, C: float) -> float:\n",
    "        \"\"\"Predict performance given parameters, data, and compute.\"\"\"\n",
    "        # Combined scaling law (simplified)\n",
    "        A, alpha, beta, E = self.constants['A'], self.constants['alpha'], self.constants['beta'], self.constants['E']\n",
    "        \n",
    "        # Weighted combination of parameter and data scaling\n",
    "        param_term = A * (N ** -alpha)\n",
    "        data_term = A * (D ** -beta)\n",
    "        \n",
    "        # Take the maximum (bottleneck)\n",
    "        loss = max(param_term, data_term) + E\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def demonstrate_scaling_laws():\n",
    "    \"\"\"Demonstrate neural scaling laws.\"\"\"\n",
    "    \n",
    "    scaling = ScalingLaws()\n",
    "    \n",
    "    # Parameter ranges for analysis\n",
    "    N_range = np.logspace(6, 11, 50)  # 1M to 100B parameters\n",
    "    D_range = np.logspace(8, 13, 50)  # 100M to 10T tokens\n",
    "    C_range = np.logspace(18, 25, 50) # 1e18 to 1e25 FLOPs\n",
    "    \n",
    "    # Calculate losses\n",
    "    loss_N = scaling.loss_vs_parameters(N_range)\n",
    "    loss_D = scaling.loss_vs_data(D_range)\n",
    "    loss_C = scaling.loss_vs_compute(C_range)\n",
    "    \n",
    "    print(\"Scaling Laws Analysis:\")\n",
    "    \n",
    "    # Example predictions\n",
    "    example_models = [\n",
    "        {'name': 'GPT-2', 'params': 1.5e9, 'data': 40e9},\n",
    "        {'name': 'GPT-3', 'params': 175e9, 'data': 300e9},\n",
    "        {'name': 'Hypothetical', 'params': 1e12, 'data': 2e13}\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nModel Performance Predictions:\")\n",
    "    for model in example_models:\n",
    "        N, D = model['params'], model['data']\n",
    "        C = scaling.compute_requirements(N, D)\n",
    "        loss = scaling.performance_prediction(N, D, C)\n",
    "        \n",
    "        print(f\"  {model['name']}:\")\n",
    "        print(f\"    Parameters: {N/1e9:.1f}B\")\n",
    "        print(f\"    Data: {D/1e9:.1f}B tokens\")\n",
    "        print(f\"    Compute: {C:.2e} FLOPs\")\n",
    "        print(f\"    Predicted Loss: {loss:.3f}\")\n",
    "    \n",
    "    # Optimal allocation analysis\n",
    "    compute_budgets = [1e21, 1e22, 1e23, 1e24]  # Different compute budgets\n",
    "    \n",
    "    print(\"\\nOptimal Resource Allocation:\")\n",
    "    for budget in compute_budgets:\n",
    "        N_opt, D_opt = scaling.optimal_allocation(budget)\n",
    "        print(f\"  Budget {budget:.0e} FLOPs:\")\n",
    "        print(f\"    Optimal Parameters: {N_opt/1e9:.1f}B\")\n",
    "        print(f\"    Optimal Data: {D_opt/1e9:.1f}B tokens\")\n",
    "    \n",
    "    # Visualize scaling laws\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss vs Parameters\n",
    "    plt.subplot(3, 3, 1)\n",
    "    plt.loglog(N_range / 1e9, loss_N, 'b-', linewidth=2, label='Scaling Law')\n",
    "    \n",
    "    # Add example models\n",
    "    for model in example_models:\n",
    "        N = model['params']\n",
    "        loss = scaling.loss_vs_parameters(np.array([N]))[0]\n",
    "        plt.loglog(N / 1e9, loss, 'ro', markersize=8)\n",
    "        plt.annotate(model['name'], (N / 1e9, loss), xytext=(5, 5), \n",
    "                    textcoords='offset points')\n",
    "    \n",
    "    plt.xlabel('Parameters (Billions)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs Parameters')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss vs Data\n",
    "    plt.subplot(3, 3, 2)\n",
    "    plt.loglog(D_range / 1e9, loss_D, 'g-', linewidth=2, label='Scaling Law')\n",
    "    \n",
    "    for model in example_models:\n",
    "        D = model['data']\n",
    "        loss = scaling.loss_vs_data(np.array([D]))[0]\n",
    "        plt.loglog(D / 1e9, loss, 'ro', markersize=8)\n",
    "        plt.annotate(model['name'], (D / 1e9, loss), xytext=(5, 5), \n",
    "                    textcoords='offset points')\n",
    "    \n",
    "    plt.xlabel('Training Data (Billion Tokens)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs Training Data')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss vs Compute\n",
    "    plt.subplot(3, 3, 3)\n",
    "    plt.loglog(C_range, loss_C, 'r-', linewidth=2, label='Scaling Law')\n",
    "    \n",
    "    for model in example_models:\n",
    "        N, D = model['params'], model['data']\n",
    "        C = scaling.compute_requirements(N, D)\n",
    "        loss = scaling.loss_vs_compute(np.array([C]))[0]\n",
    "        plt.loglog(C, loss, 'ro', markersize=8)\n",
    "        plt.annotate(model['name'], (C, loss), xytext=(5, 5), \n",
    "                    textcoords='offset points')\n",
    "    \n",
    "    plt.xlabel('Compute (FLOPs)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs Compute')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Optimal allocation frontier\n",
    "    plt.subplot(3, 3, 4)\n",
    "    \n",
    "    compute_range = np.logspace(20, 24, 20)\n",
    "    optimal_N = []\n",
    "    optimal_D = []\n",
    "    \n",
    "    for C in compute_range:\n",
    "        N_opt, D_opt = scaling.optimal_allocation(C)\n",
    "        optimal_N.append(N_opt)\n",
    "        optimal_D.append(D_opt)\n",
    "    \n",
    "    plt.loglog(optimal_N, optimal_D, 'b-', linewidth=2, label='Optimal Frontier')\n",
    "    \n",
    "    # Add example models\n",
    "    for model in example_models:\n",
    "        N, D = model['params'], model['data']\n",
    "        plt.loglog(N, D, 'ro', markersize=8)\n",
    "        plt.annotate(model['name'], (N, D), xytext=(5, 5), \n",
    "                    textcoords='offset points')\n",
    "    \n",
    "    plt.xlabel('Parameters')\n",
    "    plt.ylabel('Training Tokens')\n",
    "    plt.title('Optimal Parameter-Data Allocation')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Compute efficiency analysis\n",
    "    plt.subplot(3, 3, 5)\n",
    "    \n",
    "    # Show how loss improves with compute for different allocation strategies\n",
    "    compute_budgets = np.logspace(20, 24, 20)\n",
    "    \n",
    "    # Optimal allocation\n",
    "    optimal_losses = []\n",
    "    for C in compute_budgets:\n",
    "        N_opt, D_opt = scaling.optimal_allocation(C)\n",
    "        loss = scaling.performance_prediction(N_opt, D_opt, C)\n",
    "        optimal_losses.append(loss)\n",
    "    \n",
    "    # Suboptimal: only scaling parameters\n",
    "    param_only_losses = []\n",
    "    fixed_data = 1e11  # Fixed 100B tokens\n",
    "    for C in compute_budgets:\n",
    "        # Estimate max parameters for this compute budget\n",
    "        N_max = C / (6 * fixed_data)\n",
    "        loss = scaling.performance_prediction(N_max, fixed_data, C)\n",
    "        param_only_losses.append(loss)\n",
    "    \n",
    "    plt.loglog(compute_budgets, optimal_losses, 'b-', linewidth=2, label='Optimal Allocation')\n",
    "    plt.loglog(compute_budgets, param_only_losses, 'r--', linewidth=2, label='Parameters Only')\n",
    "    \n",
    "    plt.xlabel('Compute Budget (FLOPs)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Allocation Strategy Comparison')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Training efficiency over time\n",
    "    plt.subplot(3, 3, 6)\n",
    "    \n",
    "    # Simulate training progress\n",
    "    epochs = np.linspace(0.1, 3, 30)\n",
    "    base_loss = 2.5\n",
    "    \n",
    "    # Different model sizes\n",
    "    model_sizes = [1e9, 10e9, 100e9]  # 1B, 10B, 100B parameters\n",
    "    \n",
    "    for i, N in enumerate(model_sizes):\n",
    "        # Simulate loss decay during training\n",
    "        losses = base_loss * np.exp(-epochs * 0.5) + scaling.constants['E']\n",
    "        # Adjust for model size\n",
    "        losses = losses * (1e9 / N) ** 0.1\n",
    "        \n",
    "        plt.plot(epochs, losses, label=f'{N/1e9:.0f}B params', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Training Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Progress by Model Size')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Scaling exponents visualization\n",
    "    plt.subplot(3, 3, 7)\n",
    "    \n",
    "    exponents = ['α (Parameters)', 'β (Data)', 'γ (Compute)']\n",
    "    values = [scaling.constants['alpha'], scaling.constants['beta'], scaling.constants['gamma']]\n",
    "    \n",
    "    bars = plt.bar(exponents, values, alpha=0.7)\n",
    "    plt.title('Scaling Exponents')\n",
    "    plt.ylabel('Exponent Value')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "                f'{val:.3f}', ha='center')\n",
    "    \n",
    "    # Performance vs cost trade-off\n",
    "    plt.subplot(3, 3, 8)\n",
    "    \n",
    "    # Different model configurations\n",
    "    configs = [\n",
    "        {'name': 'Small', 'params': 1e8, 'data': 1e10},\n",
    "        {'name': 'Medium', 'params': 1e9, 'data': 1e11},\n",
    "        {'name': 'Large', 'params': 1e10, 'data': 1e12},\n",
    "        {'name': 'XL', 'params': 1e11, 'data': 1e13}\n",
    "    ]\n",
    "    \n",
    "    costs = []\n",
    "    performances = []\n",
    "    \n",
    "    for config in configs:\n",
    "        N, D = config['params'], config['data']\n",
    "        cost = scaling.compute_requirements(N, D) / 1e21  # Normalize\n",
    "        performance = 1 / scaling.performance_prediction(N, D, cost * 1e21)  # Inverse loss\n",
    "        \n",
    "        costs.append(cost)\n",
    "        performances.append(performance)\n",
    "    \n",
    "    plt.scatter(costs, performances, s=100, alpha=0.7)\n",
    "    \n",
    "    for i, config in enumerate(configs):\n",
    "        plt.annotate(config['name'], (costs[i], performances[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.xlabel('Relative Training Cost')\n",
    "    plt.ylabel('Performance (1/Loss)')\n",
    "    plt.title('Performance vs Cost Trade-off')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Future projections\n",
    "    plt.subplot(3, 3, 9)\n",
    "    \n",
    "    # Project future model capabilities\n",
    "    years = np.arange(2020, 2031)\n",
    "    \n",
    "    # Assume compute grows exponentially (Moore's law-like)\n",
    "    compute_growth = 1e21 * (2 ** ((years - 2020) / 2))  # Double every 2 years\n",
    "    \n",
    "    projected_losses = []\n",
    "    for C in compute_growth:\n",
    "        N_opt, D_opt = scaling.optimal_allocation(C)\n",
    "        loss = scaling.performance_prediction(N_opt, D_opt, C)\n",
    "        projected_losses.append(loss)\n",
    "    \n",
    "    plt.plot(years, projected_losses, 'o-', linewidth=2)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Projected Loss')\n",
    "    plt.title('Future Performance Projections')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nScaling Laws Insights:\")\n",
    "    \n",
    "    print(\"\\n1. Power Law Relationships:\")\n",
    "    print(f\"   - Loss ∝ N^(-{scaling.constants['alpha']:.3f}) for parameters\")\n",
    "    print(f\"   - Loss ∝ D^(-{scaling.constants['beta']:.3f}) for data\")\n",
    "    print(f\"   - Loss ∝ C^(-{scaling.constants['gamma']:.3f}) for compute\")\n",
    "    \n",
    "    print(\"\\n2. Optimal Allocation (Chinchilla):\")\n",
    "    print(\"   - Data should scale ~20x faster than parameters\")\n",
    "    print(\"   - Many models are undertrained (too few tokens)\")\n",
    "    print(\"   - Compute-optimal models are smaller but see more data\")\n",
    "    \n",
    "    print(\"\\n3. Practical Implications:\")\n",
    "    print(\"   - Doubling compute improves loss by ~30%\")\n",
    "    print(\"   - Data quality matters as much as quantity\")\n",
    "    print(\"   - Diminishing returns require exponential resources\")\n",
    "\n",
    "demonstrate_scaling_laws()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
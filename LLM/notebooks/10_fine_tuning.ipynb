{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Fine-Tuning Techniques\n",
    "\n",
    "This notebook covers various fine-tuning approaches for language models.\n",
    "\n",
    "## Topics Covered:\n",
    "- Supervised fine-tuning\n",
    "- Instruction tuning\n",
    "- Parameter-efficient fine-tuning (LoRA, Adapters, Prefix tuning)\n",
    "- Continual learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameter-Efficient Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class LoRA:\n",
    "    \"\"\"Low-Rank Adaptation implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, rank: int, alpha: float = 1.0):\n",
    "        self.d_model = d_model\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # LoRA matrices: W = W0 + (alpha/rank) * B * A\n",
    "        self.A = np.random.randn(rank, d_model) * 0.01\n",
    "        self.B = np.zeros((d_model, rank))\n",
    "        \n",
    "        # Original frozen weights\n",
    "        self.W0 = np.random.randn(d_model, d_model) * 0.1\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass with LoRA adaptation.\"\"\"\n",
    "        # Original transformation\n",
    "        original_output = x @ self.W0\n",
    "        \n",
    "        # LoRA adaptation\n",
    "        lora_output = x @ self.B @ self.A\n",
    "        \n",
    "        # Combined output\n",
    "        return original_output + (self.alpha / self.rank) * lora_output\n",
    "    \n",
    "    def get_trainable_params(self) -> int:\n",
    "        \"\"\"Number of trainable parameters.\"\"\"\n",
    "        return self.A.size + self.B.size\n",
    "    \n",
    "    def get_total_params(self) -> int:\n",
    "        \"\"\"Total parameters including frozen.\"\"\"\n",
    "        return self.W0.size + self.get_trainable_params()\n",
    "\n",
    "class Adapter:\n",
    "    \"\"\"Adapter layer implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, bottleneck_size: int):\n",
    "        self.d_model = d_model\n",
    "        self.bottleneck_size = bottleneck_size\n",
    "        \n",
    "        # Adapter weights\n",
    "        self.W_down = np.random.randn(d_model, bottleneck_size) * 0.1\n",
    "        self.W_up = np.random.randn(bottleneck_size, d_model) * 0.1\n",
    "        self.b_down = np.zeros(bottleneck_size)\n",
    "        self.b_up = np.zeros(d_model)\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass through adapter.\"\"\"\n",
    "        # Down projection\n",
    "        hidden = np.maximum(0, x @ self.W_down + self.b_down)  # ReLU\n",
    "        \n",
    "        # Up projection\n",
    "        adapter_output = hidden @ self.W_up + self.b_up\n",
    "        \n",
    "        # Residual connection\n",
    "        return x + adapter_output\n",
    "    \n",
    "    def get_trainable_params(self) -> int:\n",
    "        \"\"\"Number of trainable parameters.\"\"\"\n",
    "        return (self.W_down.size + self.W_up.size + \n",
    "                self.b_down.size + self.b_up.size)\n",
    "\n",
    "class PrefixTuning:\n",
    "    \"\"\"Prefix tuning implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, prefix_length: int, num_layers: int):\n",
    "        self.d_model = d_model\n",
    "        self.prefix_length = prefix_length\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Learnable prefix parameters for each layer\n",
    "        self.prefix_keys = np.random.randn(num_layers, prefix_length, d_model) * 0.1\n",
    "        self.prefix_values = np.random.randn(num_layers, prefix_length, d_model) * 0.1\n",
    "    \n",
    "    def get_prefix_kv(self, layer_idx: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Get prefix key-value pairs for a specific layer.\"\"\"\n",
    "        return self.prefix_keys[layer_idx], self.prefix_values[layer_idx]\n",
    "    \n",
    "    def forward_with_prefix(self, x: np.ndarray, layer_idx: int) -> np.ndarray:\n",
    "        \"\"\"Forward pass with prefix conditioning.\"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Get prefix for this layer\n",
    "        prefix_k, prefix_v = self.get_prefix_kv(layer_idx)\n",
    "        \n",
    "        # Expand prefix for batch\n",
    "        prefix_k_batch = np.tile(prefix_k[None, :, :], (batch_size, 1, 1))\n",
    "        prefix_v_batch = np.tile(prefix_v[None, :, :], (batch_size, 1, 1))\n",
    "        \n",
    "        # Concatenate prefix with input\n",
    "        extended_keys = np.concatenate([prefix_k_batch, x], axis=1)\n",
    "        extended_values = np.concatenate([prefix_v_batch, x], axis=1)\n",
    "        \n",
    "        # Simplified attention computation\n",
    "        attention_weights = np.random.rand(batch_size, seq_len, seq_len + self.prefix_length)\n",
    "        attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n",
    "        \n",
    "        output = attention_weights @ extended_values\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_trainable_params(self) -> int:\n",
    "        \"\"\"Number of trainable parameters.\"\"\"\n",
    "        return self.prefix_keys.size + self.prefix_values.size\n",
    "\n",
    "def demonstrate_peft_methods():\n",
    "    \"\"\"Demonstrate parameter-efficient fine-tuning methods.\"\"\"\n",
    "    \n",
    "    d_model = 512\n",
    "    batch_size, seq_len = 4, 16\n",
    "    \n",
    "    # Sample input\n",
    "    x = np.random.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Initialize PEFT methods\n",
    "    lora = LoRA(d_model=d_model, rank=16, alpha=16)\n",
    "    adapter = Adapter(d_model=d_model, bottleneck_size=64)\n",
    "    prefix_tuning = PrefixTuning(d_model=d_model, prefix_length=10, num_layers=12)\n",
    "    \n",
    "    print(\"Parameter-Efficient Fine-Tuning Comparison:\")\n",
    "    \n",
    "    # LoRA\n",
    "    lora_output = lora.forward(x.reshape(-1, d_model)).reshape(batch_size, seq_len, d_model)\n",
    "    lora_params = lora.get_trainable_params()\n",
    "    lora_total = lora.get_total_params()\n",
    "    \n",
    "    print(f\"\\nLoRA (rank={lora.rank}):\")\n",
    "    print(f\"  Trainable params: {lora_params:,}\")\n",
    "    print(f\"  Total params: {lora_total:,}\")\n",
    "    print(f\"  Efficiency: {lora_params/lora_total*100:.2f}% trainable\")\n",
    "    \n",
    "    # Adapter\n",
    "    adapter_output = adapter.forward(x)\n",
    "    adapter_params = adapter.get_trainable_params()\n",
    "    \n",
    "    print(f\"\\nAdapter (bottleneck={adapter.bottleneck_size}):\")\n",
    "    print(f\"  Trainable params: {adapter_params:,}\")\n",
    "    print(f\"  Added per layer: {adapter_params:,}\")\n",
    "    \n",
    "    # Prefix Tuning\n",
    "    prefix_output = prefix_tuning.forward_with_prefix(x, layer_idx=0)\n",
    "    prefix_params = prefix_tuning.get_trainable_params()\n",
    "    \n",
    "    print(f\"\\nPrefix Tuning (length={prefix_tuning.prefix_length}):\")\n",
    "    print(f\"  Trainable params: {prefix_params:,}\")\n",
    "    print(f\"  Params per layer: {prefix_params // prefix_tuning.num_layers:,}\")\n",
    "    \n",
    "    # Full fine-tuning comparison\n",
    "    full_model_params = 12 * d_model * d_model * 4  # Approximate transformer\n",
    "    \n",
    "    print(f\"\\nFull Fine-tuning:\")\n",
    "    print(f\"  All params trainable: {full_model_params:,}\")\n",
    "    \n",
    "    # Visualize efficiency\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Parameter efficiency comparison\n",
    "    plt.subplot(2, 3, 1)\n",
    "    \n",
    "    methods = ['Full FT', 'LoRA', 'Adapter', 'Prefix']\n",
    "    trainable_params = [\n",
    "        full_model_params,\n",
    "        lora_params,\n",
    "        adapter_params * 12,  # 12 layers\n",
    "        prefix_params\n",
    "    ]\n",
    "    \n",
    "    plt.bar(methods, trainable_params, alpha=0.7)\n",
    "    plt.ylabel('Trainable Parameters')\n",
    "    plt.title('Parameter Efficiency')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # Add efficiency percentages\n",
    "    for i, (method, params) in enumerate(zip(methods, trainable_params)):\n",
    "        efficiency = params / full_model_params * 100\n",
    "        plt.text(i, params, f'{efficiency:.2f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # LoRA rank analysis\n",
    "    plt.subplot(2, 3, 2)\n",
    "    \n",
    "    ranks = [4, 8, 16, 32, 64]\n",
    "    lora_params_by_rank = []\n",
    "    \n",
    "    for rank in ranks:\n",
    "        params = 2 * rank * d_model  # A and B matrices\n",
    "        lora_params_by_rank.append(params)\n",
    "    \n",
    "    plt.plot(ranks, lora_params_by_rank, 'o-', linewidth=2)\n",
    "    plt.xlabel('LoRA Rank')\n",
    "    plt.ylabel('Parameters per Layer')\n",
    "    plt.title('LoRA Rank vs Parameters')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Adapter bottleneck analysis\n",
    "    plt.subplot(2, 3, 3)\n",
    "    \n",
    "    bottlenecks = [32, 64, 128, 256, 512]\n",
    "    adapter_params_by_bottleneck = []\n",
    "    \n",
    "    for bottleneck in bottlenecks:\n",
    "        params = 2 * d_model * bottleneck + d_model + bottleneck\n",
    "        adapter_params_by_bottleneck.append(params)\n",
    "    \n",
    "    plt.plot(bottlenecks, adapter_params_by_bottleneck, 's-', linewidth=2, color='orange')\n",
    "    plt.xlabel('Bottleneck Size')\n",
    "    plt.ylabel('Parameters per Layer')\n",
    "    plt.title('Adapter Bottleneck vs Parameters')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Memory usage comparison\n",
    "    plt.subplot(2, 3, 4)\n",
    "    \n",
    "    # Approximate memory usage (parameters * 4 bytes for fp32)\n",
    "    memory_usage = [p * 4 / (1024**3) for p in trainable_params]  # GB\n",
    "    \n",
    "    plt.bar(methods, memory_usage, alpha=0.7, color='green')\n",
    "    plt.ylabel('Memory Usage (GB)')\n",
    "    plt.title('Training Memory Requirements')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # Performance vs efficiency trade-off\n",
    "    plt.subplot(2, 3, 5)\n",
    "    \n",
    "    # Simulated performance scores\n",
    "    performance_scores = [100, 95, 92, 88]  # Full FT = 100%\n",
    "    efficiency_scores = [1, 99, 95, 97]     # % parameters saved\n",
    "    \n",
    "    plt.scatter(efficiency_scores, performance_scores, s=100, alpha=0.7)\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        plt.annotate(method, (efficiency_scores[i], performance_scores[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.xlabel('Parameter Efficiency (%)')\n",
    "    plt.ylabel('Performance Score')\n",
    "    plt.title('Efficiency vs Performance')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training speed comparison\n",
    "    plt.subplot(2, 3, 6)\n",
    "    \n",
    "    # Relative training speeds (higher = faster)\n",
    "    training_speeds = [1.0, 3.5, 2.8, 3.2]\n",
    "    \n",
    "    bars = plt.bar(methods, training_speeds, alpha=0.7, color='purple')\n",
    "    plt.ylabel('Relative Training Speed')\n",
    "    plt.title('Training Speed Comparison')\n",
    "    \n",
    "    # Add speedup labels\n",
    "    for bar, speed in zip(bars, training_speeds):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05, \n",
    "                f'{speed:.1f}x', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPEFT Method Characteristics:\")\n",
    "    \n",
    "    print(\"\\nLoRA:\")\n",
    "    print(\"  + Very parameter efficient\")\n",
    "    print(\"  + No inference overhead\")\n",
    "    print(\"  + Easy to merge with base model\")\n",
    "    print(\"  - Limited to linear layers\")\n",
    "    \n",
    "    print(\"\\nAdapters:\")\n",
    "    print(\"  + Modular and swappable\")\n",
    "    print(\"  + Good performance retention\")\n",
    "    print(\"  - Adds inference latency\")\n",
    "    print(\"  - More parameters than LoRA\")\n",
    "    \n",
    "    print(\"\\nPrefix Tuning:\")\n",
    "    print(\"  + No architectural changes\")\n",
    "    print(\"  + Works with frozen models\")\n",
    "    print(\"  - Reduces effective context length\")\n",
    "    print(\"  - Task-specific optimization needed\")\n",
    "\n",
    "demonstrate_peft_methods()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 - Multimodal Language Models\n",
    "\n",
    "This notebook covers multimodal language models that process text, images, and audio.\n",
    "\n",
    "## Topics Covered:\n",
    "- Text-image models\n",
    "- Text-audio models\n",
    "- Vision-language transformers\n",
    "- Multimodal fusion techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text-Image Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Simple Vision Transformer for image encoding.\"\"\"\n",
    "    \n",
    "    def __init__(self, image_size=224, patch_size=16, d_model=512, num_layers=6, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Calculate number of patches\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embedding = nn.Linear(patch_size * patch_size * 3, d_model)\n",
    "        \n",
    "        # Position embeddings\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, self.num_patches + 1, d_model))\n",
    "        \n",
    "        # CLS token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        \n",
    "        # Transformer layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Layer norm\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def patchify(self, images):\n",
    "        \"\"\"Convert images to patches.\"\"\"\n",
    "        batch_size, channels, height, width = images.shape\n",
    "        \n",
    "        # Reshape to patches\n",
    "        patches = images.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        patches = patches.contiguous().view(\n",
    "            batch_size, channels, -1, self.patch_size, self.patch_size\n",
    "        )\n",
    "        patches = patches.permute(0, 2, 1, 3, 4).contiguous()\n",
    "        patches = patches.view(batch_size, -1, channels * self.patch_size * self.patch_size)\n",
    "        \n",
    "        return patches\n",
    "    \n",
    "    def forward(self, images):\n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        # Convert to patches\n",
    "        patches = self.patchify(images)  # (batch_size, num_patches, patch_dim)\n",
    "        \n",
    "        # Embed patches\n",
    "        patch_embeddings = self.patch_embedding(patches)  # (batch_size, num_patches, d_model)\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        embeddings = torch.cat([cls_tokens, patch_embeddings], dim=1)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        embeddings = embeddings + self.position_embeddings\n",
    "        \n",
    "        # Apply transformer\n",
    "        features = self.transformer(embeddings)\n",
    "        \n",
    "        # Apply layer norm\n",
    "        features = self.layer_norm(features)\n",
    "        \n",
    "        return features\n",
    "\n",
    "class TextImageModel(nn.Module):\n",
    "    \"\"\"Text-Image multimodal model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=50000, d_model=512, num_heads=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Text encoder\n",
    "        self.text_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.text_position_embedding = nn.Embedding(1000, d_model)  # Max sequence length\n",
    "        \n",
    "        text_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.text_encoder = nn.TransformerEncoder(text_encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Image encoder\n",
    "        self.image_encoder = VisionTransformer(d_model=d_model, num_layers=num_layers, num_heads=num_heads)\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        self.cross_attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Layer norms\n",
    "        self.text_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.image_layer_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def encode_text(self, text_tokens):\n",
    "        \"\"\"Encode text tokens.\"\"\"\n",
    "        batch_size, seq_len = text_tokens.shape\n",
    "        \n",
    "        # Text embeddings\n",
    "        text_emb = self.text_embedding(text_tokens)\n",
    "        \n",
    "        # Position embeddings\n",
    "        positions = torch.arange(seq_len, device=text_tokens.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        pos_emb = self.text_position_embedding(positions)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        embeddings = text_emb + pos_emb\n",
    "        \n",
    "        # Apply transformer\n",
    "        text_features = self.text_encoder(embeddings)\n",
    "        text_features = self.text_layer_norm(text_features)\n",
    "        \n",
    "        return text_features\n",
    "    \n",
    "    def encode_image(self, images):\n",
    "        \"\"\"Encode images.\"\"\"\n",
    "        image_features = self.image_encoder(images)\n",
    "        image_features = self.image_layer_norm(image_features)\n",
    "        return image_features\n",
    "    \n",
    "    def forward(self, text_tokens, images):\n",
    "        \"\"\"Forward pass with text and images.\"\"\"\n",
    "        # Encode modalities\n",
    "        text_features = self.encode_text(text_tokens)\n",
    "        image_features = self.encode_image(images)\n",
    "        \n",
    "        # Cross-modal attention: text attends to image\n",
    "        attended_text, attention_weights = self.cross_attention(\n",
    "            query=text_features,\n",
    "            key=image_features,\n",
    "            value=image_features\n",
    "        )\n",
    "        \n",
    "        # Combine with residual connection\n",
    "        fused_features = text_features + attended_text\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.output_projection(fused_features)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Demonstrate text-image model\n",
    "def demonstrate_text_image_model():\n",
    "    \"\"\"Demonstrate text-image multimodal model.\"\"\"\n",
    "    \n",
    "    print(\"Text-Image Multimodal Model Demo:\")\n",
    "    \n",
    "    # Model parameters\n",
    "    vocab_size = 10000\n",
    "    d_model = 256\n",
    "    batch_size = 2\n",
    "    seq_len = 20\n",
    "    image_size = 224\n",
    "    \n",
    "    # Initialize model\n",
    "    model = TextImageModel(vocab_size=vocab_size, d_model=d_model)\n",
    "    \n",
    "    # Sample data\n",
    "    text_tokens = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    images = torch.randn(batch_size, 3, image_size, image_size)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output, attention_weights = model(text_tokens, images)\n",
    "    \n",
    "    print(f\"Input shapes:\")\n",
    "    print(f\"  Text tokens: {text_tokens.shape}\")\n",
    "    print(f\"  Images: {images.shape}\")\n",
    "    print(f\"Output shapes:\")\n",
    "    print(f\"  Model output: {output.shape}\")\n",
    "    print(f\"  Attention weights: {attention_weights.shape}\")\n",
    "    \n",
    "    # Visualize attention\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot attention weights\n",
    "    plt.subplot(1, 2, 1)\n",
    "    attention_avg = attention_weights[0].mean(dim=0).numpy()  # Average over heads\n",
    "    plt.imshow(attention_avg, cmap='Blues', aspect='auto')\n",
    "    plt.title('Cross-Modal Attention\\n(Text â†’ Image)')\n",
    "    plt.xlabel('Image Patches')\n",
    "    plt.ylabel('Text Tokens')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Plot attention distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    attention_sum = attention_avg.sum(axis=0)\n",
    "    plt.bar(range(len(attention_sum)), attention_sum)\n",
    "    plt.title('Attention Distribution\\nAcross Image Patches')\n",
    "    plt.xlabel('Image Patch Index')\n",
    "    plt.ylabel('Total Attention')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "text_image_model = demonstrate_text_image_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text-Audio Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class AudioEncoder(nn.Module):\n",
    "    \"\"\"Audio encoder using 1D convolutions and transformers.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=80, d_model=512, num_layers=6, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Convolutional layers for feature extraction\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(512, d_model, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Position embeddings\n",
    "        self.position_embedding = nn.Embedding(5000, d_model)  # Max audio length\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Layer norm\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, audio_features):\n",
    "        \"\"\"Forward pass for audio encoding.\"\"\"\n",
    "        batch_size, input_dim, seq_len = audio_features.shape\n",
    "        \n",
    "        # Apply convolutions\n",
    "        conv_features = self.conv_layers(audio_features)  # (batch, d_model, seq_len)\n",
    "        conv_features = conv_features.transpose(1, 2)  # (batch, seq_len, d_model)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        positions = torch.arange(seq_len, device=audio_features.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        pos_emb = self.position_embedding(positions)\n",
    "        \n",
    "        embeddings = conv_features + pos_emb\n",
    "        \n",
    "        # Apply transformer\n",
    "        audio_encoded = self.transformer(embeddings)\n",
    "        audio_encoded = self.layer_norm(audio_encoded)\n",
    "        \n",
    "        return audio_encoded\n",
    "\n",
    "class TextAudioModel(nn.Module):\n",
    "    \"\"\"Text-Audio multimodal model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=50000, audio_dim=80, d_model=512, num_heads=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Text encoder (same as before)\n",
    "        self.text_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.text_position_embedding = nn.Embedding(1000, d_model)\n",
    "        \n",
    "        text_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.text_encoder = nn.TransformerEncoder(text_encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Audio encoder\n",
    "        self.audio_encoder = AudioEncoder(input_dim=audio_dim, d_model=d_model, num_layers=num_layers)\n",
    "        \n",
    "        # Cross-modal fusion\n",
    "        self.text_to_audio_attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "        self.audio_to_text_attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "        \n",
    "        # Fusion layer\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Layer norms\n",
    "        self.text_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.audio_layer_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def encode_text(self, text_tokens):\n",
    "        \"\"\"Encode text tokens.\"\"\"\n",
    "        batch_size, seq_len = text_tokens.shape\n",
    "        \n",
    "        # Text embeddings\n",
    "        text_emb = self.text_embedding(text_tokens)\n",
    "        \n",
    "        # Position embeddings\n",
    "        positions = torch.arange(seq_len, device=text_tokens.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        pos_emb = self.text_position_embedding(positions)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        embeddings = text_emb + pos_emb\n",
    "        \n",
    "        # Apply transformer\n",
    "        text_features = self.text_encoder(embeddings)\n",
    "        text_features = self.text_layer_norm(text_features)\n",
    "        \n",
    "        return text_features\n",
    "    \n",
    "    def encode_audio(self, audio_features):\n",
    "        \"\"\"Encode audio features.\"\"\"\n",
    "        audio_encoded = self.audio_encoder(audio_features)\n",
    "        audio_encoded = self.audio_layer_norm(audio_encoded)\n",
    "        return audio_encoded\n",
    "    \n",
    "    def forward(self, text_tokens, audio_features):\n",
    "        \"\"\"Forward pass with text and audio.\"\"\"\n",
    "        # Encode modalities\n",
    "        text_features = self.encode_text(text_tokens)\n",
    "        audio_features = self.encode_audio(audio_features)\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        text_attended, _ = self.text_to_audio_attention(\n",
    "            query=text_features,\n",
    "            key=audio_features,\n",
    "            value=audio_features\n",
    "        )\n",
    "        \n",
    "        audio_attended, _ = self.audio_to_text_attention(\n",
    "            query=audio_features,\n",
    "            key=text_features,\n",
    "            value=text_features\n",
    "        )\n",
    "        \n",
    "        # Pool audio features (mean pooling)\n",
    "        audio_pooled = audio_attended.mean(dim=1, keepdim=True)  # (batch, 1, d_model)\n",
    "        audio_pooled = audio_pooled.expand(-1, text_features.size(1), -1)  # Match text length\n",
    "        \n",
    "        # Fusion\n",
    "        combined_features = torch.cat([text_attended, audio_pooled], dim=-1)\n",
    "        fused_features = self.fusion_layer(combined_features)\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.output_projection(fused_features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Demonstrate text-audio model\n",
    "def demonstrate_text_audio_model():\n",
    "    \"\"\"Demonstrate text-audio multimodal model.\"\"\"\n",
    "    \n",
    "    print(\"\\nText-Audio Multimodal Model Demo:\")\n",
    "    \n",
    "    # Model parameters\n",
    "    vocab_size = 10000\n",
    "    audio_dim = 80  # Mel spectrogram features\n",
    "    d_model = 256\n",
    "    batch_size = 2\n",
    "    text_seq_len = 20\n",
    "    audio_seq_len = 100\n",
    "    \n",
    "    # Initialize model\n",
    "    model = TextAudioModel(vocab_size=vocab_size, audio_dim=audio_dim, d_model=d_model)\n",
    "    \n",
    "    # Sample data\n",
    "    text_tokens = torch.randint(0, vocab_size, (batch_size, text_seq_len))\n",
    "    audio_features = torch.randn(batch_size, audio_dim, audio_seq_len)  # Mel spectrogram\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(text_tokens, audio_features)\n",
    "    \n",
    "    print(f\"Input shapes:\")\n",
    "    print(f\"  Text tokens: {text_tokens.shape}\")\n",
    "    print(f\"  Audio features: {audio_features.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "    # Visualize audio features and model components\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot sample audio features (mel spectrogram)\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.imshow(audio_features[0].numpy(), cmap='viridis', aspect='auto')\n",
    "    plt.title('Sample Audio Features\\n(Mel Spectrogram)')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Mel Frequency Bins')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Plot text token distribution\n",
    "    plt.subplot(2, 3, 2)\n",
    "    token_counts = torch.bincount(text_tokens.flatten(), minlength=100)[:100]\n",
    "    plt.bar(range(len(token_counts)), token_counts.numpy())\n",
    "    plt.title('Text Token Distribution\\n(First 100 tokens)')\n",
    "    plt.xlabel('Token ID')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # Plot output logits distribution\n",
    "    plt.subplot(2, 3, 3)\n",
    "    output_sample = output[0, 0, :100].numpy()  # First position, first 100 logits\n",
    "    plt.plot(output_sample)\n",
    "    plt.title('Output Logits\\n(First position, 100 dims)')\n",
    "    plt.xlabel('Vocabulary Index')\n",
    "    plt.ylabel('Logit Value')\n",
    "    \n",
    "    # Model architecture visualization\n",
    "    plt.subplot(2, 3, 4)\n",
    "    components = ['Text\\nEncoder', 'Audio\\nEncoder', 'Cross\\nAttention', 'Fusion\\nLayer', 'Output\\nProjection']\n",
    "    positions = range(len(components))\n",
    "    plt.barh(positions, [1, 1, 1, 1, 1], alpha=0.7)\n",
    "    plt.yticks(positions, components)\n",
    "    plt.title('Model Architecture\\nComponents')\n",
    "    plt.xlabel('Processing Stage')\n",
    "    \n",
    "    # Parameter count comparison\n",
    "    plt.subplot(2, 3, 5)\n",
    "    param_counts = {\n",
    "        'Text Encoder': sum(p.numel() for p in model.text_encoder.parameters()),\n",
    "        'Audio Encoder': sum(p.numel() for p in model.audio_encoder.parameters()),\n",
    "        'Fusion': sum(p.numel() for p in model.fusion_layer.parameters()),\n",
    "        'Output': sum(p.numel() for p in model.output_projection.parameters())\n",
    "    }\n",
    "    \n",
    "    plt.pie(param_counts.values(), labels=param_counts.keys(), autopct='%1.1f%%')\n",
    "    plt.title('Parameter Distribution')\n",
    "    \n",
    "    # Audio processing pipeline\n",
    "    plt.subplot(2, 3, 6)\n",
    "    pipeline_steps = ['Raw\\nAudio', 'Mel\\nSpectrogram', 'Conv\\nLayers', 'Transformer\\nEncoder', 'Features']\n",
    "    step_positions = range(len(pipeline_steps))\n",
    "    \n",
    "    for i in range(len(pipeline_steps) - 1):\n",
    "        plt.arrow(i, 0, 0.8, 0, head_width=0.1, head_length=0.1, fc='blue', ec='blue')\n",
    "    \n",
    "    plt.scatter(step_positions, [0] * len(pipeline_steps), s=100, c='red')\n",
    "    for i, step in enumerate(pipeline_steps):\n",
    "        plt.text(i, 0.2, step, ha='center', va='bottom')\n",
    "    \n",
    "    plt.xlim(-0.5, len(pipeline_steps) - 0.5)\n",
    "    plt.ylim(-0.5, 0.5)\n",
    "    plt.title('Audio Processing Pipeline')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "text_audio_model = demonstrate_text_audio_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
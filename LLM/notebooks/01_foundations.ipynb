{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Foundations of Natural Language Processing\n",
    "\n",
    "This notebook covers the fundamental concepts of NLP that form the foundation for understanding Large Language Models.\n",
    "\n",
    "## Topics Covered:\n",
    "- Text normalization\n",
    "- Tokenization methods\n",
    "- Vocabulary construction\n",
    "- Encoding schemes\n",
    "- Basic linguistic representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy pandas matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Sample text for demonstrations\n",
    "sample_text = \"\"\"\n",
    "Natural Language Processing (NLP) is a fascinating field! \n",
    "It combines linguistics, computer science, and AI.\n",
    "Modern LLMs like GPT-4 have revolutionized the field.\n",
    "They can understand context, generate text, and even code.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Normalization\n",
    "\n",
    "Text normalization is the process of converting text to a standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Basic text normalization.\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Demonstrate normalization\n",
    "print(\"Original:\")\n",
    "print(repr(sample_text))\n",
    "print(\"\\nNormalized:\")\n",
    "print(repr(normalize_text(sample_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization Methods\n",
    "\n",
    "### 2.1 Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Simple word tokenization.\"\"\"\n",
    "    # Remove punctuation and split on whitespace\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text.split()\n",
    "\n",
    "def advanced_word_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"More sophisticated word tokenization.\"\"\"\n",
    "    # Keep contractions together, handle punctuation better\n",
    "    pattern = r\"\\b\\w+(?:'\\w+)?\\b|[.,!?;]\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "# Compare tokenization methods\n",
    "text = \"I can't believe it's working! Amazing, isn't it?\"\n",
    "print(\"Text:\", text)\n",
    "print(\"Simple:\", word_tokenize(text))\n",
    "print(\"Advanced:\", advanced_word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Character-level tokenization.\"\"\"\n",
    "    return list(text)\n",
    "\n",
    "# Demonstrate character tokenization\n",
    "text = \"Hello, World!\"\n",
    "print(\"Text:\", text)\n",
    "print(\"Characters:\", char_tokenize(text))\n",
    "print(\"Unique chars:\", sorted(set(char_tokenize(text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Subword Tokenization (Simplified BPE)\n",
    "\n",
    "**Byte Pair Encoding (BPE)** is a subword tokenization algorithm that learns to merge the most frequent character pairs iteratively. This creates a vocabulary that balances between character-level and word-level representations.\n",
    "\n",
    "**How BPE Works:**\n",
    "1. **Initialize**: Start with individual characters as the base vocabulary\n",
    "2. **Count Pairs**: Find all adjacent character pairs in the corpus\n",
    "3. **Merge Most Frequent**: Merge the most frequent pair into a single token\n",
    "4. **Repeat**: Continue until reaching desired vocabulary size\n",
    "5. **Encode**: Use learned merges to tokenize new text\n",
    "\n",
    "**Example Process:**\n",
    "- Corpus: ['hello', 'world', 'hello']\n",
    "- Initial: ['h', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd']\n",
    "- Most frequent pair: ('l', 'l') → merge to 'll'\n",
    "- Next: ('e', 'll') → merge to 'ell'\n",
    "- Continue until vocabulary size reached\n",
    "\n",
    "**Advantages:**\n",
    "- Handles unknown words by breaking them into subwords\n",
    "- Balances vocabulary size with representation quality\n",
    "- Works well across different languages\n",
    "- Used in modern models like GPT, BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPE:\n",
    "    \"\"\"\n",
    "    Simplified Byte Pair Encoding implementation.\n",
    "    \n",
    "    This class demonstrates the core BPE algorithm:\n",
    "    - Training: Learn merge rules from a corpus\n",
    "    - Encoding: Apply learned rules to tokenize new text\n",
    "    \n",
    "    Attributes:\n",
    "        vocab_size (int): Maximum vocabulary size\n",
    "        vocab (dict): Final vocabulary mapping tokens to IDs\n",
    "        merges (dict): Learned merge rules (pair -> merged_token)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "    \n",
    "    def get_pairs(self, word: List[str]) -> Dict[Tuple[str, str], int]:\n",
    "        \"\"\"\n",
    "        Extract all adjacent character pairs from a word.\n",
    "        \n",
    "        Args:\n",
    "            word: List of characters/tokens\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping pairs to their frequency\n",
    "        \n",
    "        Example:\n",
    "            get_pairs(['h', 'e', 'l', 'l', 'o'])\n",
    "            # Returns: {('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1}\n",
    "        \"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        for i in range(len(word) - 1):\n",
    "            pairs[(word[i], word[i + 1])] += 1\n",
    "        return pairs\n",
    "    \n",
    "    def train(self, texts: List[str]):\n",
    "        \"\"\"\n",
    "        Train BPE on a corpus by learning merge rules.\n",
    "        \n",
    "        Training Process:\n",
    "        1. Initialize vocabulary with individual characters\n",
    "        2. Count word frequencies in corpus\n",
    "        3. Iteratively find and merge most frequent character pairs\n",
    "        4. Stop when reaching desired vocabulary size\n",
    "        \n",
    "        Args:\n",
    "            texts: List of training texts\n",
    "        \"\"\"\n",
    "        # Initialize with character vocabulary\n",
    "        vocab = set()\n",
    "        word_freqs = defaultdict(int)\n",
    "        \n",
    "        # Count word frequencies and collect characters\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                word_freqs[word] += 1\n",
    "                vocab.update(word)  # Add all characters to vocab\n",
    "        \n",
    "        # Convert words to character lists for processing\n",
    "        word_splits = {word: list(word) for word in word_freqs}\n",
    "        \n",
    "        # Iteratively merge most frequent pairs\n",
    "        for i in range(self.vocab_size - len(vocab)):\n",
    "            pairs = defaultdict(int)\n",
    "            \n",
    "            # Count all pairs across all words (weighted by frequency)\n",
    "            for word, freq in word_freqs.items():\n",
    "                word_pairs = self.get_pairs(word_splits[word])\n",
    "                for pair, count in word_pairs.items():\n",
    "                    pairs[pair] += count * freq\n",
    "            \n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            # Find most frequent pair to merge\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            \n",
    "            # Apply merge to all words\n",
    "            for word in word_splits:\n",
    "                new_word = []\n",
    "                i = 0\n",
    "                while i < len(word_splits[word]):\n",
    "                    # Check if current position matches the pair to merge\n",
    "                    if (i < len(word_splits[word]) - 1 and \n",
    "                        word_splits[word][i] == best_pair[0] and \n",
    "                        word_splits[word][i + 1] == best_pair[1]):\n",
    "                        # Merge the pair\n",
    "                        new_word.append(best_pair[0] + best_pair[1])\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        # Keep character as is\n",
    "                        new_word.append(word_splits[word][i])\n",
    "                        i += 1\n",
    "                word_splits[word] = new_word\n",
    "            \n",
    "            # Add merged token to vocabulary and record merge rule\n",
    "            vocab.add(best_pair[0] + best_pair[1])\n",
    "            self.merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "        \n",
    "        # Create final vocabulary mapping\n",
    "        self.vocab = {token: i for i, token in enumerate(sorted(vocab))}\n",
    "    \n",
    "    def encode(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Encode text using learned BPE merges.\n",
    "        \n",
    "        Encoding Process:\n",
    "        1. Split text into words\n",
    "        2. For each word, start with character-level tokens\n",
    "        3. Apply learned merges in the order they were learned\n",
    "        4. Continue until no more merges can be applied\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to encode\n",
    "        \n",
    "        Returns:\n",
    "            List of subword tokens\n",
    "        \n",
    "        Example:\n",
    "            encode('hello world')\n",
    "            # Might return: ['hell', 'o', 'w', 'o', 'r', 'l', 'd']\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        encoded = []\n",
    "        \n",
    "        for word in words:\n",
    "            # Start with character-level tokenization\n",
    "            word_tokens = list(word)\n",
    "            \n",
    "            # Apply merges iteratively\n",
    "            while len(word_tokens) > 1:\n",
    "                pairs = self.get_pairs(word_tokens)\n",
    "                if not pairs:\n",
    "                    break\n",
    "                \n",
    "                # Find the pair that appears in our learned merges\n",
    "                valid_pairs = [pair for pair in pairs if pair in self.merges]\n",
    "                if not valid_pairs:\n",
    "                    break\n",
    "                # Use the merge that was learned earliest (lowest index)\n",
    "                bigram = min(valid_pairs, key=lambda pair: list(self.merges.keys()).index(pair))\n",
    "                \n",
    "                # Apply the merge\n",
    "                new_word = []\n",
    "                i = 0\n",
    "                while i < len(word_tokens):\n",
    "                    if (i < len(word_tokens) - 1 and \n",
    "                        word_tokens[i] == bigram[0] and \n",
    "                        word_tokens[i + 1] == bigram[1]):\n",
    "                        # Apply merge\n",
    "                        new_word.append(self.merges[bigram])\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        # Keep token as is\n",
    "                        new_word.append(word_tokens[i])\n",
    "                        i += 1\n",
    "                word_tokens = new_word\n",
    "            \n",
    "            encoded.extend(word_tokens)\n",
    "        \n",
    "        return encoded\n",
    "\n",
    "# Demonstrate BPE with detailed output\n",
    "corpus = [\n",
    "    \"hello world\",\n",
    "    \"hello there\",\n",
    "    \"world peace\",\n",
    "    \"hello hello world\"\n",
    "]\n",
    "\n",
    "print(\"Training BPE on corpus:\", corpus)\n",
    "bpe = SimpleBPE(vocab_size=20)\n",
    "bpe.train(corpus)\n",
    "\n",
    "print(\"\\nLearned vocabulary:\", list(bpe.vocab.keys())[:10])\n",
    "print(\"Learned merges:\", list(bpe.merges.items())[:5])\n",
    "print(\"\\nEncoding examples:\")\n",
    "for text in [\"hello world\", \"hello\", \"world\", \"peace\"]:\n",
    "    encoded = bpe.encode(text)\n",
    "    print(f\"'{text}' -> {encoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vocabulary Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(texts: List[str], min_freq: int = 2) -> Dict[str, int]:\n",
    "    \"\"\"Build vocabulary from texts with frequency filtering.\"\"\"\n",
    "    word_counts = Counter()\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(normalize_text(text))\n",
    "        word_counts.update(tokens)\n",
    "    \n",
    "    # Filter by minimum frequency\n",
    "    vocab = {\"<UNK>\": 0, \"<PAD>\": 1}  # Special tokens\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# Build vocabulary from sample texts\n",
    "texts = [\n",
    "    \"Natural language processing is amazing\",\n",
    "    \"Language models are powerful tools\",\n",
    "    \"Processing natural language requires understanding\",\n",
    "    \"Amazing tools for language understanding\"\n",
    "]\n",
    "\n",
    "vocab = build_vocabulary(texts, min_freq=1)\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"Sample vocabulary:\", dict(list(vocab.items())[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encoding Schemes\n",
    "\n",
    "### 4.1 One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(tokens: List[str], vocab: Dict[str, int]) -> np.ndarray:\n",
    "    \"\"\"Convert tokens to one-hot encoded vectors.\"\"\"\n",
    "    vocab_size = len(vocab)\n",
    "    encoded = np.zeros((len(tokens), vocab_size))\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in vocab:\n",
    "            encoded[i, vocab[token]] = 1\n",
    "        else:\n",
    "            encoded[i, vocab[\"<UNK>\"]] = 1\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "# Demonstrate one-hot encoding\n",
    "tokens = [\"natural\", \"language\", \"processing\"]\n",
    "one_hot = one_hot_encode(tokens, vocab)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"One-hot shape:\", one_hot.shape)\n",
    "print(\"First token encoding:\", one_hot[0][:10])  # Show first 10 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Integer Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_encode(tokens: List[str], vocab: Dict[str, int]) -> List[int]:\n",
    "    \"\"\"Convert tokens to integer indices.\"\"\"\n",
    "    encoded = []\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            encoded.append(vocab[token])\n",
    "        else:\n",
    "            encoded.append(vocab[\"<UNK>\"])\n",
    "    return encoded\n",
    "\n",
    "def integer_decode(indices: List[int], vocab: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"Convert integer indices back to tokens.\"\"\"\n",
    "    # Create reverse vocabulary\n",
    "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "    return [reverse_vocab.get(idx, \"<UNK>\") for idx in indices]\n",
    "\n",
    "# Demonstrate integer encoding\n",
    "tokens = [\"natural\", \"language\", \"processing\", \"unknown_word\"]\n",
    "encoded = integer_encode(tokens, vocab)\n",
    "decoded = integer_decode(encoded, vocab)\n",
    "\n",
    "print(\"Original tokens:\", tokens)\n",
    "print(\"Integer encoded:\", encoded)\n",
    "print(\"Decoded tokens:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Basic Linguistic Representations\n",
    "\n",
    "### 5.1 N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:\n",
    "    \"\"\"Generate n-grams from tokens.\"\"\"\n",
    "    if n <= 0:\n",
    "        return []\n",
    "    \n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngrams.append(tuple(tokens[i:i + n]))\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "# Demonstrate n-grams\n",
    "text = \"natural language processing is fascinating\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Unigrams:\", generate_ngrams(tokens, 1))\n",
    "print(\"Bigrams:\", generate_ngrams(tokens, 2))\n",
    "print(\"Trigrams:\", generate_ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(texts: List[str], vocab: Dict[str, int]) -> np.ndarray:\n",
    "    \"\"\"Convert texts to bag-of-words representation.\"\"\"\n",
    "    bow_matrix = np.zeros((len(texts), len(vocab)))\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        tokens = word_tokenize(normalize_text(text))\n",
    "        token_counts = Counter(tokens)\n",
    "        \n",
    "        for token, count in token_counts.items():\n",
    "            if token in vocab:\n",
    "                bow_matrix[i, vocab[token]] = count\n",
    "    \n",
    "    return bow_matrix\n",
    "\n",
    "# Demonstrate bag of words\n",
    "sample_texts = [\n",
    "    \"natural language processing\",\n",
    "    \"language models are powerful\",\n",
    "    \"processing natural language\"\n",
    "]\n",
    "\n",
    "bow = bag_of_words(sample_texts, vocab)\n",
    "print(\"BoW shape:\", bow.shape)\n",
    "print(\"First document BoW:\", bow[0][:10])  # Show first 10 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(bow_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute TF-IDF from bag-of-words matrix.\"\"\"\n",
    "    # Term frequency (normalized by document length)\n",
    "    tf = bow_matrix / (bow_matrix.sum(axis=1, keepdims=True) + 1e-10)\n",
    "    \n",
    "    # Document frequency\n",
    "    df = (bow_matrix > 0).sum(axis=0)\n",
    "    \n",
    "    # Inverse document frequency\n",
    "    idf = np.log(bow_matrix.shape[0] / (df + 1e-10))\n",
    "    \n",
    "    # TF-IDF\n",
    "    tfidf = tf * idf\n",
    "    \n",
    "    return tfidf\n",
    "\n",
    "# Demonstrate TF-IDF\n",
    "tfidf_matrix = compute_tfidf(bow)\n",
    "print(\"TF-IDF shape:\", tfidf_matrix.shape)\n",
    "print(\"First document TF-IDF:\", tfidf_matrix[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercises\n",
    "\n",
    "Try these exercises to reinforce your understanding:\n",
    "\n",
    "1. **Text Preprocessing Pipeline**: Create a complete preprocessing pipeline that handles:\n",
    "   - Case normalization\n",
    "   - Punctuation handling\n",
    "   - Number normalization\n",
    "   - Stopword removal\n",
    "\n",
    "2. **Custom Tokenizer**: Implement a tokenizer that can handle:\n",
    "   - URLs and email addresses\n",
    "   - Hashtags and mentions\n",
    "   - Emoticons and emojis\n",
    "\n",
    "3. **Vocabulary Analysis**: Analyze a text corpus to find:\n",
    "   - Most frequent words\n",
    "   - Vocabulary growth curve\n",
    "   - Out-of-vocabulary rate for different vocabulary sizes\n",
    "\n",
    "4. **N-gram Language Model**: Build a simple n-gram language model that can:\n",
    "   - Calculate n-gram probabilities\n",
    "   - Generate text using the model\n",
    "   - Handle unseen n-grams with smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "- **Text Normalization**: Converting text to standard formats\n",
    "- **Tokenization**: Breaking text into meaningful units\n",
    "  - Word-level, character-level, and subword tokenization\n",
    "  - Byte Pair Encoding (BPE) algorithm\n",
    "- **Vocabulary Construction**: Building and managing vocabularies\n",
    "- **Encoding Schemes**: Converting text to numerical representations\n",
    "- **Linguistic Representations**: N-grams, BoW, and TF-IDF\n",
    "\n",
    "These concepts form the foundation for understanding how modern language models process and represent text. In the next notebook, we'll explore neural network fundamentals that build upon these representations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Foundations of Natural Language Processing\n",
    "\n",
    "This notebook covers the fundamental concepts of NLP that form the foundation for understanding Large Language Models.\n",
    "\n",
    "## Topics Covered:\n",
    "- Text normalization\n",
    "- Tokenization methods\n",
    "- Vocabulary construction\n",
    "- Encoding schemes\n",
    "- Basic linguistic representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.2)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp312-cp312-win_amd64.whl.metadata (52 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.61.1-cp312-cp312-win_amd64.whl.metadata (116 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading matplotlib-3.10.8-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 8.1/8.1 MB 45.8 MB/s  0:00:00\n",
      "Downloading contourpy-1.3.3-cp312-cp312-win_amd64.whl (226 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp312-cp312-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.3/2.3 MB 64.3 MB/s  0:00:00\n",
      "Downloading kiwisolver-1.4.9-cp312-cp312-win_amd64.whl (73 kB)\n",
      "Installing collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   ------------------------ --------------- 3/5 [contourpy]\n",
      "   ------------------------ --------------- 3/5 [contourpy]\n",
      "   ------------------------ --------------- 3/5 [contourpy]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   ---------------------------------------- 5/5 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Sample text for demonstrations\n",
    "sample_text = \"\"\"\n",
    "Natural Language Processing (NLP) is a fascinating field! \n",
    "It combines linguistics, computer science, and AI.\n",
    "Modern LLMs like GPT-4 have revolutionized the field.\n",
    "They can understand context, generate text, and even code.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Normalization\n",
    "\n",
    "Text normalization is the process of converting text to a standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "'\\nNatural Language Processing (NLP) is a fascinating field! \\nIt combines linguistics, computer science, and AI.\\nModern LLMs like GPT-4 have revolutionized the field.\\nThey can understand context, generate text, and even code.\\n'\n",
      "\n",
      "Normalized:\n",
      "'natural language processing (nlp) is a fascinating field! it combines linguistics, computer science, and ai. modern llms like gpt-4 have revolutionized the field. they can understand context, generate text, and even code.'\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Basic text normalization.\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Demonstrate normalization\n",
    "print(\"Original:\")\n",
    "print(repr(sample_text))\n",
    "print(\"\\nNormalized:\")\n",
    "print(repr(normalize_text(sample_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization Methods\n",
    "\n",
    "### 2.1 Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I can't believe it's working! Amazing, isn't it?\n",
      "Simple: ['I', 'cant', 'believe', 'its', 'working', 'Amazing', 'isnt', 'it']\n",
      "Advanced: ['I', \"can't\", 'believe', \"it's\", 'working', '!', 'Amazing', ',', \"isn't\", 'it', '?']\n"
     ]
    }
   ],
   "source": [
    "def word_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Simple word tokenization.\"\"\"\n",
    "    # Remove punctuation and split on whitespace\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text.split()\n",
    "\n",
    "def advanced_word_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"More sophisticated word tokenization.\"\"\"\n",
    "    # Keep contractions together, handle punctuation better\n",
    "    pattern = r\"\\b\\w+(?:'\\w+)?\\b|[.,!?;]\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "# Compare tokenization methods\n",
    "text = \"I can't believe it's working! Amazing, isn't it?\"\n",
    "print(\"Text:\", text)\n",
    "print(\"Simple:\", word_tokenize(text))\n",
    "print(\"Advanced:\", advanced_word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hello, World!\n",
      "Characters: ['H', 'e', 'l', 'l', 'o', ',', ' ', 'W', 'o', 'r', 'l', 'd', '!']\n",
      "Unique chars: [' ', '!', ',', 'H', 'W', 'd', 'e', 'l', 'o', 'r']\n"
     ]
    }
   ],
   "source": [
    "def char_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Character-level tokenization.\"\"\"\n",
    "    return list(text)\n",
    "\n",
    "# Demonstrate character tokenization\n",
    "text = \"Hello, World!\"\n",
    "print(\"Text:\", text)\n",
    "print(\"Characters:\", char_tokenize(text))\n",
    "print(\"Unique chars:\", sorted(set(char_tokenize(text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Subword Tokenization (Simplified BPE)\n",
    "\n",
    "**Byte Pair Encoding (BPE)** is a subword tokenization algorithm that learns to merge the most frequent character pairs iteratively. This creates a vocabulary that balances between character-level and word-level representations.\n",
    "\n",
    "**How BPE Works:**\n",
    "1. **Initialize**: Start with individual characters as the base vocabulary\n",
    "2. **Count Pairs**: Find all adjacent character pairs in the corpus\n",
    "3. **Merge Most Frequent**: Merge the most frequent pair into a single token\n",
    "4. **Repeat**: Continue until reaching desired vocabulary size\n",
    "5. **Encode**: Use learned merges to tokenize new text\n",
    "\n",
    "**Example Process:**\n",
    "- Corpus: ['hello', 'world', 'hello']\n",
    "- Initial: ['h', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd']\n",
    "- Most frequent pair: ('l', 'l') → merge to 'll'\n",
    "- Next: ('e', 'll') → merge to 'ell'\n",
    "- Continue until vocabulary size reached\n",
    "\n",
    "**Advantages:**\n",
    "- Handles unknown words by breaking them into subwords\n",
    "- Balances vocabulary size with representation quality\n",
    "- Works well across different languages\n",
    "- Used in modern models like GPT, BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE on corpus: ['hello world', 'hello there', 'world peace', 'hello hello world']\n",
      "\n",
      "Learned vocabulary: ['a', 'c', 'd', 'e', 'h', 'he', 'hel', 'hell', 'hello', 'l']\n",
      "Learned merges: [(('h', 'e'), 'he'), (('he', 'l'), 'hel'), (('hel', 'l'), 'hell'), (('hell', 'o'), 'hello'), (('w', 'o'), 'wo')]\n",
      "\n",
      "Encoding examples:\n",
      "'hello world' -> ['hello', 'world']\n",
      "'hello' -> ['hello']\n",
      "'world' -> ['world']\n",
      "'peace' -> ['p', 'e', 'a', 'c', 'e']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "class SimpleBPE:\n",
    "    \"\"\"\n",
    "    Simplified Byte Pair Encoding implementation.\n",
    "    \n",
    "    This class demonstrates the core BPE algorithm:\n",
    "    - Training: Learn merge rules from a corpus\n",
    "    - Encoding: Apply learned rules to tokenize new text\n",
    "    \n",
    "    Attributes:\n",
    "        vocab_size (int): Maximum vocabulary size\n",
    "        vocab (dict): Final vocabulary mapping tokens to IDs\n",
    "        merges (dict): Learned merge rules (pair -> merged_token)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "    \n",
    "    def get_pairs(self, word: List[str]) -> Dict[Tuple[str, str], int]:\n",
    "        \"\"\"\n",
    "        Extract all adjacent character pairs from a word.\n",
    "        \n",
    "        Args:\n",
    "            word: List of characters/tokens\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping pairs to their frequency\n",
    "        \n",
    "        Example:\n",
    "            get_pairs(['h', 'e', 'l', 'l', 'o'])\n",
    "            # Returns: {('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1}\n",
    "        \"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        for i in range(len(word) - 1):\n",
    "            pairs[(word[i], word[i + 1])] += 1\n",
    "        return pairs\n",
    "    \n",
    "    def train(self, texts: List[str]):\n",
    "        \"\"\"\n",
    "        Train BPE on a corpus by learning merge rules.\n",
    "        \n",
    "        Training Process:\n",
    "        1. Initialize vocabulary with individual characters\n",
    "        2. Count word frequencies in corpus\n",
    "        3. Iteratively find and merge most frequent character pairs\n",
    "        4. Stop when reaching desired vocabulary size\n",
    "        \n",
    "        Args:\n",
    "            texts: List of training texts\n",
    "        \"\"\"\n",
    "        # Initialize with character vocabulary\n",
    "        vocab = set()\n",
    "        word_freqs = defaultdict(int)\n",
    "        \n",
    "        # Count word frequencies and collect characters\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                word_freqs[word] += 1\n",
    "                vocab.update(word)  # Add all characters to vocab\n",
    "        \n",
    "        # Convert words to character lists for processing\n",
    "        word_splits = {word: list(word) for word in word_freqs}\n",
    "        \n",
    "        # Iteratively merge most frequent pairs\n",
    "        for i in range(self.vocab_size - len(vocab)):\n",
    "            pairs = defaultdict(int)\n",
    "            \n",
    "            # Count all pairs across all words (weighted by frequency)\n",
    "            for word, freq in word_freqs.items():\n",
    "                word_pairs = self.get_pairs(word_splits[word])\n",
    "                for pair, count in word_pairs.items():\n",
    "                    pairs[pair] += count * freq\n",
    "            \n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            # Find most frequent pair to merge\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            \n",
    "            # Apply merge to all words\n",
    "            for word in word_splits:\n",
    "                new_word = []\n",
    "                i = 0\n",
    "                while i < len(word_splits[word]):\n",
    "                    # Check if current position matches the pair to merge\n",
    "                    if (i < len(word_splits[word]) - 1 and \n",
    "                        word_splits[word][i] == best_pair[0] and \n",
    "                        word_splits[word][i + 1] == best_pair[1]):\n",
    "                        # Merge the pair\n",
    "                        new_word.append(best_pair[0] + best_pair[1])\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        # Keep character as is\n",
    "                        new_word.append(word_splits[word][i])\n",
    "                        i += 1\n",
    "                word_splits[word] = new_word\n",
    "            \n",
    "            # Add merged token to vocabulary and record merge rule\n",
    "            vocab.add(best_pair[0] + best_pair[1])\n",
    "            self.merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "        \n",
    "        # Create final vocabulary mapping\n",
    "        self.vocab = {token: i for i, token in enumerate(sorted(vocab))}\n",
    "    \n",
    "    def encode(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Encode text using learned BPE merges.\n",
    "        \n",
    "        Encoding Process:\n",
    "        1. Split text into words\n",
    "        2. For each word, start with character-level tokens\n",
    "        3. Apply learned merges in the order they were learned\n",
    "        4. Continue until no more merges can be applied\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to encode\n",
    "        \n",
    "        Returns:\n",
    "            List of subword tokens\n",
    "        \n",
    "        Example:\n",
    "            encode('hello world')\n",
    "            # Might return: ['hell', 'o', 'w', 'o', 'r', 'l', 'd']\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        encoded = []\n",
    "        \n",
    "        for word in words:\n",
    "            # Start with character-level tokenization\n",
    "            word_tokens = list(word)\n",
    "            \n",
    "            # Apply merges iteratively\n",
    "            while len(word_tokens) > 1:\n",
    "                pairs = self.get_pairs(word_tokens)\n",
    "                if not pairs:\n",
    "                    break\n",
    "                \n",
    "                # Find the pair that appears in our learned merges\n",
    "                valid_pairs = [pair for pair in pairs if pair in self.merges]\n",
    "                if not valid_pairs:\n",
    "                    break\n",
    "                # Use the merge that was learned earliest (lowest index)\n",
    "                bigram = min(valid_pairs, key=lambda pair: list(self.merges.keys()).index(pair))\n",
    "                \n",
    "                # Apply the merge\n",
    "                new_word = []\n",
    "                i = 0\n",
    "                while i < len(word_tokens):\n",
    "                    if (i < len(word_tokens) - 1 and \n",
    "                        word_tokens[i] == bigram[0] and \n",
    "                        word_tokens[i + 1] == bigram[1]):\n",
    "                        # Apply merge\n",
    "                        new_word.append(self.merges[bigram])\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        # Keep token as is\n",
    "                        new_word.append(word_tokens[i])\n",
    "                        i += 1\n",
    "                word_tokens = new_word\n",
    "            \n",
    "            encoded.extend(word_tokens)\n",
    "        \n",
    "        return encoded\n",
    "\n",
    "# Demonstrate BPE with detailed output\n",
    "corpus = [\n",
    "    \"hello world\",\n",
    "    \"hello there\",\n",
    "    \"world peace\",\n",
    "    \"hello hello world\"\n",
    "]\n",
    "\n",
    "print(\"Training BPE on corpus:\", corpus)\n",
    "bpe = SimpleBPE(vocab_size=20)\n",
    "bpe.train(corpus)\n",
    "\n",
    "print(\"\\nLearned vocabulary:\", list(bpe.vocab.keys())[:10])\n",
    "print(\"Learned merges:\", list(bpe.merges.items())[:5])\n",
    "print(\"\\nEncoding examples:\")\n",
    "for text in [\"hello world\", \"hello\", \"world\", \"peace\"]:\n",
    "    encoded = bpe.encode(text)\n",
    "    print(f\"'{text}' -> {encoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vocabulary Construction\n",
    "\n",
    "Vocabulary construction is the process of creating a mapping between words/tokens and numerical indices that machine learning models can work with.\n",
    "\n",
    "**Core Concept:**\n",
    "- Convert text tokens into a fixed set of unique identifiers\n",
    "- Create a dictionary mapping each unique token to a number\n",
    "- Handle unknown words that weren't seen during training\n",
    "\n",
    "**Key Steps:**\n",
    "1. **Collect all unique tokens** from your training data\n",
    "2. **Sort by frequency** (most common words get lower indices)\n",
    "3. **Add special tokens** like `<UNK>` (unknown), `<PAD>` (padding), `<START>`, `<END>`\n",
    "4. **Set vocabulary size limit** to control model complexity\n",
    "\n",
    "**Example Process:**\n",
    "```\n",
    "Text: \"the cat sat on the mat\"\n",
    "Tokens: [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "Unique tokens: [\"the\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "Vocabulary: {\n",
    "    \"<PAD>\": 0,\n",
    "    \"<UNK>\": 1, \n",
    "    \"the\": 2,    # most frequent\n",
    "    \"cat\": 3,\n",
    "    \"sat\": 4,\n",
    "    \"on\": 5,\n",
    "    \"mat\": 6\n",
    "}\n",
    "```\n",
    "\n",
    "**Why It Matters:**\n",
    "- Models need numbers, not words\n",
    "- Vocabulary size directly affects model parameters\n",
    "- Determines how the model handles new/rare words\n",
    "- Foundation for embedding layers in neural networks\n",
    "\n",
    "The vocabulary becomes your model's \"dictionary\" - any word not in it gets mapped to `<UNK>`, which is why vocabulary construction strategy significantly impacts model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 14\n",
      "Sample vocabulary: {'<UNK>': 0, '<PAD>': 1, 'natural': 2, 'language': 3, 'processing': 4, 'is': 5, 'amazing': 6, 'models': 7, 'are': 8, 'powerful': 9, 'tools': 10, 'requires': 11, 'understanding': 12, 'for': 13}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "import string\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def word_tokenize(text: str) -> List[str]:\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text.split()\n",
    "\n",
    "def build_vocabulary(texts: List[str], min_freq: int = 2) -> Dict[str, int]:\n",
    "    \"\"\"Build vocabulary from texts with frequency filtering.\"\"\"\n",
    "    word_counts = Counter()\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(normalize_text(text))\n",
    "        word_counts.update(tokens)\n",
    "    \n",
    "    # Filter by minimum frequency\n",
    "    vocab = {\"<UNK>\": 0, \"<PAD>\": 1}  # Special tokens\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# Build vocabulary from sample texts\n",
    "texts = [\n",
    "    \"Natural language processing is amazing\",\n",
    "    \"Language models are powerful tools\",\n",
    "    \"Processing natural language requires understanding\",\n",
    "    \"Amazing tools for language understanding\"\n",
    "]\n",
    "\n",
    "vocab = build_vocabulary(texts, min_freq=1)\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"Sample vocabulary:\", dict(list(vocab.items())[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encoding Schemes\n",
    "\n",
    "### 4.1 One-hot Encoding\n",
    "\n",
    "One-hot encoding represents each word as a binary vector where only one position is 1 (hot) and all others are 0 (cold).\n",
    "\n",
    "**How it works:**\n",
    "- Create a vector of length = vocabulary size\n",
    "- Set position corresponding to word's index to 1\n",
    "- All other positions remain 0\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Vocabulary: {'cat': 0, 'dog': 1, 'bird': 2}\n",
    "\n",
    "'cat'  -> [1, 0, 0]\n",
    "'dog'  -> [0, 1, 0]\n",
    "'bird' -> [0, 0, 1]\n",
    "```\n",
    "\n",
    "**Pros:** Simple, no similarity assumptions\n",
    "**Cons:** Sparse, large memory usage, no semantic relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['natural', 'language', 'processing']\n",
      "One-hot shape: (3, 14)\n",
      "First token encoding: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(tokens: List[str], vocab: Dict[str, int]) -> np.ndarray:\n",
    "    \"\"\"Convert tokens to one-hot encoded vectors.\"\"\"\n",
    "    vocab_size = len(vocab)\n",
    "    encoded = np.zeros((len(tokens), vocab_size))\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in vocab:\n",
    "            encoded[i, vocab[token]] = 1\n",
    "        else:\n",
    "            encoded[i, vocab[\"<UNK>\"]] = 1\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "# Demonstrate one-hot encoding\n",
    "tokens = [\"natural\", \"language\", \"processing\"]\n",
    "one_hot = one_hot_encode(tokens, vocab)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"One-hot shape:\", one_hot.shape)\n",
    "print(\"First token encoding:\", one_hot)  # Show first 10 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Integer Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['natural', 'language', 'processing', 'unknown_word']\n",
      "Integer encoded: [2, 3, 4, 0]\n",
      "Decoded tokens: ['natural', 'language', 'processing', '<UNK>']\n"
     ]
    }
   ],
   "source": [
    "def integer_encode(tokens: List[str], vocab: Dict[str, int]) -> List[int]:\n",
    "    \"\"\"Convert tokens to integer indices.\"\"\"\n",
    "    encoded = []\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            encoded.append(vocab[token])\n",
    "        else:\n",
    "            encoded.append(vocab[\"<UNK>\"])\n",
    "    return encoded\n",
    "\n",
    "def integer_decode(indices: List[int], vocab: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"Convert integer indices back to tokens.\"\"\"\n",
    "    # Create reverse vocabulary\n",
    "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "    return [reverse_vocab.get(idx, \"<UNK>\") for idx in indices]\n",
    "\n",
    "# Demonstrate integer encoding\n",
    "tokens = [\"natural\", \"language\", \"processing\", \"unknown_word\"]\n",
    "encoded = integer_encode(tokens, vocab)\n",
    "decoded = integer_decode(encoded, vocab)\n",
    "\n",
    "print(\"Original tokens:\", tokens)\n",
    "print(\"Integer encoded:\", encoded)\n",
    "print(\"Decoded tokens:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Basic Linguistic Representations\n",
    "\n",
    "### 5.1 N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['natural', 'language', 'processing', 'is', 'fascinating']\n",
      "Unigrams: [('natural',), ('language',), ('processing',), ('is',), ('fascinating',)]\n",
      "Bigrams: [('natural', 'language'), ('language', 'processing'), ('processing', 'is'), ('is', 'fascinating')]\n",
      "Trigrams: [('natural', 'language', 'processing'), ('language', 'processing', 'is'), ('processing', 'is', 'fascinating')]\n"
     ]
    }
   ],
   "source": [
    "def generate_ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:\n",
    "    \"\"\"Generate n-grams from tokens.\"\"\"\n",
    "    if n <= 0:\n",
    "        return []\n",
    "    \n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngrams.append(tuple(tokens[i:i + n]))\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "# Demonstrate n-grams\n",
    "text = \"natural language processing is fascinating\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Unigrams:\", generate_ngrams(tokens, 1))\n",
    "print(\"Bigrams:\", generate_ngrams(tokens, 2))\n",
    "print(\"Trigrams:\", generate_ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW shape: (3, 14)\n",
      "First document BoW: [0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words(texts: List[str], vocab: Dict[str, int]) -> np.ndarray:\n",
    "    \"\"\"Convert texts to bag-of-words representation.\"\"\"\n",
    "    bow_matrix = np.zeros((len(texts), len(vocab)))\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        tokens = word_tokenize(normalize_text(text))\n",
    "        token_counts = Counter(tokens)\n",
    "        \n",
    "        for token, count in token_counts.items():\n",
    "            if token in vocab:\n",
    "                bow_matrix[i, vocab[token]] = count\n",
    "    \n",
    "    return bow_matrix\n",
    "\n",
    "# Demonstrate bag of words\n",
    "sample_texts = [\n",
    "    \"natural language processing\",\n",
    "    \"language models are powerful\",\n",
    "    \"processing natural language\"\n",
    "]\n",
    "\n",
    "bow = bag_of_words(sample_texts, vocab)\n",
    "print(\"BoW shape:\", bow.shape)\n",
    "# print(\"First document BoW:\", bow[0][:10])  # Show first 10 features\n",
    "print(\"First document BoW:\", bow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (3, 14)\n",
      "First document TF-IDF: [ 0.00000000e+00  0.00000000e+00  1.35155036e-01 -1.11111120e-11\n",
      "  1.35155036e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "def compute_tfidf(bow_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute TF-IDF from bag-of-words matrix.\"\"\"\n",
    "    # Term frequency (normalized by document length)\n",
    "    tf = bow_matrix / (bow_matrix.sum(axis=1, keepdims=True) + 1e-10)\n",
    "    \n",
    "    # Document frequency\n",
    "    df = (bow_matrix > 0).sum(axis=0)\n",
    "    \n",
    "    # Inverse document frequency\n",
    "    idf = np.log(bow_matrix.shape[0] / (df + 1e-10))\n",
    "    \n",
    "    # TF-IDF\n",
    "    tfidf = tf * idf\n",
    "    \n",
    "    return tfidf\n",
    "\n",
    "# Demonstrate TF-IDF\n",
    "tfidf_matrix = compute_tfidf(bow)\n",
    "print(\"TF-IDF shape:\", tfidf_matrix.shape)\n",
    "print(\"First document TF-IDF:\", tfidf_matrix[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercises\n",
    "\n",
    "Try these exercises to reinforce your understanding:\n",
    "\n",
    "1. **Text Preprocessing Pipeline**: Create a complete preprocessing pipeline that handles:\n",
    "   - Case normalization\n",
    "   - Punctuation handling\n",
    "   - Number normalization\n",
    "   - Stopword removal\n",
    "\n",
    "2. **Custom Tokenizer**: Implement a tokenizer that can handle:\n",
    "   - URLs and email addresses\n",
    "   - Hashtags and mentions\n",
    "   - Emoticons and emojis\n",
    "\n",
    "3. **Vocabulary Analysis**: Analyze a text corpus to find:\n",
    "   - Most frequent words\n",
    "   - Vocabulary growth curve\n",
    "   - Out-of-vocabulary rate for different vocabulary sizes\n",
    "\n",
    "4. **N-gram Language Model**: Build a simple n-gram language model that can:\n",
    "   - Calculate n-gram probabilities\n",
    "   - Generate text using the model\n",
    "   - Handle unseen n-grams with smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "- **Text Normalization**: Converting text to standard formats\n",
    "- **Tokenization**: Breaking text into meaningful units\n",
    "  - Word-level, character-level, and subword tokenization\n",
    "  - Byte Pair Encoding (BPE) algorithm\n",
    "- **Vocabulary Construction**: Building and managing vocabularies\n",
    "- **Encoding Schemes**: Converting text to numerical representations\n",
    "- **Linguistic Representations**: N-grams, BoW, and TF-IDF\n",
    "\n",
    "These concepts form the foundation for understanding how modern language models process and represent text. In the next notebook, we'll explore neural network fundamentals that build upon these representations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

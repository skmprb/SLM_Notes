{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Foundations of Natural Language Processing\n",
    "\n",
    "This notebook covers the fundamental concepts of NLP that form the foundation for understanding Large Language Models.\n",
    "\n",
    "## Topics Covered:\n",
    "- Text normalization\n",
    "- Tokenization methods\n",
    "- Vocabulary construction\n",
    "- Encoding schemes\n",
    "- Basic linguistic representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.4.0-cp314-cp314-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.8-cp314-cp314-win_amd64.whl.metadata (52 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp314-cp314-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.61.1-cp314-cp314-win_amd64.whl.metadata (116 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp314-cp314-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\appdata\\roaming\\python\\python314\\site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-12.1.0-cp314-cp314-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Using cached pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\admin\\appdata\\roaming\\python\\python314\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\roaming\\python\\python314\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Using cached numpy-2.4.0-cp314-cp314-win_amd64.whl (12.4 MB)\n",
      "Using cached matplotlib-3.10.8-cp314-cp314-win_amd64.whl (8.3 MB)\n",
      "Downloading contourpy-1.3.3-cp314-cp314-win_amd64.whl (232 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp314-cp314-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.3 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.8/2.3 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.8/2.3 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 1.0/2.3 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.3/2.3 MB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.6/2.3 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.1/2.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 1.2 MB/s  0:00:01\n",
      "Downloading kiwisolver-1.4.9-cp314-cp314-win_amd64.whl (75 kB)\n",
      "Downloading pillow-12.1.0-cp314-cp314-win_amd64.whl (7.2 MB)\n",
      "   ---------------------------------------- 0.0/7.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/7.2 MB 3.2 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.0/7.2 MB 2.9 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.0/7.2 MB 2.9 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.3/7.2 MB 1.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.3/7.2 MB 1.6 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.6/7.2 MB 1.3 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 1.8/7.2 MB 1.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 2.1/7.2 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 2.4/7.2 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 2.9/7.2 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 3.1/7.2 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 3.4/7.2 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.4/7.2 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.4/7.2 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.4/7.2 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 3.7/7.2 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 3.9/7.2 MB 1.0 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 4.2/7.2 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 4.5/7.2 MB 1.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 5.0/7.2 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.0/7.2 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.2/7.2 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.2/7.2 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.2/7.2 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.5/7.2 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.5/7.2 MB 1.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 5.8/7.2 MB 993.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.0/7.2 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 6.6/7.2 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.8/7.2 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.2/7.2 MB 1.1 MB/s  0:00:06\n",
      "Downloading pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: pyparsing, pillow, numpy, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   ---------------------------------------- 0/8 [pyparsing]\n",
      "   ---------------------------------------- 0/8 [pyparsing]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ----- ---------------------------------- 1/8 [pillow]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   --------------- ------------------------ 3/8 [kiwisolver]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   -------------------- ------------------- 4/8 [fonttools]\n",
      "   ------------------------------ --------- 6/8 [contourpy]\n",
      "   ------------------------------ --------- 6/8 [contourpy]\n",
      "   ------------------------------ --------- 6/8 [contourpy]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [matplotlib]\n",
      "   ---------------------------------------- 8/8 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 numpy-2.4.0 pillow-12.1.0 pyparsing-3.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vocabulary Construction\n",
    "\n",
    "Vocabulary construction creates a mapping between words/tokens and numerical indices.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Collect unique tokens from training data\n",
    "2. Sort by frequency \n",
    "3. Add special tokens (`<PAD>`, `<UNK>`)\n",
    "4. Create word-to-index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample texts:\n",
      "1. Natural language processing is amazing\n",
      "2. Language models are powerful tools\n",
      "3. Processing natural language requires understanding\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Sample texts\n",
    "texts = [\n",
    "    \"Natural language processing is amazing\",\n",
    "    \"Language models are powerful tools\", \n",
    "    \"Processing natural language requires understanding\"\n",
    "]\n",
    "\n",
    "print(\"Sample texts:\")\n",
    "for i, text in enumerate(texts, 1):\n",
    "    print(f\"{i}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Natural language processing is amazing' -> ['natural', 'language', 'processing', 'is', 'amazing']\n",
      "'Language models are powerful tools' -> ['language', 'models', 'are', 'powerful', 'tools']\n",
      "'Processing natural language requires understanding' -> ['processing', 'natural', 'language', 'requires', 'understanding']\n",
      "\n",
      "All tokens: ['natural', 'language', 'processing', 'is', 'amazing', 'language', 'models', 'are', 'powerful', 'tools', 'processing', 'natural', 'language', 'requires', 'understanding']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Tokenize and collect all words\n",
    "all_tokens = []\n",
    "for text in texts:\n",
    "    tokens = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    all_tokens.extend(tokens)\n",
    "    print(f\"'{text}' -> {tokens}\")\n",
    "\n",
    "print(f\"\\nAll tokens: {all_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word frequencies:\n",
      "  'language': 3\n",
      "  'natural': 2\n",
      "  'processing': 2\n",
      "  'is': 1\n",
      "  'amazing': 1\n",
      "  'models': 1\n",
      "  'are': 1\n",
      "  'powerful': 1\n",
      "  'tools': 1\n",
      "  'requires': 1\n",
      "  'understanding': 1\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Count frequencies\n",
    "word_counts = Counter(all_tokens)\n",
    "\n",
    "print(\"Word frequencies:\")\n",
    "for word, count in word_counts.most_common():\n",
    "    print(f\"  '{word}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'language' -> index 2\n",
      "Added 'natural' -> index 3\n",
      "Added 'processing' -> index 4\n",
      "Added 'is' -> index 5\n",
      "Added 'amazing' -> index 6\n",
      "Added 'models' -> index 7\n",
      "Added 'are' -> index 8\n",
      "Added 'powerful' -> index 9\n",
      "Added 'tools' -> index 10\n",
      "Added 'requires' -> index 11\n",
      "Added 'understanding' -> index 12\n",
      "\n",
      "Final vocabulary: {'<PAD>': 0, '<UNK>': 1, 'language': 2, 'natural': 3, 'processing': 4, 'is': 5, 'amazing': 6, 'models': 7, 'are': 8, 'powerful': 9, 'tools': 10, 'requires': 11, 'understanding': 12}\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Build vocabulary\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # Special tokens first\n",
    "\n",
    "for word, count in word_counts.most_common():\n",
    "    vocab[word] = len(vocab)\n",
    "    print(f\"Added '{word}' -> index {vocab[word]}\")\n",
    "\n",
    "print(f\"\\nFinal vocabulary: {vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 One-Hot Encoding\n",
    "\n",
    "One-hot encoding represents each word as a binary vector where only one position is 1.\n",
    "\n",
    "**Example:** For vocab {'cat': 0, 'dog': 1, 'bird': 2}\n",
    "- 'cat' -> [1, 0, 0]\n",
    "- 'dog' -> [0, 1, 0] \n",
    "- 'bird' -> [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "  '<PAD>': 0\n",
      "  '<UNK>': 1\n",
      "  'cat': 2\n",
      "  'dog': 3\n",
      "  'bird': 4\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Simple vocabulary\n",
    "simple_vocab = {'<PAD>': 0, '<UNK>': 1, 'cat': 2, 'dog': 3, 'bird': 4}\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "for word, idx in simple_vocab.items():\n",
    "    print(f\"  '{word}': {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'cat' -> [0. 0. 1. 0. 0.]\n",
      "Position 2 is 1, others are 0\n"
     ]
    }
   ],
   "source": [
    "# Step 2: One-hot encode single word\n",
    "def one_hot_encode_word(word, vocab):\n",
    "    vector = np.zeros(len(vocab))\n",
    "    if word in vocab:\n",
    "        vector[vocab[word]] = 1\n",
    "    else:\n",
    "        vector[vocab['<UNK>']] = 1\n",
    "    return vector\n",
    "\n",
    "word = 'cat'\n",
    "encoded = one_hot_encode_word(word, simple_vocab)\n",
    "print(f\"'{word}' -> {encoded}\")\n",
    "print(f\"Position {simple_vocab[word]} is 1, others are 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vocab size 50,000 and sentence length 20:\n",
      "Memory needed: 1,000,000 numbers\n",
      "Sparsity: Only 0.0020% are 1s, rest are 0s\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Memory usage problem\n",
    "vocab_size = 50000  # Realistic size\n",
    "sentence_length = 20\n",
    "memory_needed = vocab_size * sentence_length\n",
    "\n",
    "print(f\"For vocab size {vocab_size:,} and sentence length {sentence_length}:\")\n",
    "print(f\"Memory needed: {memory_needed:,} numbers\")\n",
    "print(f\"Sparsity: Only {(1/vocab_size)*100:.4f}% are 1s, rest are 0s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Integer Encoding\n",
    "\n",
    "Integer encoding represents each word as a single integer (its vocabulary index).\n",
    "\n",
    "**Example:** 'cat dog bird' -> [2, 3, 4]\n",
    "\n",
    "**Pros:** Memory efficient\n",
    "**Cons:** Implies false ordering (cat < dog < bird)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['cat', 'and', 'dog', 'are', 'pets']\n",
      "Encoded:  [2, 1, 3, 1, 1]\n",
      "  'cat' -> 2 (known)\n",
      "  'and' -> 1 (unknown)\n",
      "  'dog' -> 3 (known)\n",
      "  'are' -> 1 (unknown)\n",
      "  'pets' -> 1 (unknown)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Encode sequence\n",
    "def encode_sequence(words, vocab):\n",
    "    return [vocab.get(word, vocab['<UNK>']) for word in words]\n",
    "\n",
    "sentence = ['cat', 'and', 'dog', 'are', 'pets']\n",
    "encoded = encode_sequence(sentence, simple_vocab)\n",
    "\n",
    "print(f\"Original: {sentence}\")\n",
    "print(f\"Encoded:  {encoded}\")\n",
    "\n",
    "for word, idx in zip(sentence, encoded):\n",
    "    status = \"(known)\" if word in simple_vocab else \"(unknown)\"\n",
    "    print(f\"  '{word}' -> {idx} {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer encoding: 12 bytes\n",
      "One-hot encoding: 60 bytes\n",
      "Memory ratio: 5:1\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Memory comparison\n",
    "sentence = ['cat', 'dog', 'bird']\n",
    "vocab_size = len(simple_vocab)\n",
    "\n",
    "# Integer encoding: 3 numbers\n",
    "integer_memory = len(sentence) * 4  # 4 bytes per int\n",
    "\n",
    "# One-hot encoding: 3 × 5 = 15 numbers  \n",
    "onehot_memory = len(sentence) * vocab_size * 4  # 4 bytes per float\n",
    "\n",
    "print(f\"Integer encoding: {integer_memory} bytes\")\n",
    "print(f\"One-hot encoding: {onehot_memory} bytes\")\n",
    "print(f\"Memory ratio: {onehot_memory // integer_memory}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 N-grams\n",
    "\n",
    "N-grams are contiguous sequences of N words that capture local patterns.\n",
    "\n",
    "- **Unigrams:** Individual words\n",
    "- **Bigrams:** Word pairs  \n",
    "- **Trigrams:** Word triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'the cat sat on the mat'\n",
      "Tokens: ['the', 'cat', 'sat', 'on', 'the', 'mat']\n",
      "1-grams: [('the',), ('cat',), ('sat',), ('on',), ('the',), ('mat',)]\n",
      "2-grams: [('the', 'cat'), ('cat', 'sat'), ('sat', 'on'), ('on', 'the'), ('the', 'mat')]\n",
      "3-grams: [('the', 'cat', 'sat'), ('cat', 'sat', 'on'), ('sat', 'on', 'the'), ('on', 'the', 'mat')]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Generate N-grams\n",
    "text = \"the cat sat on the mat\"\n",
    "tokens = text.split()\n",
    "\n",
    "def generate_ngrams(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngrams.append(tuple(tokens[i:i + n]))\n",
    "    return ngrams\n",
    "\n",
    "print(f\"Text: '{text}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "for n in range(1, 4):\n",
    "    ngrams = generate_ngrams(tokens, n)\n",
    "    print(f\"{n}-grams: {ngrams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram relationships:\n",
      "  1. 'the' followed by 'cat'\n",
      "  2. 'cat' followed by 'sat'\n",
      "  3. 'sat' followed by 'on'\n",
      "  4. 'on' followed by 'the'\n",
      "  5. 'the' followed by 'mat'\n",
      "\n",
      "Trigram contexts:\n",
      "  1. 'the cat' -> 'sat'\n",
      "  2. 'cat sat' -> 'on'\n",
      "  3. 'sat on' -> 'the'\n",
      "  4. 'on the' -> 'mat'\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Show context capture\n",
    "bigrams = generate_ngrams(tokens, 2)\n",
    "print(\"Bigram relationships:\")\n",
    "for i, (word1, word2) in enumerate(bigrams):\n",
    "    print(f\"  {i+1}. '{word1}' followed by '{word2}'\")\n",
    "\n",
    "trigrams = generate_ngrams(tokens, 3)  \n",
    "print(\"\\nTrigram contexts:\")\n",
    "for i, (w1, w2, w3) in enumerate(trigrams):\n",
    "    print(f\"  {i+1}. '{w1} {w2}' -> '{w3}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Bag of Words (BoW)\n",
    "\n",
    "BoW represents text as word frequency counts, ignoring order.\n",
    "\n",
    "**Example:**\n",
    "- Doc1: 'cat dog' -> [1, 1, 0] (1 cat, 1 dog, 0 bird)\n",
    "- Doc2: 'dog bird' -> [0, 1, 1] (0 cat, 1 dog, 1 bird)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: ['cat dog', 'dog bird', 'cat cat dog', 'bird bird cat']\n",
      "Vocabulary: ['bird', 'cat', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Sample documents\n",
    "documents = [\n",
    "    \"cat dog\",\n",
    "    \"dog bird\", \n",
    "    \"cat cat dog\",\n",
    "    \"bird bird cat\"\n",
    "]\n",
    "\n",
    "# Build vocabulary\n",
    "all_words = []\n",
    "for doc in documents:\n",
    "    all_words.extend(doc.split())\n",
    "\n",
    "vocabulary = sorted(set(all_words))\n",
    "print(f\"Documents: {documents}\")\n",
    "print(f\"Vocabulary: {vocabulary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1 ('cat dog'):\n",
      "  'cat' (index 1): 1\n",
      "  'dog' (index 2): 1\n",
      "Doc 2 ('dog bird'):\n",
      "  'dog' (index 2): 1\n",
      "  'bird' (index 0): 1\n",
      "Doc 3 ('cat cat dog'):\n",
      "  'cat' (index 1): 2\n",
      "  'dog' (index 2): 1\n",
      "Doc 4 ('bird bird cat'):\n",
      "  'bird' (index 0): 2\n",
      "  'cat' (index 1): 1\n",
      "\n",
      "BoW Matrix:\n",
      "[[0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [2. 1. 0.]]\n",
      "Columns: ['bird', 'cat', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create BoW matrix\n",
    "bow_matrix = np.zeros((len(documents), len(vocabulary)))\n",
    "\n",
    "for doc_idx, doc in enumerate(documents):\n",
    "    word_counts = Counter(doc.split())\n",
    "    print(f\"Doc {doc_idx+1} ('{doc}'):\")\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        word_idx = vocabulary.index(word)\n",
    "        bow_matrix[doc_idx, word_idx] = count\n",
    "        print(f\"  '{word}' (index {word_idx}): {count}\")\n",
    "\n",
    "print(f\"\\nBoW Matrix:\\n{bow_matrix}\")\n",
    "print(f\"Columns: {vocabulary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'dog bites man' -> [1, 1, 1]\n",
      "'man bites dog' -> [1, 1, 1]\n",
      "Identical vectors: True\n",
      "Problem: Different meanings, same BoW representation!\n"
     ]
    }
   ],
   "source": [
    "# Step 3: BoW limitation - word order ignored\n",
    "doc_a = \"dog bites man\"\n",
    "doc_b = \"man bites dog\"\n",
    "\n",
    "vocab_ab = sorted(set(doc_a.split() + doc_b.split()))\n",
    "\n",
    "def create_bow_vector(doc, vocab):\n",
    "    word_counts = Counter(doc.split())\n",
    "    return [word_counts.get(word, 0) for word in vocab]\n",
    "\n",
    "bow_a = create_bow_vector(doc_a, vocab_ab)\n",
    "bow_b = create_bow_vector(doc_b, vocab_ab)\n",
    "\n",
    "print(f\"'{doc_a}' -> {bow_a}\")\n",
    "print(f\"'{doc_b}' -> {bow_b}\")\n",
    "print(f\"Identical vectors: {bow_a == bow_b}\")\n",
    "print(\"Problem: Different meanings, same BoW representation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 TF-IDF\n",
    "\n",
    "TF-IDF weights words by importance: TF-IDF = TF × IDF\n",
    "\n",
    "- **TF (Term Frequency):** How often word appears in document\n",
    "- **IDF (Inverse Document Frequency):** How rare word is across all documents\n",
    "\n",
    "**Result:** Common words get low scores, rare meaningful words get high scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: ['the cat sat on the mat', 'the dog ran in the park', 'cats and dogs are pets']\n",
      "Vocabulary: ['and', 'are', 'cat', 'cats', 'dog', 'dogs', 'in', 'mat', 'on', 'park', 'pets', 'ran', 'sat', 'the']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Sample documents\n",
    "documents = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog ran in the park\", \n",
    "    \"cats and dogs are pets\"\n",
    "]\n",
    "\n",
    "# Build vocabulary\n",
    "all_words = []\n",
    "for doc in documents:\n",
    "    all_words.extend(doc.split())\n",
    "vocabulary = sorted(set(all_words))\n",
    "\n",
    "print(f\"Documents: {documents}\")\n",
    "print(f\"Vocabulary: {vocabulary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1: 'the cat sat on the mat'\n",
      "  Total words: 6\n",
      "  'cat': 1/6 = 0.167\n",
      "  'mat': 1/6 = 0.167\n",
      "  'on': 1/6 = 0.167\n",
      "  'sat': 1/6 = 0.167\n",
      "  'the': 2/6 = 0.333\n",
      "\n",
      "Doc 2: 'the dog ran in the park'\n",
      "  Total words: 6\n",
      "  'dog': 1/6 = 0.167\n",
      "  'in': 1/6 = 0.167\n",
      "  'park': 1/6 = 0.167\n",
      "  'ran': 1/6 = 0.167\n",
      "  'the': 2/6 = 0.333\n",
      "\n",
      "Doc 3: 'cats and dogs are pets'\n",
      "  Total words: 5\n",
      "  'and': 1/5 = 0.200\n",
      "  'are': 1/5 = 0.200\n",
      "  'cats': 1/5 = 0.200\n",
      "  'dogs': 1/5 = 0.200\n",
      "  'pets': 1/5 = 0.200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Calculate TF (Term Frequency)\n",
    "def calculate_tf(doc):\n",
    "    words = doc.split()\n",
    "    word_count = len(words)\n",
    "    tf_dict = {}\n",
    "    \n",
    "    for word in vocabulary:\n",
    "        tf_dict[word] = words.count(word) / word_count\n",
    "    return tf_dict\n",
    "\n",
    "tf_docs = []\n",
    "for i, doc in enumerate(documents):\n",
    "    tf = calculate_tf(doc)\n",
    "    tf_docs.append(tf)\n",
    "    \n",
    "    print(f\"Doc {i+1}: '{doc}'\")\n",
    "    words = doc.split()\n",
    "    print(f\"  Total words: {len(words)}\")\n",
    "    \n",
    "    for word in vocabulary:\n",
    "        if tf[word] > 0:\n",
    "            count = words.count(word)\n",
    "            print(f\"  '{word}': {count}/{len(words)} = {tf[word]:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF calculation:\n",
      "'and': appears in 1/3 docs\n",
      "  IDF = log(3/1) = 1.099\n",
      "  -> Rare (high IDF)\n",
      "\n",
      "'are': appears in 1/3 docs\n",
      "  IDF = log(3/1) = 1.099\n",
      "  -> Rare (high IDF)\n",
      "\n",
      "'cat': appears in 1/3 docs\n",
      "  IDF = log(3/1) = 1.099\n",
      "  -> Rare (high IDF)\n",
      "\n",
      "'cats': appears in 1/3 docs\n",
      "  IDF = log(3/1) = 1.099\n",
      "  -> Rare (high IDF)\n",
      "\n",
      "'dog': appears in 1/3 docs\n",
      "  IDF = log(3/1) = 1.099\n",
      "  -> Rare (high IDF)\n",
      "\n",
      "'dogs': appears in 1/3 docs\n",
      "  IDF = log(3/1) = 1.099\n",
      "  -> Rare (high IDF)\n",
      "\n",
      "'in': appears in 1/3 docs\n",
      "  IDF = log(3/1) = 1.099\n",
      "  -> Rare (high IDF)\n",
      "\n",
      "'mat': appears in 1/3 docs\n",
      "  IDF = log(3/1) = 1.099\n",
      "  -> Rare (high IDF)\n",
      "\n",
      "'on': appears in 1/3 docs\n",
      "  IDF = log(3/1) = 1.099\n",
      "  -> Rare (high IDF)\n",
      "\n",
      "'park': appears in 1/3 docs\n",
      "  IDF = log(3/1) = 1.099\n",
      "  -> Rare (high IDF)\n",
      "\n",
      "'pets': appears in 1/3 docs\n",
      "  IDF = log(3/1) = 1.099\n",
      "  -> Rare (high IDF)\n",
      "\n",
      "'ran': appears in 1/3 docs\n",
      "  IDF = log(3/1) = 1.099\n",
      "  -> Rare (high IDF)\n",
      "\n",
      "'sat': appears in 1/3 docs\n",
      "  IDF = log(3/1) = 1.099\n",
      "  -> Rare (high IDF)\n",
      "\n",
      "'the': appears in 2/3 docs\n",
      "  IDF = log(3/2) = 0.405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Calculate IDF (Inverse Document Frequency)\n",
    "def calculate_idf(vocabulary, documents):\n",
    "    idf_dict = {}\n",
    "    total_docs = len(documents)\n",
    "    \n",
    "    for word in vocabulary:\n",
    "        docs_containing_word = sum(1 for doc in documents if word in doc.split())\n",
    "        if docs_containing_word > 0:\n",
    "            idf_dict[word] = math.log(total_docs / docs_containing_word)\n",
    "        else:\n",
    "            idf_dict[word] = 0\n",
    "    return idf_dict\n",
    "\n",
    "idf_values = calculate_idf(vocabulary, documents)\n",
    "\n",
    "print(\"IDF calculation:\")\n",
    "for word in vocabulary:\n",
    "    docs_with_word = sum(1 for doc in documents if word in doc.split())\n",
    "    print(f\"'{word}': appears in {docs_with_word}/{len(documents)} docs\")\n",
    "    print(f\"  IDF = log({len(documents)}/{docs_with_word}) = {idf_values[word]:.3f}\")\n",
    "    \n",
    "    if docs_with_word == len(documents):\n",
    "        print(\"  -> Very common (low IDF)\")\n",
    "    elif docs_with_word == 1:\n",
    "        print(\"  -> Rare (high IDF)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1: 'the cat sat on the mat'\n",
      "  TF-IDF scores (TF × IDF):\n",
      "    'cat': 0.167 × 1.099 = 0.183\n",
      "    'mat': 0.167 × 1.099 = 0.183\n",
      "    'on': 0.167 × 1.099 = 0.183\n",
      "    'sat': 0.167 × 1.099 = 0.183\n",
      "    'the': 0.333 × 0.405 = 0.135\n",
      "\n",
      "Doc 2: 'the dog ran in the park'\n",
      "  TF-IDF scores (TF × IDF):\n",
      "    'dog': 0.167 × 1.099 = 0.183\n",
      "    'in': 0.167 × 1.099 = 0.183\n",
      "    'park': 0.167 × 1.099 = 0.183\n",
      "    'ran': 0.167 × 1.099 = 0.183\n",
      "    'the': 0.333 × 0.405 = 0.135\n",
      "\n",
      "Doc 3: 'cats and dogs are pets'\n",
      "  TF-IDF scores (TF × IDF):\n",
      "    'and': 0.200 × 1.099 = 0.220\n",
      "    'are': 0.200 × 1.099 = 0.220\n",
      "    'cats': 0.200 × 1.099 = 0.220\n",
      "    'dogs': 0.200 × 1.099 = 0.220\n",
      "    'pets': 0.200 × 1.099 = 0.220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Calculate TF-IDF = TF × IDF\n",
    "tfidf_docs = []\n",
    "for i, (doc, tf) in enumerate(zip(documents, tf_docs)):\n",
    "    tfidf = {word: tf[word] * idf_values[word] for word in vocabulary}\n",
    "    tfidf_docs.append(tfidf)\n",
    "    \n",
    "    print(f\"Doc {i+1}: '{doc}'\")\n",
    "    print(\"  TF-IDF scores (TF × IDF):\")\n",
    "    \n",
    "    # Show top words by TF-IDF score\n",
    "    sorted_words = sorted(tfidf.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words:\n",
    "        if score > 0:\n",
    "            tf_val = tf[word]\n",
    "            idf_val = idf_values[word]\n",
    "            print(f\"    '{word}': {tf_val:.3f} × {idf_val:.3f} = {score:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Matrix (raw counts):\n",
      "[[0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 2.]\n",
      " [0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 2.]\n",
      " [1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "\n",
      "TF-IDF Matrix (weighted scores):\n",
      "[[0.    0.    0.183 0.    0.    0.    0.    0.183 0.183 0.    0.    0.\n",
      "  0.183 0.135]\n",
      " [0.    0.    0.    0.    0.183 0.    0.183 0.    0.    0.183 0.    0.183\n",
      "  0.    0.135]\n",
      " [0.22  0.22  0.    0.22  0.    0.22  0.    0.    0.    0.    0.22  0.\n",
      "  0.    0.   ]]\n",
      "\n",
      "Word 'the' (very common):\n",
      "  BoW scores: [2. 2. 0.]\n",
      "  TF-IDF scores: [0.135 0.135 0.   ]\n",
      "  -> TF-IDF reduces importance of common words\n",
      "\n",
      "Word 'pets' (rare):\n",
      "  BoW scores: [0. 0. 1.]\n",
      "  TF-IDF scores: [0.   0.   0.22]\n",
      "  -> TF-IDF boosts importance of rare, distinctive words\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Compare BoW vs TF-IDF\n",
    "# Create BoW matrix\n",
    "bow_matrix = np.zeros((len(documents), len(vocabulary)))\n",
    "for doc_idx, doc in enumerate(documents):\n",
    "    words = doc.split()\n",
    "    for word_idx, word in enumerate(vocabulary):\n",
    "        bow_matrix[doc_idx, word_idx] = words.count(word)\n",
    "\n",
    "# Create TF-IDF matrix\n",
    "tfidf_matrix = np.zeros((len(documents), len(vocabulary)))\n",
    "for doc_idx, tfidf in enumerate(tfidf_docs):\n",
    "    for word_idx, word in enumerate(vocabulary):\n",
    "        tfidf_matrix[doc_idx, word_idx] = tfidf[word]\n",
    "\n",
    "print(\"BoW Matrix (raw counts):\")\n",
    "print(bow_matrix)\n",
    "print(\"\\nTF-IDF Matrix (weighted scores):\")\n",
    "print(np.round(tfidf_matrix, 3))\n",
    "\n",
    "# Analyze common word 'the'\n",
    "the_idx = vocabulary.index('the')\n",
    "print(f\"\\nWord 'the' (very common):\")\n",
    "print(f\"  BoW scores: {bow_matrix[:, the_idx]}\")\n",
    "print(f\"  TF-IDF scores: {np.round(tfidf_matrix[:, the_idx], 3)}\")\n",
    "print(\"  -> TF-IDF reduces importance of common words\")\n",
    "\n",
    "# Analyze rare word 'pets'\n",
    "pets_idx = vocabulary.index('pets')\n",
    "print(f\"\\nWord 'pets' (rare):\")\n",
    "print(f\"  BoW scores: {bow_matrix[:, pets_idx]}\")\n",
    "print(f\"  TF-IDF scores: {np.round(tfidf_matrix[:, pets_idx], 3)}\")\n",
    "print(\"  -> TF-IDF boosts importance of rare, distinctive words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

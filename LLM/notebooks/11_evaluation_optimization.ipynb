{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 - Evaluation and Optimization\n",
    "\n",
    "This notebook covers evaluation metrics and optimization techniques for language models.\n",
    "\n",
    "## Topics Covered:\n",
    "- Evaluation metrics (Perplexity, BLEU, ROUGE)\n",
    "- Optimization techniques (Quantization, Pruning, Knowledge Distillation)\n",
    "- Caching strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class EvaluationMetrics:\n",
    "    \"\"\"Language model evaluation metrics.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def perplexity(probabilities: np.ndarray) -> float:\n",
    "        \"\"\"Calculate perplexity from token probabilities.\"\"\"\n",
    "        log_probs = np.log(probabilities + 1e-10)\n",
    "        avg_log_prob = np.mean(log_probs)\n",
    "        return np.exp(-avg_log_prob)\n",
    "    \n",
    "    @staticmethod\n",
    "    def bleu_score(reference: List[str], candidate: List[str], n: int = 4) -> float:\n",
    "        \"\"\"Calculate BLEU score.\"\"\"\n",
    "        def get_ngrams(tokens: List[str], n: int) -> Counter:\n",
    "            return Counter([tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)])\n",
    "        \n",
    "        # Calculate precision for each n-gram order\n",
    "        precisions = []\n",
    "        \n",
    "        for i in range(1, n+1):\n",
    "            ref_ngrams = get_ngrams(reference, i)\n",
    "            cand_ngrams = get_ngrams(candidate, i)\n",
    "            \n",
    "            if len(cand_ngrams) == 0:\n",
    "                precisions.append(0)\n",
    "                continue\n",
    "            \n",
    "            matches = sum(min(ref_ngrams[ngram], cand_ngrams[ngram]) \n",
    "                         for ngram in cand_ngrams)\n",
    "            precision = matches / len(cand_ngrams)\n",
    "            precisions.append(precision)\n",
    "        \n",
    "        # Brevity penalty\n",
    "        bp = min(1, np.exp(1 - len(reference) / len(candidate))) if len(candidate) > 0 else 0\n",
    "        \n",
    "        # Geometric mean of precisions\n",
    "        if all(p > 0 for p in precisions):\n",
    "            bleu = bp * np.exp(np.mean(np.log(precisions)))\n",
    "        else:\n",
    "            bleu = 0\n",
    "        \n",
    "        return bleu\n",
    "    \n",
    "    @staticmethod\n",
    "    def rouge_l(reference: List[str], candidate: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate ROUGE-L score.\"\"\"\n",
    "        def lcs_length(x: List[str], y: List[str]) -> int:\n",
    "            \"\"\"Longest common subsequence length.\"\"\"\n",
    "            m, n = len(x), len(y)\n",
    "            dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "            \n",
    "            for i in range(1, m + 1):\n",
    "                for j in range(1, n + 1):\n",
    "                    if x[i-1] == y[j-1]:\n",
    "                        dp[i][j] = dp[i-1][j-1] + 1\n",
    "                    else:\n",
    "                        dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "            \n",
    "            return dp[m][n]\n",
    "        \n",
    "        lcs_len = lcs_length(reference, candidate)\n",
    "        \n",
    "        if len(reference) == 0 or len(candidate) == 0:\n",
    "            return {'precision': 0, 'recall': 0, 'f1': 0}\n",
    "        \n",
    "        precision = lcs_len / len(candidate)\n",
    "        recall = lcs_len / len(reference)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "        \n",
    "        return {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "class ModelOptimization:\n",
    "    \"\"\"Model optimization techniques.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantize_weights(weights: np.ndarray, bits: int = 8) -> Tuple[np.ndarray, float, float]:\n",
    "        \"\"\"Quantize weights to specified bit precision.\"\"\"\n",
    "        # Calculate scale and zero point\n",
    "        w_min, w_max = weights.min(), weights.max()\n",
    "        \n",
    "        if bits == 8:\n",
    "            qmin, qmax = 0, 255\n",
    "        elif bits == 4:\n",
    "            qmin, qmax = 0, 15\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported bit width: {bits}\")\n",
    "        \n",
    "        scale = (w_max - w_min) / (qmax - qmin)\n",
    "        zero_point = qmin - w_min / scale\n",
    "        zero_point = np.clip(np.round(zero_point), qmin, qmax)\n",
    "        \n",
    "        # Quantize\n",
    "        quantized = np.clip(np.round(weights / scale + zero_point), qmin, qmax)\n",
    "        \n",
    "        # Dequantize for comparison\n",
    "        dequantized = scale * (quantized - zero_point)\n",
    "        \n",
    "        return dequantized, scale, zero_point\n",
    "    \n",
    "    @staticmethod\n",
    "    def magnitude_pruning(weights: np.ndarray, sparsity: float) -> np.ndarray:\n",
    "        \"\"\"Prune weights by magnitude.\"\"\"\n",
    "        flat_weights = weights.flatten()\n",
    "        threshold = np.percentile(np.abs(flat_weights), sparsity * 100)\n",
    "        \n",
    "        pruned_weights = weights.copy()\n",
    "        pruned_weights[np.abs(weights) < threshold] = 0\n",
    "        \n",
    "        return pruned_weights\n",
    "    \n",
    "    @staticmethod\n",
    "    def knowledge_distillation_loss(student_logits: np.ndarray, teacher_logits: np.ndarray, \n",
    "                                  temperature: float = 3.0, alpha: float = 0.7) -> float:\n",
    "        \"\"\"Calculate knowledge distillation loss.\"\"\"\n",
    "        # Soft targets from teacher\n",
    "        teacher_probs = ModelOptimization._softmax(teacher_logits / temperature)\n",
    "        student_log_probs = ModelOptimization._log_softmax(student_logits / temperature)\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_loss = -np.sum(teacher_probs * student_log_probs, axis=-1)\n",
    "        \n",
    "        return np.mean(kl_loss) * (temperature ** 2) * alpha\n",
    "    \n",
    "    @staticmethod\n",
    "    def _softmax(x: np.ndarray) -> np.ndarray:\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _log_softmax(x: np.ndarray) -> np.ndarray:\n",
    "        return x - np.log(np.sum(np.exp(x - np.max(x, axis=-1, keepdims=True)), axis=-1, keepdims=True))\n",
    "\n",
    "def demonstrate_evaluation_optimization():\n",
    "    \"\"\"Demonstrate evaluation metrics and optimization techniques.\"\"\"\n",
    "    \n",
    "    print(\"Language Model Evaluation and Optimization:\")\n",
    "    \n",
    "    # Evaluation metrics demonstration\n",
    "    evaluator = EvaluationMetrics()\n",
    "    \n",
    "    # Perplexity example\n",
    "    good_probs = np.array([0.8, 0.7, 0.9, 0.6, 0.8])\n",
    "    bad_probs = np.array([0.3, 0.2, 0.4, 0.1, 0.3])\n",
    "    \n",
    "    good_ppl = evaluator.perplexity(good_probs)\n",
    "    bad_ppl = evaluator.perplexity(bad_probs)\n",
    "    \n",
    "    print(f\"\\nPerplexity Comparison:\")\n",
    "    print(f\"  Good model: {good_ppl:.2f}\")\n",
    "    print(f\"  Bad model: {bad_ppl:.2f}\")\n",
    "    \n",
    "    # BLEU score example\n",
    "    reference = \"the cat sat on the mat\".split()\n",
    "    good_candidate = \"the cat is sitting on the mat\".split()\n",
    "    bad_candidate = \"a dog ran in the park\".split()\n",
    "    \n",
    "    good_bleu = evaluator.bleu_score(reference, good_candidate)\n",
    "    bad_bleu = evaluator.bleu_score(reference, bad_candidate)\n",
    "    \n",
    "    print(f\"\\nBLEU Score Comparison:\")\n",
    "    print(f\"  Reference: {' '.join(reference)}\")\n",
    "    print(f\"  Good candidate: {' '.join(good_candidate)} (BLEU: {good_bleu:.3f})\")\n",
    "    print(f\"  Bad candidate: {' '.join(bad_candidate)} (BLEU: {bad_bleu:.3f})\")\n",
    "    \n",
    "    # ROUGE-L example\n",
    "    rouge_good = evaluator.rouge_l(reference, good_candidate)\n",
    "    rouge_bad = evaluator.rouge_l(reference, bad_candidate)\n",
    "    \n",
    "    print(f\"\\nROUGE-L Comparison:\")\n",
    "    print(f\"  Good candidate F1: {rouge_good['f1']:.3f}\")\n",
    "    print(f\"  Bad candidate F1: {rouge_bad['f1']:.3f}\")\n",
    "    \n",
    "    # Optimization techniques\n",
    "    optimizer = ModelOptimization()\n",
    "    \n",
    "    # Generate sample weights\n",
    "    weights = np.random.randn(100, 100) * 0.1\n",
    "    \n",
    "    # Quantization\n",
    "    quant_8bit, scale_8, zp_8 = optimizer.quantize_weights(weights, bits=8)\n",
    "    quant_4bit, scale_4, zp_4 = optimizer.quantize_weights(weights, bits=4)\n",
    "    \n",
    "    quant_error_8 = np.mean((weights - quant_8bit) ** 2)\n",
    "    quant_error_4 = np.mean((weights - quant_4bit) ** 2)\n",
    "    \n",
    "    print(f\"\\nQuantization Results:\")\n",
    "    print(f\"  8-bit MSE: {quant_error_8:.6f}\")\n",
    "    print(f\"  4-bit MSE: {quant_error_4:.6f}\")\n",
    "    print(f\"  Memory reduction: 4x (FP32→INT8), 8x (FP32→INT4)\")\n",
    "    \n",
    "    # Pruning\n",
    "    sparsities = [0.5, 0.7, 0.9]\n",
    "    \n",
    "    print(f\"\\nPruning Results:\")\n",
    "    for sparsity in sparsities:\n",
    "        pruned = optimizer.magnitude_pruning(weights, sparsity)\n",
    "        actual_sparsity = np.mean(pruned == 0)\n",
    "        print(f\"  Target {sparsity*100}% sparse → Actual {actual_sparsity*100:.1f}% sparse\")\n",
    "    \n",
    "    # Knowledge distillation\n",
    "    teacher_logits = np.random.randn(10, 1000) * 2\n",
    "    student_logits = np.random.randn(10, 1000) * 1.5\n",
    "    \n",
    "    kd_loss = optimizer.knowledge_distillation_loss(student_logits, teacher_logits)\n",
    "    print(f\"\\nKnowledge Distillation Loss: {kd_loss:.4f}\")\n",
    "    \n",
    "    # Visualizations\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Perplexity vs probability\n",
    "    plt.subplot(3, 3, 1)\n",
    "    probs = np.linspace(0.1, 0.9, 20)\n",
    "    perplexities = [evaluator.perplexity(np.array([p])) for p in probs]\n",
    "    \n",
    "    plt.plot(probs, perplexities, 'b-', linewidth=2)\n",
    "    plt.xlabel('Token Probability')\n",
    "    plt.ylabel('Perplexity')\n",
    "    plt.title('Perplexity vs Token Probability')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Quantization error analysis\n",
    "    plt.subplot(3, 3, 2)\n",
    "    \n",
    "    bit_widths = [32, 16, 8, 4]\n",
    "    memory_usage = [4, 2, 1, 0.5]  # Bytes per parameter\n",
    "    \n",
    "    # Simulate quantization errors\n",
    "    errors = [0, 0.001, quant_error_8, quant_error_4]\n",
    "    \n",
    "    plt.scatter(memory_usage, errors, s=100, alpha=0.7)\n",
    "    \n",
    "    for i, bits in enumerate(bit_widths):\n",
    "        plt.annotate(f'{bits}-bit', (memory_usage[i], errors[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.xlabel('Memory per Parameter (Bytes)')\n",
    "    plt.ylabel('Quantization Error (MSE)')\n",
    "    plt.title('Memory vs Accuracy Trade-off')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Pruning sparsity analysis\n",
    "    plt.subplot(3, 3, 3)\n",
    "    \n",
    "    sparsity_levels = np.linspace(0, 0.95, 20)\n",
    "    \n",
    "    # Simulate performance degradation\n",
    "    performance = 100 * (1 - sparsity_levels) ** 0.3\n",
    "    speedup = 1 / (1 - sparsity_levels * 0.8)\n",
    "    \n",
    "    ax1 = plt.gca()\n",
    "    ax1.plot(sparsity_levels * 100, performance, 'r-', label='Performance', linewidth=2)\n",
    "    ax1.set_xlabel('Sparsity (%)')\n",
    "    ax1.set_ylabel('Performance (%)', color='r')\n",
    "    ax1.tick_params(axis='y', labelcolor='r')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(sparsity_levels * 100, speedup, 'b-', label='Speedup', linewidth=2)\n",
    "    ax2.set_ylabel('Speedup (x)', color='b')\n",
    "    ax2.tick_params(axis='y', labelcolor='b')\n",
    "    \n",
    "    plt.title('Pruning: Performance vs Speedup')\n",
    "    \n",
    "    # BLEU score components\n",
    "    plt.subplot(3, 3, 4)\n",
    "    \n",
    "    n_grams = ['1-gram', '2-gram', '3-gram', '4-gram']\n",
    "    \n",
    "    # Calculate n-gram precisions for good candidate\n",
    "    precisions = []\n",
    "    for n in range(1, 5):\n",
    "        ref_ngrams = Counter([tuple(reference[i:i+n]) for i in range(len(reference)-n+1)])\n",
    "        cand_ngrams = Counter([tuple(good_candidate[i:i+n]) for i in range(len(good_candidate)-n+1)])\n",
    "        \n",
    "        if len(cand_ngrams) > 0:\n",
    "            matches = sum(min(ref_ngrams[ngram], cand_ngrams[ngram]) for ngram in cand_ngrams)\n",
    "            precision = matches / len(cand_ngrams)\n",
    "        else:\n",
    "            precision = 0\n",
    "        precisions.append(precision)\n",
    "    \n",
    "    plt.bar(n_grams, precisions, alpha=0.7)\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('BLEU N-gram Precisions')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Model compression comparison\n",
    "    plt.subplot(3, 3, 5)\n",
    "    \n",
    "    techniques = ['Original', 'Quantization\\n(8-bit)', 'Pruning\\n(50%)', 'Distillation\\n(0.5x)']\n",
    "    model_sizes = [100, 25, 50, 50]  # Relative sizes\n",
    "    performances = [100, 98, 95, 92]  # Performance retention\n",
    "    \n",
    "    x = np.arange(len(techniques))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, model_sizes, width, label='Model Size', alpha=0.7)\n",
    "    plt.bar(x + width/2, performances, width, label='Performance', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Optimization Technique')\n",
    "    plt.ylabel('Relative Score')\n",
    "    plt.title('Optimization Techniques Comparison')\n",
    "    plt.xticks(x, techniques, rotation=45)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Knowledge distillation temperature effect\n",
    "    plt.subplot(3, 3, 6)\n",
    "    \n",
    "    temperatures = np.linspace(1, 10, 20)\n",
    "    \n",
    "    # Simulate how temperature affects distillation\n",
    "    teacher_entropy = []\n",
    "    for temp in temperatures:\n",
    "        soft_probs = optimizer._softmax(teacher_logits[0] / temp)\n",
    "        entropy = -np.sum(soft_probs * np.log(soft_probs + 1e-10))\n",
    "        teacher_entropy.append(entropy)\n",
    "    \n",
    "    plt.plot(temperatures, teacher_entropy, 'g-', linewidth=2)\n",
    "    plt.xlabel('Temperature')\n",
    "    plt.ylabel('Teacher Output Entropy')\n",
    "    plt.title('Temperature Effect on Soft Targets')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Evaluation metrics comparison\n",
    "    plt.subplot(3, 3, 7)\n",
    "    \n",
    "    metrics = ['Perplexity\\n(lower better)', 'BLEU\\n(higher better)', 'ROUGE-L\\n(higher better)']\n",
    "    good_scores = [good_ppl, good_bleu, rouge_good['f1']]\n",
    "    bad_scores = [bad_ppl, bad_bleu, rouge_bad['f1']]\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    good_norm = [1/good_ppl, good_bleu, rouge_good['f1']]\n",
    "    bad_norm = [1/bad_ppl, bad_bleu, rouge_bad['f1']]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, good_norm, width, label='Good Model', alpha=0.7)\n",
    "    plt.bar(x + width/2, bad_norm, width, label='Bad Model', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Normalized Score')\n",
    "    plt.title('Evaluation Metrics Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Optimization pipeline\n",
    "    plt.subplot(3, 3, 8)\n",
    "    \n",
    "    pipeline_steps = ['Original', 'Distill', 'Quantize', 'Prune']\n",
    "    cumulative_compression = [1, 2, 8, 16]  # Cumulative compression ratio\n",
    "    \n",
    "    plt.plot(pipeline_steps, cumulative_compression, 'o-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Optimization Step')\n",
    "    plt.ylabel('Compression Ratio')\n",
    "    plt.title('Optimization Pipeline')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Memory usage breakdown\n",
    "    plt.subplot(3, 3, 9)\n",
    "    \n",
    "    components = ['Weights', 'Activations', 'Gradients', 'Optimizer\\nStates']\n",
    "    memory_breakdown = [40, 20, 25, 15]  # Percentage breakdown\n",
    "    \n",
    "    plt.pie(memory_breakdown, labels=components, autopct='%1.1f%%')\n",
    "    plt.title('Training Memory Breakdown')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nOptimization Insights:\")\n",
    "    \n",
    "    print(\"\\nQuantization:\")\n",
    "    print(\"  + 2-8x memory reduction\")\n",
    "    print(\"  + Faster inference on specialized hardware\")\n",
    "    print(\"  - Some accuracy loss\")\n",
    "    \n",
    "    print(\"\\nPruning:\")\n",
    "    print(\"  + Reduces model size and computation\")\n",
    "    print(\"  + Can maintain performance with proper fine-tuning\")\n",
    "    print(\"  - Requires sparse computation support\")\n",
    "    \n",
    "    print(\"\\nKnowledge Distillation:\")\n",
    "    print(\"  + Creates smaller models with retained performance\")\n",
    "    print(\"  + Transfers knowledge from large to small models\")\n",
    "    print(\"  - Requires teacher model and additional training\")\n",
    "\n",
    "demonstrate_evaluation_optimization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
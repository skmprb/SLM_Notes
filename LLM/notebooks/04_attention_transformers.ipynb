{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Attention Mechanisms & Transformers\n",
    "\n",
    "This notebook covers attention mechanisms and the transformer architecture that revolutionized NLP.\n",
    "\n",
    "## Topics Covered:\n",
    "- Attention concept and mechanisms\n",
    "- Self-attention and cross-attention\n",
    "- Transformer architecture components\n",
    "- Multi-head attention\n",
    "- Positional encoding\n",
    "- Layer normalization and residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.2.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting pyparsing>=3\n",
      "  Downloading pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
      "Collecting pillow>=8\n",
      "  Downloading pillow-12.1.0-cp310-cp310-win_amd64.whl (7.0 MB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.61.1-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (24.0)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.2.6)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 pillow-12.1.0 pyparsing-3.3.1\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional\n",
    "import math\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 103\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSDP output shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msdp_output\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdditive output shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madd_output\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 103\u001b[0m \u001b[43mcompare_attention_mechanisms\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 66\u001b[0m, in \u001b[0;36mcompare_attention_mechanisms\u001b[1;34m()\u001b[0m\n\u001b[0;32m     63\u001b[0m V \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(batch_size, seq_len, d_model)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Scaled dot-product attention\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m sdp_output, sdp_weights \u001b[38;5;241m=\u001b[39m \u001b[43mAttentionMechanisms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Additive attention (simplified - using first query)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m d_att \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m, in \u001b[0;36mAttentionMechanisms.scaled_dot_product_attention\u001b[1;34m(Q, K, V, mask)\u001b[0m\n\u001b[0;32m      8\u001b[0m d_k \u001b[38;5;241m=\u001b[39m Q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Compute attention scores\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m scores \u001b[38;5;241m=\u001b[39m Q \u001b[38;5;241m@\u001b[39m \u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(d_k)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Apply mask if provided\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "class AttentionMechanisms:\n",
    "    \"\"\"Implementation of various attention mechanisms.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def scaled_dot_product_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, \n",
    "                                   mask: Optional[np.ndarray] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Scaled dot-product attention.\"\"\"\n",
    "        d_k = Q.shape[-1]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = np.where(mask == 0, -1e9, scores)\n",
    "        \n",
    "        # Softmax to get attention weights\n",
    "        attention_weights = AttentionMechanisms.softmax(scores)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = attention_weights @ V\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Numerically stable softmax.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def additive_attention(query: np.ndarray, keys: np.ndarray, values: np.ndarray,\n",
    "                          W_q: np.ndarray, W_k: np.ndarray, v: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Additive (Bahdanau) attention.\"\"\"\n",
    "        # Transform query and keys\n",
    "        query_proj = query @ W_q  # (batch, d_model) @ (d_model, d_att) -> (batch, d_att)\n",
    "        keys_proj = keys @ W_k    # (batch, seq_len, d_model) @ (d_model, d_att) -> (batch, seq_len, d_att)\n",
    "        \n",
    "        # Add query to each key position\n",
    "        combined = np.tanh(query_proj[:, None, :] + keys_proj)  # (batch, seq_len, d_att)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = combined @ v  # (batch, seq_len, d_att) @ (d_att, 1) -> (batch, seq_len, 1)\n",
    "        scores = scores.squeeze(-1)  # (batch, seq_len)\n",
    "        \n",
    "        # Softmax\n",
    "        attention_weights = AttentionMechanisms.softmax(scores)\n",
    "        \n",
    "        # Apply attention\n",
    "        output = np.sum(attention_weights[:, :, None] * values, axis=1)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Demonstrate different attention mechanisms\n",
    "def compare_attention_mechanisms():\n",
    "    \"\"\"Compare scaled dot-product vs additive attention.\"\"\"\n",
    "    \n",
    "    batch_size, seq_len, d_model = 2, 5, 8\n",
    "    \n",
    "    # Create sample data\n",
    "    Q = np.random.randn(batch_size, seq_len, d_model)\n",
    "    K = np.random.randn(batch_size, seq_len, d_model)\n",
    "    V = np.random.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Scaled dot-product attention\n",
    "    sdp_output, sdp_weights = AttentionMechanisms.scaled_dot_product_attention(Q, K, V)\n",
    "    \n",
    "    # Additive attention (simplified - using first query)\n",
    "    d_att = 4\n",
    "    W_q = np.random.randn(d_model, d_att)\n",
    "    W_k = np.random.randn(d_model, d_att)\n",
    "    v = np.random.randn(d_att, 1)\n",
    "    \n",
    "    add_output, add_weights = AttentionMechanisms.additive_attention(\n",
    "        Q[:, 0, :], K, V, W_q, W_k, v)\n",
    "    \n",
    "    # Visualize attention weights\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Scaled dot-product attention weights\n",
    "    im1 = axes[0].imshow(sdp_weights[0], cmap='Blues', aspect='auto')\n",
    "    axes[0].set_title('Scaled Dot-Product Attention')\n",
    "    axes[0].set_xlabel('Key Position')\n",
    "    axes[0].set_ylabel('Query Position')\n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    # Additive attention weights\n",
    "    im2 = axes[1].imshow(add_weights[0:1], cmap='Blues', aspect='auto')\n",
    "    axes[1].set_title('Additive Attention (Single Query)')\n",
    "    axes[1].set_xlabel('Key Position')\n",
    "    axes[1].set_ylabel('Query')\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Attention Mechanisms Comparison:\")\n",
    "    print(f\"Scaled Dot-Product: O(n²d) complexity, parallelizable\")\n",
    "    print(f\"Additive: O(n²d) complexity, more parameters\")\n",
    "    print(f\"SDP output shape: {sdp_output.shape}\")\n",
    "    print(f\"Additive output shape: {add_output.shape}\")\n",
    "\n",
    "compare_attention_mechanisms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention:\n",
    "    \"\"\"Self-attention implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int):\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = np.random.randn(d_model, d_k) / np.sqrt(d_model)\n",
    "        self.W_k = np.random.randn(d_model, d_k) / np.sqrt(d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_v) / np.sqrt(d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = np.random.randn(d_v, d_model) / np.sqrt(d_v)\n",
    "    \n",
    "    def forward(self, x: np.ndarray, mask: Optional[np.ndarray] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Forward pass of self-attention.\"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = x @ self.W_q  # (batch, seq_len, d_k)\n",
    "        K = x @ self.W_k  # (batch, seq_len, d_k)\n",
    "        V = x @ self.W_v  # (batch, seq_len, d_v)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attention_output, attention_weights = AttentionMechanisms.scaled_dot_product_attention(\n",
    "            Q, K, V, mask)\n",
    "        \n",
    "        # Output projection\n",
    "        output = attention_output @ self.W_o\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "def demonstrate_self_attention():\n",
    "    \"\"\"Demonstrate self-attention on a sequence.\"\"\"\n",
    "    \n",
    "    # Create a simple sequence with patterns\n",
    "    seq_len, d_model = 8, 16\n",
    "    \n",
    "    # Create sequence with some structure\n",
    "    x = np.random.randn(1, seq_len, d_model)\n",
    "    \n",
    "    # Add some patterns (make positions 2,3 and 5,6 similar)\n",
    "    x[0, 2] = x[0, 3] + 0.1 * np.random.randn(d_model)\n",
    "    x[0, 5] = x[0, 6] + 0.1 * np.random.randn(d_model)\n",
    "    \n",
    "    # Initialize self-attention\n",
    "    self_attn = SelfAttention(d_model=d_model, d_k=d_model//2, d_v=d_model//2)\n",
    "    \n",
    "    # Forward pass\n",
    "    output, attention_weights = self_attn.forward(x)\n",
    "    \n",
    "    # Visualize attention pattern\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(attention_weights[0], cmap='Blues', aspect='auto')\n",
    "    plt.title('Self-Attention Weights')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Show input similarity matrix\n",
    "    plt.subplot(2, 2, 2)\n",
    "    similarity = x[0] @ x[0].T\n",
    "    plt.imshow(similarity, cmap='Reds', aspect='auto')\n",
    "    plt.title('Input Similarity Matrix')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Position')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Show attention weights for specific positions\n",
    "    plt.subplot(2, 2, 3)\n",
    "    positions = [0, 2, 5]\n",
    "    for pos in positions:\n",
    "        plt.plot(attention_weights[0, pos], label=f'Query {pos}', marker='o')\n",
    "    plt.title('Attention Weights by Query Position')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Attention Weight')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Show output vs input norms\n",
    "    plt.subplot(2, 2, 4)\n",
    "    input_norms = np.linalg.norm(x[0], axis=1)\n",
    "    output_norms = np.linalg.norm(output[0], axis=1)\n",
    "    plt.plot(input_norms, label='Input', marker='o')\n",
    "    plt.plot(output_norms, label='Output', marker='s')\n",
    "    plt.title('Vector Norms')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('L2 Norm')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Self-Attention Properties:\")\n",
    "    print(\"- Each position attends to all positions in the sequence\")\n",
    "    print(\"- Attention weights show which positions are most relevant\")\n",
    "    print(\"- Similar positions tend to have higher attention weights\")\n",
    "    print(f\"- Input shape: {x.shape}\")\n",
    "    print(f\"- Output shape: {output.shape}\")\n",
    "    print(f\"- Attention weights shape: {attention_weights.shape}\")\n",
    "\n",
    "demonstrate_self_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    \"\"\"Multi-head attention implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for all heads\n",
    "        self.W_q = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_k = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "    \n",
    "    def forward(self, x: np.ndarray, mask: Optional[np.ndarray] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Forward pass of multi-head attention.\"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = x @ self.W_q  # (batch, seq_len, d_model)\n",
    "        K = x @ self.W_k\n",
    "        V = x @ self.W_v\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        K = K.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        V = V.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        # Shape: (batch, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # Apply attention to each head\n",
    "        attention_outputs = []\n",
    "        attention_weights_all = []\n",
    "        \n",
    "        for h in range(self.num_heads):\n",
    "            attn_output, attn_weights = AttentionMechanisms.scaled_dot_product_attention(\n",
    "                Q[:, h], K[:, h], V[:, h], mask)\n",
    "            attention_outputs.append(attn_output)\n",
    "            attention_weights_all.append(attn_weights)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        concat_output = np.concatenate(attention_outputs, axis=-1)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = concat_output @ self.W_o\n",
    "        \n",
    "        # Stack attention weights\n",
    "        attention_weights = np.stack(attention_weights_all, axis=1)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "def visualize_multi_head_attention():\n",
    "    \"\"\"Visualize different attention heads.\"\"\"\n",
    "    \n",
    "    # Create sample sequence\n",
    "    batch_size, seq_len, d_model = 1, 10, 64\n",
    "    num_heads = 8\n",
    "    \n",
    "    # Create structured input\n",
    "    x = np.random.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Add some patterns\n",
    "    # Pattern 1: positions 1-3 are similar\n",
    "    base_pattern1 = np.random.randn(d_model)\n",
    "    for i in range(1, 4):\n",
    "        x[0, i] = base_pattern1 + 0.1 * np.random.randn(d_model)\n",
    "    \n",
    "    # Pattern 2: positions 6-8 are similar\n",
    "    base_pattern2 = np.random.randn(d_model)\n",
    "    for i in range(6, 9):\n",
    "        x[0, i] = base_pattern2 + 0.1 * np.random.randn(d_model)\n",
    "    \n",
    "    # Initialize multi-head attention\n",
    "    mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "    \n",
    "    # Forward pass\n",
    "    output, attention_weights = mha.forward(x)\n",
    "    \n",
    "    # Visualize different heads\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for head in range(num_heads):\n",
    "        im = axes[head].imshow(attention_weights[0, head], cmap='Blues', aspect='auto')\n",
    "        axes[head].set_title(f'Head {head + 1}')\n",
    "        axes[head].set_xlabel('Key Position')\n",
    "        axes[head].set_ylabel('Query Position')\n",
    "        plt.colorbar(im, ax=axes[head])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze head specialization\n",
    "    print(\"Multi-Head Attention Analysis:\")\n",
    "    print(f\"Number of heads: {num_heads}\")\n",
    "    print(f\"Dimension per head: {d_model // num_heads}\")\n",
    "    \n",
    "    # Compute attention entropy for each head (measure of focus)\n",
    "    entropies = []\n",
    "    for head in range(num_heads):\n",
    "        head_weights = attention_weights[0, head]\n",
    "        # Compute entropy for each query position\n",
    "        head_entropy = -np.sum(head_weights * np.log(head_weights + 1e-10), axis=1)\n",
    "        entropies.append(np.mean(head_entropy))\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(range(num_heads), entropies)\n",
    "    plt.title('Attention Entropy by Head (Lower = More Focused)')\n",
    "    plt.xlabel('Head')\n",
    "    plt.ylabel('Average Entropy')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nHead Specialization:\")\n",
    "    for i, entropy in enumerate(entropies):\n",
    "        focus_level = \"High\" if entropy < np.mean(entropies) else \"Low\"\n",
    "        print(f\"Head {i+1}: Entropy = {entropy:.3f} (Focus: {focus_level})\")\n",
    "\n",
    "visualize_multi_head_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding:\n",
    "    \"\"\"Positional encoding implementations.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sinusoidal_encoding(seq_len: int, d_model: int) -> np.ndarray:\n",
    "        \"\"\"Sinusoidal positional encoding.\"\"\"\n",
    "        pe = np.zeros((seq_len, d_model))\n",
    "        \n",
    "        position = np.arange(seq_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        \n",
    "        return pe\n",
    "    \n",
    "    @staticmethod\n",
    "    def learned_encoding(seq_len: int, d_model: int) -> np.ndarray:\n",
    "        \"\"\"Learned positional encoding (random initialization).\"\"\"\n",
    "        return np.random.randn(seq_len, d_model) * 0.1\n",
    "    \n",
    "    @staticmethod\n",
    "    def relative_encoding(seq_len: int, d_model: int, max_relative_position: int = 32) -> np.ndarray:\n",
    "        \"\"\"Relative positional encoding.\"\"\"\n",
    "        # Simplified relative encoding\n",
    "        relative_positions = np.arange(seq_len)[:, None] - np.arange(seq_len)[None, :]\n",
    "        relative_positions = np.clip(relative_positions, -max_relative_position, max_relative_position)\n",
    "        \n",
    "        # Convert to embeddings (simplified)\n",
    "        relative_embeddings = np.random.randn(2 * max_relative_position + 1, d_model) * 0.1\n",
    "        \n",
    "        return relative_embeddings[relative_positions + max_relative_position]\n",
    "\n",
    "def analyze_positional_encodings():\n",
    "    \"\"\"Analyze different positional encoding methods.\"\"\"\n",
    "    \n",
    "    seq_len, d_model = 50, 64\n",
    "    \n",
    "    # Generate different encodings\n",
    "    sin_pe = PositionalEncoding.sinusoidal_encoding(seq_len, d_model)\n",
    "    learned_pe = PositionalEncoding.learned_encoding(seq_len, d_model)\n",
    "    \n",
    "    # Visualize encodings\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    # Sinusoidal encoding\n",
    "    im1 = axes[0, 0].imshow(sin_pe.T, cmap='RdBu', aspect='auto')\n",
    "    axes[0, 0].set_title('Sinusoidal Positional Encoding')\n",
    "    axes[0, 0].set_xlabel('Position')\n",
    "    axes[0, 0].set_ylabel('Dimension')\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "    \n",
    "    # Learned encoding\n",
    "    im2 = axes[0, 1].imshow(learned_pe.T, cmap='RdBu', aspect='auto')\n",
    "    axes[0, 1].set_title('Learned Positional Encoding')\n",
    "    axes[0, 1].set_xlabel('Position')\n",
    "    axes[0, 1].set_ylabel('Dimension')\n",
    "    plt.colorbar(im2, ax=axes[0, 1])\n",
    "    \n",
    "    # Show specific dimensions of sinusoidal encoding\n",
    "    positions = np.arange(seq_len)\n",
    "    for dim in [0, 1, 10, 20]:\n",
    "        axes[0, 2].plot(positions, sin_pe[:, dim], label=f'Dim {dim}')\n",
    "    axes[0, 2].set_title('Sinusoidal Encoding Dimensions')\n",
    "    axes[0, 2].set_xlabel('Position')\n",
    "    axes[0, 2].set_ylabel('Value')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Compute similarity matrices\n",
    "    sin_similarity = sin_pe @ sin_pe.T\n",
    "    learned_similarity = learned_pe @ learned_pe.T\n",
    "    \n",
    "    im3 = axes[1, 0].imshow(sin_similarity, cmap='Blues', aspect='auto')\n",
    "    axes[1, 0].set_title('Sinusoidal PE Similarity')\n",
    "    axes[1, 0].set_xlabel('Position')\n",
    "    axes[1, 0].set_ylabel('Position')\n",
    "    plt.colorbar(im3, ax=axes[1, 0])\n",
    "    \n",
    "    im4 = axes[1, 1].imshow(learned_similarity, cmap='Blues', aspect='auto')\n",
    "    axes[1, 1].set_title('Learned PE Similarity')\n",
    "    axes[1, 1].set_xlabel('Position')\n",
    "    axes[1, 1].set_ylabel('Position')\n",
    "    plt.colorbar(im4, ax=axes[1, 1])\n",
    "    \n",
    "    # Show distance decay for sinusoidal encoding\n",
    "    distances = []\n",
    "    similarities = []\n",
    "    \n",
    "    for i in range(seq_len):\n",
    "        for j in range(i+1, min(i+20, seq_len)):\n",
    "            distance = j - i\n",
    "            similarity = np.dot(sin_pe[i], sin_pe[j]) / (np.linalg.norm(sin_pe[i]) * np.linalg.norm(sin_pe[j]))\n",
    "            distances.append(distance)\n",
    "            similarities.append(similarity)\n",
    "    \n",
    "    axes[1, 2].scatter(distances, similarities, alpha=0.6)\n",
    "    axes[1, 2].set_title('Sinusoidal PE: Distance vs Similarity')\n",
    "    axes[1, 2].set_xlabel('Position Distance')\n",
    "    axes[1, 2].set_ylabel('Cosine Similarity')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Positional Encoding Properties:\")\n",
    "    print(\"\\nSinusoidal Encoding:\")\n",
    "    print(\"- Fixed, deterministic patterns\")\n",
    "    print(\"- Can extrapolate to longer sequences\")\n",
    "    print(\"- Different frequencies for different dimensions\")\n",
    "    print(\"- Relative position information preserved\")\n",
    "    \n",
    "    print(\"\\nLearned Encoding:\")\n",
    "    print(\"- Trainable parameters\")\n",
    "    print(\"- Can adapt to specific tasks\")\n",
    "    print(\"- Limited to training sequence length\")\n",
    "    print(\"- May learn task-specific position patterns\")\n",
    "\n",
    "analyze_positional_encodings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    \"\"\"Layer normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, eps: float = 1e-6):\n",
    "        self.eps = eps\n",
    "        self.gamma = np.ones(d_model)\n",
    "        self.beta = np.zeros(d_model)\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply layer normalization.\"\"\"\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        std = np.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "class FeedForward:\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int):\n",
    "        self.W1 = np.random.randn(d_model, d_ff) / np.sqrt(d_model)\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model) / np.sqrt(d_ff)\n",
    "        self.b2 = np.zeros(d_model)\n",
    "    \n",
    "    def gelu(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"GELU activation function.\"\"\"\n",
    "        return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass through feed-forward network.\"\"\"\n",
    "        hidden = self.gelu(x @ self.W1 + self.b1)\n",
    "        output = hidden @ self.W2 + self.b2\n",
    "        return output\n",
    "\n",
    "class TransformerBlock:\n",
    "    \"\"\"Complete transformer block.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.ln1 = LayerNorm(d_model)\n",
    "        self.ln2 = LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def apply_dropout(self, x: np.ndarray, training: bool = True) -> np.ndarray:\n",
    "        \"\"\"Apply dropout (simplified).\"\"\"\n",
    "        if training and self.dropout > 0:\n",
    "            mask = np.random.binomial(1, 1 - self.dropout, x.shape) / (1 - self.dropout)\n",
    "            return x * mask\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x: np.ndarray, mask: Optional[np.ndarray] = None, training: bool = True) -> np.ndarray:\n",
    "        \"\"\"Forward pass through transformer block.\"\"\"\n",
    "        # Multi-head attention with residual connection and layer norm\n",
    "        attn_output, _ = self.mha.forward(x, mask)\n",
    "        attn_output = self.apply_dropout(attn_output, training)\n",
    "        x1 = self.ln1.forward(x + attn_output)  # Residual connection\n",
    "        \n",
    "        # Feed-forward with residual connection and layer norm\n",
    "        ffn_output = self.ffn.forward(x1)\n",
    "        ffn_output = self.apply_dropout(ffn_output, training)\n",
    "        x2 = self.ln2.forward(x1 + ffn_output)  # Residual connection\n",
    "        \n",
    "        return x2\n",
    "\n",
    "def demonstrate_transformer_block():\n",
    "    \"\"\"Demonstrate complete transformer block.\"\"\"\n",
    "    \n",
    "    # Parameters\n",
    "    batch_size, seq_len, d_model = 2, 16, 128\n",
    "    num_heads, d_ff = 8, 512\n",
    "    \n",
    "    # Create input\n",
    "    x = np.random.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Add positional encoding\n",
    "    pos_encoding = PositionalEncoding.sinusoidal_encoding(seq_len, d_model)\n",
    "    x_with_pos = x + pos_encoding[None, :, :]\n",
    "    \n",
    "    # Initialize transformer block\n",
    "    transformer = TransformerBlock(d_model, num_heads, d_ff)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = transformer.forward(x_with_pos, training=False)\n",
    "    \n",
    "    # Analyze the transformation\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Input vs output statistics\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.hist(x.flatten(), bins=50, alpha=0.7, label='Input', density=True)\n",
    "    plt.hist(output.flatten(), bins=50, alpha=0.7, label='Output', density=True)\n",
    "    plt.title('Value Distribution')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Sequence-wise norms\n",
    "    plt.subplot(2, 3, 2)\n",
    "    input_norms = np.linalg.norm(x[0], axis=1)\n",
    "    output_norms = np.linalg.norm(output[0], axis=1)\n",
    "    plt.plot(input_norms, label='Input', marker='o')\n",
    "    plt.plot(output_norms, label='Output', marker='s')\n",
    "    plt.title('Vector Norms by Position')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('L2 Norm')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Attention pattern from MHA\n",
    "    _, attention_weights = transformer.mha.forward(x_with_pos)\n",
    "    avg_attention = np.mean(attention_weights[0], axis=0)  # Average over heads\n",
    "    \n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.imshow(avg_attention, cmap='Blues', aspect='auto')\n",
    "    plt.title('Average Attention Weights')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Feature similarity before and after\n",
    "    input_sim = x[0] @ x[0].T\n",
    "    output_sim = output[0] @ output[0].T\n",
    "    \n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.imshow(input_sim, cmap='RdBu', aspect='auto')\n",
    "    plt.title('Input Similarity Matrix')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Position')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.imshow(output_sim, cmap='RdBu', aspect='auto')\n",
    "    plt.title('Output Similarity Matrix')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Position')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Layer norm effect\n",
    "    plt.subplot(2, 3, 6)\n",
    "    # Show mean and std before and after layer norm\n",
    "    input_means = np.mean(x[0], axis=1)\n",
    "    input_stds = np.std(x[0], axis=1)\n",
    "    output_means = np.mean(output[0], axis=1)\n",
    "    output_stds = np.std(output[0], axis=1)\n",
    "    \n",
    "    positions = np.arange(seq_len)\n",
    "    plt.plot(positions, input_means, label='Input Mean', alpha=0.7)\n",
    "    plt.plot(positions, output_means, label='Output Mean', alpha=0.7)\n",
    "    plt.plot(positions, input_stds, label='Input Std', alpha=0.7, linestyle='--')\n",
    "    plt.plot(positions, output_stds, label='Output Std', alpha=0.7, linestyle='--')\n",
    "    plt.title('Statistics by Position')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Transformer Block Analysis:\")\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Parameters: ~{d_model * d_model * 4 + d_model * d_ff * 2:,} (approximate)\")\n",
    "    \n",
    "    print(\"\\nKey Components:\")\n",
    "    print(\"1. Multi-Head Attention: Captures relationships between positions\")\n",
    "    print(\"2. Feed-Forward Network: Processes each position independently\")\n",
    "    print(\"3. Residual Connections: Enable deep networks and gradient flow\")\n",
    "    print(\"4. Layer Normalization: Stabilizes training and normalizes features\")\n",
    "    \n",
    "    print(f\"\\nOutput statistics:\")\n",
    "    print(f\"Mean: {np.mean(output):.4f}, Std: {np.std(output):.4f}\")\n",
    "    print(f\"Layer norm ensures each position has mean≈0, std≈1\")\n",
    "\n",
    "demonstrate_transformer_block()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered the transformer architecture and attention mechanisms:\n",
    "\n",
    "### Key Innovations:\n",
    "\n",
    "1. **Attention Mechanisms**:\n",
    "   - Scaled dot-product attention: Efficient and parallelizable\n",
    "   - Self-attention: Each position attends to all positions\n",
    "   - Cross-attention: Attention between different sequences\n",
    "\n",
    "2. **Multi-Head Attention**:\n",
    "   - Multiple attention heads capture different types of relationships\n",
    "   - Parallel computation of attention in different subspaces\n",
    "   - Concatenation and projection of head outputs\n",
    "\n",
    "3. **Positional Encoding**:\n",
    "   - Sinusoidal: Fixed patterns with extrapolation capability\n",
    "   - Learned: Trainable but limited to training length\n",
    "   - Relative: Encodes relative position relationships\n",
    "\n",
    "4. **Transformer Block Components**:\n",
    "   - Multi-head attention for relationship modeling\n",
    "   - Feed-forward networks for position-wise processing\n",
    "   - Residual connections for gradient flow\n",
    "   - Layer normalization for training stability\n",
    "\n",
    "### Advantages over RNNs:\n",
    "- **Parallelization**: All positions processed simultaneously\n",
    "- **Long-range dependencies**: Direct connections between all positions\n",
    "- **Scalability**: Efficient for large models and datasets\n",
    "- **Interpretability**: Attention weights show model focus\n",
    "\n",
    "### Impact:\n",
    "The transformer architecture revolutionized NLP by:\n",
    "- Enabling efficient training of very large models\n",
    "- Achieving state-of-the-art results across many tasks\n",
    "- Providing the foundation for modern LLMs (GPT, BERT, T5, etc.)\n",
    "- Extending beyond NLP to vision, speech, and multimodal tasks\n",
    "\n",
    "The next notebook will explore how transformers are used for language modeling and the training of large language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

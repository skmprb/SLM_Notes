{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 - Context and Memory Handling\n",
    "\n",
    "This notebook covers techniques for handling long contexts and memory in language models.\n",
    "\n",
    "## Topics Covered:\n",
    "- Context length limitations\n",
    "- Sliding window attention\n",
    "- Long-context models\n",
    "- Retrieval-augmented generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Context Length Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ContextAnalyzer:\n",
    "    \"\"\"Analyze context length limitations and solutions.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def attention_complexity(seq_len: int, d_model: int) -> Dict[str, float]:\n",
    "        \"\"\"Calculate attention complexity metrics.\"\"\"\n",
    "        # Memory complexity: O(n²d)\n",
    "        memory_ops = seq_len ** 2 * d_model\n",
    "        \n",
    "        # Compute complexity: O(n²d)\n",
    "        compute_ops = seq_len ** 2 * d_model\n",
    "        \n",
    "        # Memory in GB (assuming float32)\n",
    "        memory_gb = memory_ops * 4 / (1024 ** 3)\n",
    "        \n",
    "        return {\n",
    "            'memory_ops': memory_ops,\n",
    "            'compute_ops': compute_ops,\n",
    "            'memory_gb': memory_gb\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def sliding_window_attention(x: np.ndarray, window_size: int) -> np.ndarray:\n",
    "        \"\"\"Implement sliding window attention.\"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Create sliding window mask\n",
    "        mask = np.zeros((seq_len, seq_len))\n",
    "        for i in range(seq_len):\n",
    "            start = max(0, i - window_size // 2)\n",
    "            end = min(seq_len, i + window_size // 2 + 1)\n",
    "            mask[i, start:end] = 1\n",
    "        \n",
    "        # Simplified attention computation\n",
    "        attention_weights = np.random.rand(batch_size, seq_len, seq_len)\n",
    "        attention_weights = attention_weights * mask[None, :, :]\n",
    "        \n",
    "        # Normalize\n",
    "        attention_weights = attention_weights / (np.sum(attention_weights, axis=-1, keepdims=True) + 1e-10)\n",
    "        \n",
    "        # Apply attention\n",
    "        output = attention_weights @ x\n",
    "        \n",
    "        return output\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"Retrieval-Augmented Generation system.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int = 256):\n",
    "        self.d_model = d_model\n",
    "        self.knowledge_base = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def add_document(self, text: str):\n",
    "        \"\"\"Add document to knowledge base.\"\"\"\n",
    "        # Simulate document embedding\n",
    "        embedding = np.random.randn(self.d_model)\n",
    "        embedding = embedding / np.linalg.norm(embedding)\n",
    "        \n",
    "        self.knowledge_base.append(text)\n",
    "        self.embeddings.append(embedding)\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Retrieve relevant documents.\"\"\"\n",
    "        # Simulate query embedding\n",
    "        query_embedding = np.random.randn(self.d_model)\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for i, doc_embedding in enumerate(self.embeddings):\n",
    "            similarity = np.dot(query_embedding, doc_embedding)\n",
    "            similarities.append((self.knowledge_base[i], similarity))\n",
    "        \n",
    "        # Sort by similarity and return top-k\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "    \n",
    "    def generate_with_retrieval(self, query: str, max_length: int = 50) -> str:\n",
    "        \"\"\"Generate response using retrieved context.\"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        retrieved_docs = self.retrieve(query)\n",
    "        \n",
    "        # Combine retrieved context\n",
    "        context = \" \".join([doc for doc, _ in retrieved_docs])\n",
    "        \n",
    "        # Simulate generation (simplified)\n",
    "        response = f\"Based on the context: {context[:100]}..., the answer is: [Generated response]\"\n",
    "        \n",
    "        return response\n",
    "\n",
    "def demonstrate_context_handling():\n",
    "    \"\"\"Demonstrate context and memory handling techniques.\"\"\"\n",
    "    \n",
    "    print(\"Context and Memory Handling Analysis:\")\n",
    "    \n",
    "    # Analyze context length limitations\n",
    "    analyzer = ContextAnalyzer()\n",
    "    \n",
    "    context_lengths = [512, 1024, 2048, 4096, 8192, 16384]\n",
    "    d_model = 768\n",
    "    \n",
    "    print(\"\\nContext Length Complexity Analysis:\")\n",
    "    for seq_len in context_lengths:\n",
    "        metrics = analyzer.attention_complexity(seq_len, d_model)\n",
    "        print(f\"  Length {seq_len}: {metrics['memory_gb']:.2f} GB memory\")\n",
    "    \n",
    "    # Demonstrate sliding window attention\n",
    "    batch_size, seq_len, d_model = 2, 16, 64\n",
    "    x = np.random.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    window_sizes = [4, 8, 12]\n",
    "    print(f\"\\nSliding Window Attention (seq_len={seq_len}):\")\n",
    "    \n",
    "    for window_size in window_sizes:\n",
    "        output = analyzer.sliding_window_attention(x, window_size)\n",
    "        print(f\"  Window size {window_size}: Output shape {output.shape}\")\n",
    "    \n",
    "    # Demonstrate RAG system\n",
    "    rag = RAGSystem(d_model=128)\n",
    "    \n",
    "    # Add sample documents\n",
    "    documents = [\n",
    "        \"The transformer architecture uses self-attention mechanisms.\",\n",
    "        \"Large language models are trained on massive text corpora.\",\n",
    "        \"Attention mechanisms allow models to focus on relevant information.\",\n",
    "        \"RAG combines retrieval with generation for better factual accuracy.\",\n",
    "        \"Context windows limit how much text a model can process at once.\"\n",
    "    ]\n",
    "    \n",
    "    for doc in documents:\n",
    "        rag.add_document(doc)\n",
    "    \n",
    "    # Test retrieval\n",
    "    query = \"How do attention mechanisms work?\"\n",
    "    retrieved = rag.retrieve(query, top_k=3)\n",
    "    \n",
    "    print(f\"\\nRAG System Demo:\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Retrieved documents:\")\n",
    "    for i, (doc, score) in enumerate(retrieved):\n",
    "        print(f\"  {i+1}. {doc} (score: {score:.3f})\")\n",
    "    \n",
    "    # Generate response\n",
    "    response = rag.generate_with_retrieval(query)\n",
    "    print(f\"\\nGenerated response: {response}\")\n",
    "    \n",
    "    # Visualizations\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Context length vs memory usage\n",
    "    plt.subplot(2, 3, 1)\n",
    "    \n",
    "    memory_usage = []\n",
    "    for seq_len in context_lengths:\n",
    "        metrics = analyzer.attention_complexity(seq_len, d_model)\n",
    "        memory_usage.append(metrics['memory_gb'])\n",
    "    \n",
    "    plt.loglog(context_lengths, memory_usage, 'b-o', linewidth=2)\n",
    "    plt.xlabel('Context Length')\n",
    "    plt.ylabel('Memory Usage (GB)')\n",
    "    plt.title('Quadratic Memory Growth')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sliding window attention pattern\n",
    "    plt.subplot(2, 3, 2)\n",
    "    \n",
    "    seq_len = 12\n",
    "    window_size = 6\n",
    "    \n",
    "    mask = np.zeros((seq_len, seq_len))\n",
    "    for i in range(seq_len):\n",
    "        start = max(0, i - window_size // 2)\n",
    "        end = min(seq_len, i + window_size // 2 + 1)\n",
    "        mask[i, start:end] = 1\n",
    "    \n",
    "    plt.imshow(mask, cmap='Blues', aspect='auto')\n",
    "    plt.title(f'Sliding Window Attention\\n(window={window_size})')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Attention complexity comparison\n",
    "    plt.subplot(2, 3, 3)\n",
    "    \n",
    "    methods = ['Full\\nAttention', 'Sliding\\nWindow', 'Sparse\\nAttention']\n",
    "    \n",
    "    # Complexity for seq_len = 4096\n",
    "    seq_len = 4096\n",
    "    full_complexity = seq_len ** 2\n",
    "    window_complexity = seq_len * 256  # Assuming window size 256\n",
    "    sparse_complexity = seq_len * 64   # Assuming 64 connections per token\n",
    "    \n",
    "    complexities = [full_complexity, window_complexity, sparse_complexity]\n",
    "    \n",
    "    plt.bar(methods, complexities, alpha=0.7)\n",
    "    plt.ylabel('Computational Complexity')\n",
    "    plt.title('Attention Complexity Comparison')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # RAG retrieval similarity scores\n",
    "    plt.subplot(2, 3, 4)\n",
    "    \n",
    "    doc_names = [f'Doc {i+1}' for i in range(len(documents))]\n",
    "    similarities = [score for _, score in rag.retrieve(query, top_k=len(documents))]\n",
    "    \n",
    "    plt.bar(doc_names, similarities, alpha=0.7)\n",
    "    plt.xlabel('Documents')\n",
    "    plt.ylabel('Similarity Score')\n",
    "    plt.title('RAG Retrieval Scores')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Context window utilization\n",
    "    plt.subplot(2, 3, 5)\n",
    "    \n",
    "    # Simulate context utilization over time\n",
    "    time_steps = np.arange(0, 100, 5)\n",
    "    context_usage = 50 + 30 * np.sin(time_steps * 0.1) + np.random.randn(len(time_steps)) * 5\n",
    "    context_usage = np.clip(context_usage, 0, 100)\n",
    "    \n",
    "    plt.plot(time_steps, context_usage, 'g-', linewidth=2)\n",
    "    plt.axhline(y=80, color='r', linestyle='--', label='Context Limit')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Context Usage (%)')\n",
    "    plt.title('Dynamic Context Usage')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Memory vs performance trade-off\n",
    "    plt.subplot(2, 3, 6)\n",
    "    \n",
    "    techniques = ['Full Context', 'Sliding Window', 'Hierarchical', 'RAG']\n",
    "    memory_efficiency = [20, 80, 70, 90]  # Higher = more efficient\n",
    "    performance = [100, 85, 90, 95]       # Performance retention\n",
    "    \n",
    "    plt.scatter(memory_efficiency, performance, s=100, alpha=0.7)\n",
    "    \n",
    "    for i, technique in enumerate(techniques):\n",
    "        plt.annotate(technique, (memory_efficiency[i], performance[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.xlabel('Memory Efficiency')\n",
    "    plt.ylabel('Performance Retention')\n",
    "    plt.title('Memory vs Performance Trade-off')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nContext Handling Insights:\")\n",
    "    \n",
    "    print(\"\\nContext Length Limitations:\")\n",
    "    print(\"  - Quadratic memory growth with sequence length\")\n",
    "    print(\"  - GPU memory becomes bottleneck for long sequences\")\n",
    "    print(\"  - Training becomes prohibitively expensive\")\n",
    "    \n",
    "    print(\"\\nSliding Window Attention:\")\n",
    "    print(\"  + Linear complexity instead of quadratic\")\n",
    "    print(\"  + Can process arbitrarily long sequences\")\n",
    "    print(\"  - Limited long-range dependencies\")\n",
    "    \n",
    "    print(\"\\nRAG Systems:\")\n",
    "    print(\"  + Access to external knowledge\")\n",
    "    print(\"  + Better factual accuracy\")\n",
    "    print(\"  + Updatable knowledge base\")\n",
    "    print(\"  - Additional retrieval latency\")\n",
    "    print(\"  - Requires high-quality embeddings\")\n",
    "\n",
    "demonstrate_context_handling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
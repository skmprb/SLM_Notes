{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 - Model Architectures and Variants\n",
    "\n",
    "This notebook explores different transformer architectures and their variants.\n",
    "\n",
    "## Topics Covered:\n",
    "- Decoder-only models (GPT-style)\n",
    "- Encoder-only models (BERT-style)\n",
    "- Encoder-decoder models (T5-style)\n",
    "- Dense vs Sparse models\n",
    "- Mixture of Experts (MoE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Architecture Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ModelArchitectures:\n",
    "    \"\"\"Different transformer architecture variants.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int = 512, vocab_size: int = 1000):\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "    def decoder_only_forward(self, input_ids: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        \"\"\"GPT-style decoder-only model.\"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Create causal mask\n",
    "        if mask is None:\n",
    "            mask = np.triu(np.ones((seq_len, seq_len)), k=1) == 0\n",
    "        \n",
    "        # Simplified forward pass\n",
    "        # Token embeddings\n",
    "        embeddings = np.random.randn(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Self-attention with causal mask\n",
    "        attention_output = self._masked_attention(embeddings, mask)\n",
    "        \n",
    "        # Output projection to vocabulary\n",
    "        logits = attention_output @ np.random.randn(self.d_model, self.vocab_size)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def encoder_only_forward(self, input_ids: np.ndarray, attention_mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"BERT-style encoder-only model.\"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Token embeddings\n",
    "        embeddings = np.random.randn(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Bidirectional self-attention\n",
    "        attention_output = self._bidirectional_attention(embeddings, attention_mask)\n",
    "        \n",
    "        # For classification: use [CLS] token\n",
    "        cls_output = attention_output[:, 0, :]  # First token\n",
    "        \n",
    "        return cls_output\n",
    "    \n",
    "    def encoder_decoder_forward(self, encoder_input: np.ndarray, decoder_input: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"T5-style encoder-decoder model.\"\"\"\n",
    "        # Encoder\n",
    "        encoder_embeddings = np.random.randn(*encoder_input.shape, self.d_model)\n",
    "        encoder_output = self._bidirectional_attention(encoder_embeddings, None)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_embeddings = np.random.randn(*decoder_input.shape, self.d_model)\n",
    "        \n",
    "        # Causal self-attention in decoder\n",
    "        seq_len = decoder_input.shape[1]\n",
    "        causal_mask = np.triu(np.ones((seq_len, seq_len)), k=1) == 0\n",
    "        decoder_self_attn = self._masked_attention(decoder_embeddings, causal_mask)\n",
    "        \n",
    "        # Cross-attention to encoder\n",
    "        decoder_output = self._cross_attention(decoder_self_attn, encoder_output)\n",
    "        \n",
    "        # Output projection\n",
    "        logits = decoder_output @ np.random.randn(self.d_model, self.vocab_size)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def _masked_attention(self, x: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Simplified masked attention.\"\"\"\n",
    "        # Simplified: just apply mask and return modified input\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Simulate attention computation\n",
    "        attention_weights = np.random.rand(batch_size, seq_len, seq_len)\n",
    "        \n",
    "        # Apply mask\n",
    "        attention_weights = attention_weights * mask[None, :, :]\n",
    "        \n",
    "        # Normalize\n",
    "        attention_weights = attention_weights / (np.sum(attention_weights, axis=-1, keepdims=True) + 1e-10)\n",
    "        \n",
    "        # Apply attention\n",
    "        output = attention_weights @ x\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _bidirectional_attention(self, x: np.ndarray, mask: Optional[np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"Simplified bidirectional attention.\"\"\"\n",
    "        # No causal masking - can attend to all positions\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        attention_weights = np.random.rand(batch_size, seq_len, seq_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention_weights = attention_weights * mask[:, None, :]\n",
    "        \n",
    "        attention_weights = attention_weights / (np.sum(attention_weights, axis=-1, keepdims=True) + 1e-10)\n",
    "        output = attention_weights @ x\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _cross_attention(self, queries: np.ndarray, keys_values: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Simplified cross-attention.\"\"\"\n",
    "        batch_size, q_len, d_model = queries.shape\n",
    "        kv_len = keys_values.shape[1]\n",
    "        \n",
    "        # Cross-attention weights\n",
    "        attention_weights = np.random.rand(batch_size, q_len, kv_len)\n",
    "        attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Apply to keys/values\n",
    "        output = attention_weights @ keys_values\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MixtureOfExperts:\n",
    "    \"\"\"Mixture of Experts implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_experts: int, expert_capacity: int, top_k: int = 2):\n",
    "        self.d_model = d_model\n",
    "        self.num_experts = num_experts\n",
    "        self.expert_capacity = expert_capacity\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Router network\n",
    "        self.router_weights = np.random.randn(d_model, num_experts) * 0.1\n",
    "        \n",
    "        # Expert networks (simplified as linear layers)\n",
    "        self.expert_weights = []\n",
    "        for _ in range(num_experts):\n",
    "            w1 = np.random.randn(d_model, expert_capacity) * 0.1\n",
    "            w2 = np.random.randn(expert_capacity, d_model) * 0.1\n",
    "            self.expert_weights.append((w1, w2))\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Forward pass through MoE layer.\"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Flatten for routing\n",
    "        x_flat = x.reshape(-1, d_model)  # (batch_size * seq_len, d_model)\n",
    "        \n",
    "        # Router computation\n",
    "        router_logits = x_flat @ self.router_weights  # (batch_size * seq_len, num_experts)\n",
    "        router_probs = self._softmax(router_logits)\n",
    "        \n",
    "        # Top-k routing\n",
    "        top_k_indices = np.argpartition(router_probs, -self.top_k, axis=1)[:, -self.top_k:]\n",
    "        top_k_probs = np.take_along_axis(router_probs, top_k_indices, axis=1)\n",
    "        \n",
    "        # Normalize top-k probabilities\n",
    "        top_k_probs = top_k_probs / (np.sum(top_k_probs, axis=1, keepdims=True) + 1e-10)\n",
    "        \n",
    "        # Process through experts\n",
    "        output = np.zeros_like(x_flat)\n",
    "        expert_usage = np.zeros(self.num_experts)\n",
    "        \n",
    "        for i in range(len(x_flat)):\n",
    "            token_output = np.zeros(d_model)\n",
    "            \n",
    "            for j in range(self.top_k):\n",
    "                expert_idx = top_k_indices[i, j]\n",
    "                expert_prob = top_k_probs[i, j]\n",
    "                \n",
    "                # Expert computation\n",
    "                w1, w2 = self.expert_weights[expert_idx]\n",
    "                hidden = np.maximum(0, x_flat[i] @ w1)  # ReLU activation\n",
    "                expert_output = hidden @ w2\n",
    "                \n",
    "                token_output += expert_prob * expert_output\n",
    "                expert_usage[expert_idx] += expert_prob\n",
    "            \n",
    "            output[i] = token_output\n",
    "        \n",
    "        # Reshape back\n",
    "        output = output.reshape(batch_size, seq_len, d_model)\n",
    "        \n",
    "        return output, expert_usage\n",
    "    \n",
    "    def _softmax(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Softmax activation.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def demonstrate_architectures():\n",
    "    \"\"\"Demonstrate different model architectures.\"\"\"\n",
    "    \n",
    "    # Initialize models\n",
    "    d_model, vocab_size = 256, 1000\n",
    "    models = ModelArchitectures(d_model, vocab_size)\n",
    "    \n",
    "    # Sample inputs\n",
    "    batch_size, seq_len = 2, 8\n",
    "    input_ids = np.random.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    attention_mask = np.ones((batch_size, seq_len))\n",
    "    \n",
    "    print(\"Model Architecture Comparison:\")\n",
    "    \n",
    "    # Decoder-only (GPT-style)\n",
    "    decoder_output = models.decoder_only_forward(input_ids)\n",
    "    print(f\"\\nDecoder-only output shape: {decoder_output.shape}\")\n",
    "    print(f\"  - Causal attention (can't see future tokens)\")\n",
    "    print(f\"  - Autoregressive generation\")\n",
    "    print(f\"  - Applications: Text generation, completion\")\n",
    "    \n",
    "    # Encoder-only (BERT-style)\n",
    "    encoder_output = models.encoder_only_forward(input_ids, attention_mask)\n",
    "    print(f\"\\nEncoder-only output shape: {encoder_output.shape}\")\n",
    "    print(f\"  - Bidirectional attention\")\n",
    "    print(f\"  - Masked language modeling\")\n",
    "    print(f\"  - Applications: Classification, understanding\")\n",
    "    \n",
    "    # Encoder-decoder (T5-style)\n",
    "    encoder_input = input_ids\n",
    "    decoder_input = np.random.randint(0, vocab_size, (batch_size, 6))\n",
    "    enc_dec_output = models.encoder_decoder_forward(encoder_input, decoder_input)\n",
    "    print(f\"\\nEncoder-decoder output shape: {enc_dec_output.shape}\")\n",
    "    print(f\"  - Encoder: bidirectional, Decoder: causal\")\n",
    "    print(f\"  - Cross-attention between encoder and decoder\")\n",
    "    print(f\"  - Applications: Translation, summarization\")\n",
    "    \n",
    "    # Mixture of Experts\n",
    "    moe = MixtureOfExperts(d_model=d_model, num_experts=8, expert_capacity=512, top_k=2)\n",
    "    x = np.random.randn(batch_size, seq_len, d_model)\n",
    "    moe_output, expert_usage = moe.forward(x)\n",
    "    \n",
    "    print(f\"\\nMixture of Experts:\")\n",
    "    print(f\"  - Output shape: {moe_output.shape}\")\n",
    "    print(f\"  - Expert usage: {expert_usage}\")\n",
    "    print(f\"  - Sparse computation (only top-k experts active)\")\n",
    "    \n",
    "    # Visualize architectures\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Architecture comparison\n",
    "    plt.subplot(2, 3, 1)\n",
    "    \n",
    "    architectures = ['Decoder-only\\n(GPT)', 'Encoder-only\\n(BERT)', 'Encoder-Decoder\\n(T5)']\n",
    "    \n",
    "    # Simulate complexity metrics\n",
    "    params = [100, 110, 150]  # Relative parameter counts\n",
    "    memory = [80, 70, 120]    # Relative memory usage\n",
    "    \n",
    "    x = np.arange(len(architectures))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, params, width, label='Parameters', alpha=0.7)\n",
    "    plt.bar(x + width/2, memory, width, label='Memory', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Architecture')\n",
    "    plt.ylabel('Relative Cost')\n",
    "    plt.title('Architecture Comparison')\n",
    "    plt.xticks(x, architectures)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Attention patterns visualization\n",
    "    plt.subplot(2, 3, 2)\n",
    "    \n",
    "    # Causal mask (decoder-only)\n",
    "    causal_mask = np.triu(np.ones((8, 8)), k=1) == 0\n",
    "    plt.imshow(causal_mask, cmap='RdYlBu', aspect='auto')\n",
    "    plt.title('Causal Attention\\n(Decoder-only)')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    \n",
    "    plt.subplot(2, 3, 3)\n",
    "    \n",
    "    # Bidirectional mask (encoder-only)\n",
    "    bidirectional_mask = np.ones((8, 8))\n",
    "    plt.imshow(bidirectional_mask, cmap='RdYlBu', aspect='auto')\n",
    "    plt.title('Bidirectional Attention\\n(Encoder-only)')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    \n",
    "    # MoE expert usage\n",
    "    plt.subplot(2, 3, 4)\n",
    "    \n",
    "    expert_ids = range(len(expert_usage))\n",
    "    plt.bar(expert_ids, expert_usage, alpha=0.7)\n",
    "    plt.xlabel('Expert ID')\n",
    "    plt.ylabel('Usage')\n",
    "    plt.title('MoE Expert Usage')\n",
    "    \n",
    "    # Model capacity vs efficiency\n",
    "    plt.subplot(2, 3, 5)\n",
    "    \n",
    "    model_types = ['Dense\\nSmall', 'Dense\\nLarge', 'Sparse\\nMoE']\n",
    "    capacity = [50, 100, 120]     # Model capacity\n",
    "    efficiency = [90, 60, 85]    # Computational efficiency\n",
    "    \n",
    "    plt.scatter(capacity, efficiency, s=[50, 200, 150], alpha=0.7)\n",
    "    \n",
    "    for i, model_type in enumerate(model_types):\n",
    "        plt.annotate(model_type, (capacity[i], efficiency[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.xlabel('Model Capacity')\n",
    "    plt.ylabel('Computational Efficiency')\n",
    "    plt.title('Capacity vs Efficiency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Task suitability\n",
    "    plt.subplot(2, 3, 6)\n",
    "    \n",
    "    tasks = ['Generation', 'Classification', 'Translation']\n",
    "    decoder_scores = [0.9, 0.3, 0.4]\n",
    "    encoder_scores = [0.2, 0.9, 0.3]\n",
    "    enc_dec_scores = [0.6, 0.5, 0.9]\n",
    "    \n",
    "    x = np.arange(len(tasks))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x - width, decoder_scores, width, label='Decoder-only', alpha=0.7)\n",
    "    plt.bar(x, encoder_scores, width, label='Encoder-only', alpha=0.7)\n",
    "    plt.bar(x + width, enc_dec_scores, width, label='Encoder-Decoder', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Task Type')\n",
    "    plt.ylabel('Suitability Score')\n",
    "    plt.title('Task Suitability')\n",
    "    plt.xticks(x, tasks)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nArchitecture Insights:\")\n",
    "    \n",
    "    print(\"\\nDecoder-only (GPT-style):\")\n",
    "    print(\"  + Excellent for generation tasks\")\n",
    "    print(\"  + Simple architecture\")\n",
    "    print(\"  - Limited for understanding tasks\")\n",
    "    \n",
    "    print(\"\\nEncoder-only (BERT-style):\")\n",
    "    print(\"  + Great for classification/understanding\")\n",
    "    print(\"  + Bidirectional context\")\n",
    "    print(\"  - Cannot generate text naturally\")\n",
    "    \n",
    "    print(\"\\nEncoder-Decoder (T5-style):\")\n",
    "    print(\"  + Versatile for seq2seq tasks\")\n",
    "    print(\"  + Good for translation/summarization\")\n",
    "    print(\"  - More complex architecture\")\n",
    "    \n",
    "    print(\"\\nMixture of Experts:\")\n",
    "    print(\"  + Scales parameters without proportional compute\")\n",
    "    print(\"  + Specialization through expert routing\")\n",
    "    print(\"  - Complex training and load balancing\")\n",
    "\n",
    "demonstrate_architectures()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
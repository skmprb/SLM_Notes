{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Neural Network Fundamentals\n",
    "\n",
    "This notebook covers the neural network concepts essential for understanding Large Language Models.\n",
    "\n",
    "## Topics Covered:\n",
    "- Perceptrons and basic neurons\n",
    "- Feedforward neural networks\n",
    "- Activation functions\n",
    "- Loss functions\n",
    "- Backpropagation\n",
    "- Gradient descent and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, List, Tuple\n",
    "import math\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perceptrons\n",
    "\n",
    "The perceptron is the simplest neural network unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class Perceptron:\n",
    "    \"\"\"Simple perceptron implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, learning_rate: float = 0.01):\n",
    "        self.weights = np.random.randn(input_size) * 0.1\n",
    "        self.bias = 0.0\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> float:\n",
    "        \"\"\"Forward pass through perceptron.\"\"\"\n",
    "        return np.dot(x, self.weights) + self.bias\n",
    "    \n",
    "    def predict(self, x: np.ndarray) -> int:\n",
    "        \"\"\"Make binary prediction.\"\"\"\n",
    "        return 1 if self.forward(x) > 0 else 0\n",
    "    \n",
    "    def train_step(self, x: np.ndarray, y: int) -> float:\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        prediction = self.predict(x)\n",
    "        error = y - prediction\n",
    "        \n",
    "        # Update weights and bias\n",
    "        self.weights += self.learning_rate * error * x\n",
    "        self.bias += self.learning_rate * error\n",
    "        \n",
    "        return abs(error)\n",
    "\n",
    "# Demonstrate perceptron on AND gate\n",
    "# Training data for AND gate\n",
    "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_and = np.array([0, 0, 0, 1])\n",
    "\n",
    "perceptron = Perceptron(input_size=2, learning_rate=0.1)\n",
    "\n",
    "# Training\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    total_error = 0\n",
    "    for x, y in zip(X_and, y_and):\n",
    "        error = perceptron.train_step(x, y)\n",
    "        total_error += error\n",
    "    \n",
    "    if total_error == 0:\n",
    "        print(f\"Converged after {epoch + 1} epochs\")\n",
    "        break\n",
    "\n",
    "# Test the trained perceptron\n",
    "print(\"\\nAND Gate Results:\")\n",
    "for x, y_true in zip(X_and, y_and):\n",
    "    y_pred = perceptron.predict(x)\n",
    "    print(f\"Input: {x}, True: {y_true}, Predicted: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity to neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ActivationFunctions:\n",
    "    \"\"\"Collection of activation functions and their derivatives.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        # Clip x to prevent overflow\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Derivative of sigmoid function.\"\"\"\n",
    "        s = ActivationFunctions.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Hyperbolic tangent activation function.\"\"\"\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Derivative of tanh function.\"\"\"\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"ReLU activation function.\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Derivative of ReLU function.\"\"\"\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def gelu(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"GELU activation function (used in transformers).\"\"\"\n",
    "        return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Softmax activation function.\"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Visualize activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "activations = ActivationFunctions()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot activation functions\n",
    "functions = [\n",
    "    ('Sigmoid', activations.sigmoid),\n",
    "    ('Tanh', activations.tanh),\n",
    "    ('ReLU', activations.relu),\n",
    "    ('GELU', activations.gelu)\n",
    "]\n",
    "\n",
    "for i, (name, func) in enumerate(functions, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.plot(x, func(x), label=name, linewidth=2)\n",
    "    plt.title(f'{name} Activation Function')\n",
    "    plt.xlabel('Input')\n",
    "    plt.ylabel('Output')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare activation function properties\n",
    "print(\"Activation Function Properties:\")\n",
    "test_input = np.array([-2, -1, 0, 1, 2])\n",
    "for name, func in functions:\n",
    "    output = func(test_input)\n",
    "    print(f\"{name:8}: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class FeedforwardNetwork:\n",
    "    \"\"\"Simple feedforward neural network.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes: List[int], activation: str = 'sigmoid'):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(self.num_layers - 1):\n",
    "            # Xavier initialization\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2.0 / layer_sizes[i])\n",
    "            b = np.zeros((1, layer_sizes[i + 1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "        \n",
    "        # Set activation function\n",
    "        self.activations = ActivationFunctions()\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = self.activations.sigmoid\n",
    "            self.activation_derivative = self.activations.sigmoid_derivative\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = self.activations.tanh\n",
    "            self.activation_derivative = self.activations.tanh_derivative\n",
    "        elif activation == 'relu':\n",
    "            self.activation = self.activations.relu\n",
    "            self.activation_derivative = self.activations.relu_derivative\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> Tuple[np.ndarray, List[np.ndarray]]:\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        activations = [X]\n",
    "        \n",
    "        for i in range(self.num_layers - 1):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            \n",
    "            # Use softmax for output layer, activation function for hidden layers\n",
    "            if i == self.num_layers - 2:  # Output layer\n",
    "                a = self.activations.softmax(z)\n",
    "            else:  # Hidden layers\n",
    "                a = self.activation(z)\n",
    "            \n",
    "            activations.append(a)\n",
    "        \n",
    "        return activations[-1], activations\n",
    "    \n",
    "    def backward(self, X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01):\n",
    "        \"\"\"Backward pass (backpropagation).\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        output, activations = self.forward(X)\n",
    "        \n",
    "        # Compute loss (cross-entropy)\n",
    "        loss = -np.mean(np.sum(y * np.log(output + 1e-15), axis=1))\n",
    "        \n",
    "        # Backward pass\n",
    "        deltas = [output - y]  # Error at output layer\n",
    "        \n",
    "        # Compute deltas for hidden layers\n",
    "        for i in range(self.num_layers - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.activation_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.weights[i] -= learning_rate * np.dot(activations[i].T, deltas[i]) / m\n",
    "            self.biases[i] -= learning_rate * np.mean(deltas[i], axis=0, keepdims=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        output, _ = self.forward(X)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "# Create a simple classification dataset\n",
    "def create_spiral_dataset(n_points: int = 100, n_classes: int = 3) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Create a spiral dataset for classification.\"\"\"\n",
    "    X = np.zeros((n_points * n_classes, 2))\n",
    "    y = np.zeros(n_points * n_classes, dtype=int)\n",
    "    \n",
    "    for class_num in range(n_classes):\n",
    "        ix = range(n_points * class_num, n_points * (class_num + 1))\n",
    "        r = np.linspace(0.0, 1, n_points)\n",
    "        t = np.linspace(class_num * 4, (class_num + 1) * 4, n_points) + np.random.randn(n_points) * 0.2\n",
    "        X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n",
    "        y[ix] = class_num\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate dataset\n",
    "X, y = create_spiral_dataset(n_points=50, n_classes=3)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_onehot = np.eye(3)[y]\n",
    "\n",
    "# Create and train network\n",
    "network = FeedforwardNetwork([2, 10, 10, 3], activation='relu')\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = network.backward(X, y_onehot, learning_rate=0.1)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        predictions = network.predict(X)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.subplot(1, 2, 2)\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = network.predict(mesh_points)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "plt.title('Decision Boundary')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "final_accuracy = np.mean(network.predict(X) == y)\n",
    "print(f\"\\nFinal Accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class LossFunctions:\n",
    "    \"\"\"Collection of loss functions.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"Mean Squared Error loss.\"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"Cross-entropy loss for classification.\"\"\"\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"Binary cross-entropy loss.\"\"\"\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    @staticmethod\n",
    "    def huber_loss(y_true: np.ndarray, y_pred: np.ndarray, delta: float = 1.0) -> float:\n",
    "        \"\"\"Huber loss (robust to outliers).\"\"\"\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = np.abs(error) <= delta\n",
    "        squared_loss = 0.5 * error ** 2\n",
    "        linear_loss = delta * np.abs(error) - 0.5 * delta ** 2\n",
    "        return np.mean(np.where(is_small_error, squared_loss, linear_loss))\n",
    "\n",
    "# Demonstrate different loss functions\n",
    "y_true = np.array([0, 1, 1, 0, 1])\n",
    "y_pred_good = np.array([0.1, 0.9, 0.8, 0.2, 0.9])\n",
    "y_pred_bad = np.array([0.9, 0.1, 0.2, 0.8, 0.1])\n",
    "\n",
    "loss_funcs = LossFunctions()\n",
    "\n",
    "print(\"Loss Function Comparison:\")\n",
    "print(f\"Good predictions: {y_pred_good}\")\n",
    "print(f\"Bad predictions:  {y_pred_bad}\")\n",
    "print(f\"True labels:      {y_true}\")\n",
    "print()\n",
    "\n",
    "# Binary cross-entropy\n",
    "bce_good = loss_funcs.binary_cross_entropy(y_true, y_pred_good)\n",
    "bce_bad = loss_funcs.binary_cross_entropy(y_true, y_pred_bad)\n",
    "print(f\"Binary Cross-Entropy - Good: {bce_good:.4f}, Bad: {bce_bad:.4f}\")\n",
    "\n",
    "# MSE\n",
    "mse_good = loss_funcs.mean_squared_error(y_true, y_pred_good)\n",
    "mse_bad = loss_funcs.mean_squared_error(y_true, y_pred_bad)\n",
    "print(f\"Mean Squared Error   - Good: {mse_good:.4f}, Bad: {mse_bad:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class Optimizers:\n",
    "    \"\"\"Collection of optimization algorithms.\"\"\"\n",
    "    \n",
    "    class SGD:\n",
    "        \"\"\"Stochastic Gradient Descent optimizer.\"\"\"\n",
    "        \n",
    "        def __init__(self, learning_rate: float = 0.01, momentum: float = 0.0):\n",
    "            self.learning_rate = learning_rate\n",
    "            self.momentum = momentum\n",
    "            self.velocity = None\n",
    "        \n",
    "        def update(self, params: np.ndarray, gradients: np.ndarray) -> np.ndarray:\n",
    "            if self.velocity is None:\n",
    "                self.velocity = np.zeros_like(params)\n",
    "            \n",
    "            self.velocity = self.momentum * self.velocity - self.learning_rate * gradients\n",
    "            return params + self.velocity\n",
    "    \n",
    "    class Adam:\n",
    "        \"\"\"Adam optimizer.\"\"\"\n",
    "        \n",
    "        def __init__(self, learning_rate: float = 0.001, beta1: float = 0.9, \n",
    "                     beta2: float = 0.999, epsilon: float = 1e-8):\n",
    "            self.learning_rate = learning_rate\n",
    "            self.beta1 = beta1\n",
    "            self.beta2 = beta2\n",
    "            self.epsilon = epsilon\n",
    "            self.m = None  # First moment\n",
    "            self.v = None  # Second moment\n",
    "            self.t = 0     # Time step\n",
    "        \n",
    "        def update(self, params: np.ndarray, gradients: np.ndarray) -> np.ndarray:\n",
    "            if self.m is None:\n",
    "                self.m = np.zeros_like(params)\n",
    "                self.v = np.zeros_like(params)\n",
    "            \n",
    "            self.t += 1\n",
    "            \n",
    "            # Update biased first moment estimate\n",
    "            self.m = self.beta1 * self.m + (1 - self.beta1) * gradients\n",
    "            \n",
    "            # Update biased second raw moment estimate\n",
    "            self.v = self.beta2 * self.v + (1 - self.beta2) * (gradients ** 2)\n",
    "            \n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_hat = self.m / (1 - self.beta1 ** self.t)\n",
    "            \n",
    "            # Compute bias-corrected second raw moment estimate\n",
    "            v_hat = self.v / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            # Update parameters\n",
    "            return params - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "# Demonstrate optimization on a simple quadratic function\n",
    "def quadratic_function(x: np.ndarray) -> float:\n",
    "    \"\"\"Simple quadratic function: f(x) = x^2 + 2x + 1\"\"\"\n",
    "    return np.sum(x**2 + 2*x + 1)\n",
    "\n",
    "def quadratic_gradient(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Gradient of quadratic function: f'(x) = 2x + 2\"\"\"\n",
    "    return 2*x + 2\n",
    "\n",
    "# Compare optimizers\n",
    "def compare_optimizers(initial_x: np.ndarray, steps: int = 100):\n",
    "    \"\"\"Compare different optimizers on the quadratic function.\"\"\"\n",
    "    \n",
    "    # Initialize optimizers\n",
    "    sgd = Optimizers.SGD(learning_rate=0.1)\n",
    "    sgd_momentum = Optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "    adam = Optimizers.Adam(learning_rate=0.1)\n",
    "    \n",
    "    # Track optimization paths\n",
    "    optimizers = {\n",
    "        'SGD': (sgd, initial_x.copy()),\n",
    "        'SGD + Momentum': (sgd_momentum, initial_x.copy()),\n",
    "        'Adam': (adam, initial_x.copy())\n",
    "    }\n",
    "    \n",
    "    histories = {name: [] for name in optimizers.keys()}\n",
    "    \n",
    "    for step in range(steps):\n",
    "        for name, (optimizer, x) in optimizers.items():\n",
    "            # Compute gradient\n",
    "            grad = quadratic_gradient(x)\n",
    "            \n",
    "            # Update parameters\n",
    "            x_new = optimizer.update(x, grad)\n",
    "            optimizers[name] = (optimizer, x_new)\n",
    "            \n",
    "            # Record function value\n",
    "            histories[name].append(quadratic_function(x_new))\n",
    "    \n",
    "    return histories\n",
    "\n",
    "# Run comparison\n",
    "initial_point = np.array([5.0])\n",
    "histories = compare_optimizers(initial_point, steps=50)\n",
    "\n",
    "# Plot optimization paths\n",
    "plt.figure(figsize=(10, 6))\n",
    "for name, history in histories.items():\n",
    "    plt.plot(history, label=name, linewidth=2)\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Function Value')\n",
    "plt.title('Optimizer Comparison on Quadratic Function')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "# Print final values\n",
    "print(\"Final function values:\")\n",
    "for name, history in histories.items():\n",
    "    print(f\"{name:15}: {history[-1]:.6f}\")\n",
    "print(f\"{'Optimal':15}: {0.000000:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradient Descent Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def batch_gradient_descent(X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, \n",
    "                          epochs: int = 100) -> Tuple[np.ndarray, List[float]]:\n",
    "    \"\"\"Batch gradient descent for linear regression.\"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.random.randn(n, 1) * 0.01\n",
    "    costs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        predictions = X.dot(theta)\n",
    "        cost = np.mean((predictions - y) ** 2) / 2\n",
    "        costs.append(cost)\n",
    "        \n",
    "        # Backward pass\n",
    "        gradients = X.T.dot(predictions - y) / m\n",
    "        theta -= learning_rate * gradients\n",
    "    \n",
    "    return theta, costs\n",
    "\n",
    "def stochastic_gradient_descent(X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, \n",
    "                               epochs: int = 100) -> Tuple[np.ndarray, List[float]]:\n",
    "    \"\"\"Stochastic gradient descent for linear regression.\"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.random.randn(n, 1) * 0.01\n",
    "    costs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        \n",
    "        # Shuffle data\n",
    "        indices = np.random.permutation(m)\n",
    "        \n",
    "        for i in indices:\n",
    "            xi = X[i:i+1]\n",
    "            yi = y[i:i+1]\n",
    "            \n",
    "            # Forward pass\n",
    "            prediction = xi.dot(theta)\n",
    "            cost = (prediction - yi) ** 2 / 2\n",
    "            epoch_cost += cost[0, 0]\n",
    "            \n",
    "            # Backward pass\n",
    "            gradient = xi.T.dot(prediction - yi)\n",
    "            theta -= learning_rate * gradient\n",
    "        \n",
    "        costs.append(epoch_cost / m)\n",
    "    \n",
    "    return theta, costs\n",
    "\n",
    "def mini_batch_gradient_descent(X: np.ndarray, y: np.ndarray, batch_size: int = 32,\n",
    "                               learning_rate: float = 0.01, epochs: int = 100) -> Tuple[np.ndarray, List[float]]:\n",
    "    \"\"\"Mini-batch gradient descent for linear regression.\"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.random.randn(n, 1) * 0.01\n",
    "    costs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Shuffle data\n",
    "        indices = np.random.permutation(m)\n",
    "        \n",
    "        for i in range(0, m, batch_size):\n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = X_batch.dot(theta)\n",
    "            cost = np.mean((predictions - y_batch) ** 2) / 2\n",
    "            epoch_cost += cost\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Backward pass\n",
    "            gradients = X_batch.T.dot(predictions - y_batch) / len(X_batch)\n",
    "            theta -= learning_rate * gradients\n",
    "        \n",
    "        costs.append(epoch_cost / num_batches)\n",
    "    \n",
    "    return theta, costs\n",
    "\n",
    "# Generate synthetic linear regression data\n",
    "np.random.seed(42)\n",
    "m = 1000  # Number of samples\n",
    "X = np.random.randn(m, 1)\n",
    "y = 4 + 3 * X + np.random.randn(m, 1) * 0.5  # y = 4 + 3x + noise\n",
    "\n",
    "# Add bias term\n",
    "X_with_bias = np.c_[np.ones((m, 1)), X]\n",
    "\n",
    "# Compare gradient descent variants\n",
    "methods = {\n",
    "    'Batch GD': lambda: batch_gradient_descent(X_with_bias, y, learning_rate=0.1, epochs=100),\n",
    "    'Stochastic GD': lambda: stochastic_gradient_descent(X_with_bias, y, learning_rate=0.01, epochs=100),\n",
    "    'Mini-batch GD': lambda: mini_batch_gradient_descent(X_with_bias, y, batch_size=32, learning_rate=0.1, epochs=100)\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i, (name, method) in enumerate(methods.items(), 1):\n",
    "    theta, costs = method()\n",
    "    \n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.plot(costs, linewidth=2)\n",
    "    plt.title(f'{name}\\nFinal θ = [{theta[0,0]:.2f}, {theta[1,0]:.2f}]')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"True parameters: θ₀ = 4.00, θ₁ = 3.00\")\n",
    "print(\"\\nLearned parameters:\")\n",
    "for name, method in methods.items():\n",
    "    theta, _ = method()\n",
    "    print(f\"{name:15}: θ₀ = {theta[0,0]:.2f}, θ₁ = {theta[1,0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "1. **Custom Activation Function**: Implement the Swish activation function (x * sigmoid(x)) and compare its performance with ReLU on the spiral dataset.\n",
    "\n",
    "2. **Learning Rate Scheduling**: Implement different learning rate schedules (exponential decay, cosine annealing) and observe their effects on training.\n",
    "\n",
    "3. **Regularization**: Add L1 and L2 regularization to the feedforward network and study their effects on overfitting.\n",
    "\n",
    "4. **Batch Normalization**: Implement a simplified version of batch normalization and observe its effects on training stability.\n",
    "\n",
    "5. **Universal Approximation**: Experiment with different network architectures to approximate complex functions like sin(x) or x²."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered the fundamental neural network concepts:\n",
    "\n",
    "- **Perceptrons**: The basic building blocks of neural networks\n",
    "- **Activation Functions**: Non-linear functions that enable learning complex patterns\n",
    "- **Feedforward Networks**: Multi-layer networks for classification and regression\n",
    "- **Loss Functions**: Measures of prediction error for different tasks\n",
    "- **Backpropagation**: The algorithm for computing gradients\n",
    "- **Optimization**: Different algorithms for updating network parameters\n",
    "- **Gradient Descent Variants**: Batch, stochastic, and mini-batch approaches\n",
    "\n",
    "These concepts form the foundation for understanding more complex architectures like RNNs and Transformers, which we'll explore in subsequent notebooks. The principles of forward propagation, backpropagation, and optimization remain consistent across all neural network architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
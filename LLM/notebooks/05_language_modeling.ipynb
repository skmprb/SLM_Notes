{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Language Modeling\n",
    "\n",
    "This notebook covers different approaches to language modeling that form the foundation of modern LLMs.\n",
    "\n",
    "## Topics Covered:\n",
    "- Statistical language models\n",
    "- Neural language models\n",
    "- Autoregressive modeling\n",
    "- Masked language modeling\n",
    "- Causal language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import re\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Statistical Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class NGramLanguageModel:\n",
    "    \"\"\"N-gram statistical language model.\"\"\"\n",
    "    \n",
    "    def __init__(self, n: int = 3, smoothing: str = 'laplace', alpha: float = 1.0):\n",
    "        self.n = n\n",
    "        self.smoothing = smoothing\n",
    "        self.alpha = alpha\n",
    "        self.ngram_counts = defaultdict(int)\n",
    "        self.context_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple tokenization.\"\"\"\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[.,!?;]', text.lower())\n",
    "        return ['<BOS>'] * (self.n - 1) + tokens + ['<EOS>']\n",
    "    \n",
    "    def get_ngrams(self, tokens: List[str]) -> List[Tuple[str, ...]]:\n",
    "        \"\"\"Extract n-grams from tokens.\"\"\"\n",
    "        return [tuple(tokens[i:i+self.n]) for i in range(len(tokens) - self.n + 1)]\n",
    "    \n",
    "    def train(self, texts: List[str]):\n",
    "        \"\"\"Train the n-gram model.\"\"\"\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            self.vocab.update(tokens)\n",
    "            \n",
    "            ngrams = self.get_ngrams(tokens)\n",
    "            for ngram in ngrams:\n",
    "                context = ngram[:-1]\n",
    "                word = ngram[-1]\n",
    "                \n",
    "                self.ngram_counts[ngram] += 1\n",
    "                self.context_counts[context] += 1\n",
    "    \n",
    "    def probability(self, word: str, context: Tuple[str, ...]) -> float:\n",
    "        \"\"\"Calculate probability of word given context.\"\"\"\n",
    "        ngram = context + (word,)\n",
    "        \n",
    "        if self.smoothing == 'laplace':\n",
    "            # Laplace smoothing\n",
    "            numerator = self.ngram_counts[ngram] + self.alpha\n",
    "            denominator = self.context_counts[context] + self.alpha * len(self.vocab)\n",
    "            return numerator / denominator if denominator > 0 else 1.0 / len(self.vocab)\n",
    "        \n",
    "        elif self.smoothing == 'mle':\n",
    "            # Maximum likelihood estimation\n",
    "            return (self.ngram_counts[ngram] / self.context_counts[context] \n",
    "                   if self.context_counts[context] > 0 else 0.0)\n",
    "    \n",
    "    def perplexity(self, text: str) -> float:\n",
    "        \"\"\"Calculate perplexity on test text.\"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        ngrams = self.get_ngrams(tokens)\n",
    "        \n",
    "        log_prob_sum = 0\n",
    "        for ngram in ngrams:\n",
    "            context = ngram[:-1]\n",
    "            word = ngram[-1]\n",
    "            prob = self.probability(word, context)\n",
    "            log_prob_sum += np.log(prob + 1e-10)\n",
    "        \n",
    "        return np.exp(-log_prob_sum / len(ngrams))\n",
    "    \n",
    "    def generate(self, max_length: int = 20, seed_context: Optional[Tuple[str, ...]] = None) -> str:\n",
    "        \"\"\"Generate text using the model.\"\"\"\n",
    "        if seed_context is None:\n",
    "            context = tuple(['<BOS>'] * (self.n - 1))\n",
    "        else:\n",
    "            context = seed_context\n",
    "        \n",
    "        generated = list(context)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Get all possible next words\n",
    "            candidates = []\n",
    "            probs = []\n",
    "            \n",
    "            for word in self.vocab:\n",
    "                if word not in ['<BOS>', '<EOS>']:\n",
    "                    prob = self.probability(word, context)\n",
    "                    candidates.append(word)\n",
    "                    probs.append(prob)\n",
    "            \n",
    "            if not candidates:\n",
    "                break\n",
    "            \n",
    "            # Sample next word\n",
    "            probs = np.array(probs)\n",
    "            probs = probs / np.sum(probs)\n",
    "            next_word = np.random.choice(candidates, p=probs)\n",
    "            \n",
    "            generated.append(next_word)\n",
    "            \n",
    "            if next_word == '<EOS>':\n",
    "                break\n",
    "            \n",
    "            # Update context\n",
    "            context = context[1:] + (next_word,)\n",
    "        \n",
    "        # Remove special tokens and join\n",
    "        words = [w for w in generated if w not in ['<BOS>', '<EOS>']]\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Demonstrate n-gram models\n",
    "def demonstrate_ngram_models():\n",
    "    \"\"\"Compare different n-gram models.\"\"\"\n",
    "    \n",
    "    # Sample training data\n",
    "    training_texts = [\n",
    "        \"The cat sat on the mat.\",\n",
    "        \"The dog ran in the park.\",\n",
    "        \"A cat and a dog played together.\",\n",
    "        \"The mat was comfortable for the cat.\",\n",
    "        \"In the park, children played with their dog.\",\n",
    "        \"The comfortable mat attracted the sleeping cat.\"\n",
    "    ]\n",
    "    \n",
    "    test_text = \"The cat played in the park.\"\n",
    "    \n",
    "    # Train different n-gram models\n",
    "    models = {}\n",
    "    for n in [1, 2, 3, 4]:\n",
    "        model = NGramLanguageModel(n=n, smoothing='laplace')\n",
    "        model.train(training_texts)\n",
    "        models[n] = model\n",
    "    \n",
    "    # Compare perplexities\n",
    "    perplexities = []\n",
    "    for n in [1, 2, 3, 4]:\n",
    "        perp = models[n].perplexity(test_text)\n",
    "        perplexities.append(perp)\n",
    "        print(f\"{n}-gram perplexity: {perp:.2f}\")\n",
    "    \n",
    "    # Plot perplexities\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot([1, 2, 3, 4], perplexities, marker='o')\n",
    "    plt.title('Perplexity vs N-gram Order')\n",
    "    plt.xlabel('N-gram Order')\n",
    "    plt.ylabel('Perplexity')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Show vocabulary sizes\n",
    "    vocab_sizes = [len(models[n].vocab) for n in [1, 2, 3, 4]]\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.bar([1, 2, 3, 4], vocab_sizes)\n",
    "    plt.title('Vocabulary Size')\n",
    "    plt.xlabel('N-gram Order')\n",
    "    plt.ylabel('Vocabulary Size')\n",
    "    \n",
    "    # Show n-gram counts\n",
    "    ngram_counts = [len(models[n].ngram_counts) for n in [1, 2, 3, 4]]\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.bar([1, 2, 3, 4], ngram_counts)\n",
    "    plt.title('Number of N-grams')\n",
    "    plt.xlabel('N-gram Order')\n",
    "    plt.ylabel('N-gram Count')\n",
    "    \n",
    "    # Generate samples\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.text(0.1, 0.8, \"Generated Samples:\", fontsize=12, weight='bold')\n",
    "    \n",
    "    y_pos = 0.7\n",
    "    for n in [2, 3]:\n",
    "        sample = models[n].generate(max_length=10)\n",
    "        plt.text(0.1, y_pos, f\"{n}-gram: {sample}\", fontsize=10, wrap=True)\n",
    "        y_pos -= 0.15\n",
    "    \n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nN-gram Model Analysis:\")\n",
    "    print(\"- Higher order n-grams capture more context\")\n",
    "    print(\"- But suffer from data sparsity\")\n",
    "    print(\"- Smoothing helps with unseen n-grams\")\n",
    "    print(\"- Trade-off between context and generalization\")\n",
    "\n",
    "demonstrate_ngram_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SimpleNeuralLM:\n",
    "    \"\"\"Simple feedforward neural language model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, context_size: int):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.context_size = context_size\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.embeddings = np.random.randn(vocab_size, embedding_dim) * 0.1\n",
    "        self.W1 = np.random.randn(context_size * embedding_dim, hidden_dim) * 0.1\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.W2 = np.random.randn(hidden_dim, vocab_size) * 0.1\n",
    "        self.b2 = np.zeros(vocab_size)\n",
    "        \n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "    \n",
    "    def build_vocab(self, texts: List[str]):\n",
    "        \"\"\"Build vocabulary from texts.\"\"\"\n",
    "        words = set()\n",
    "        for text in texts:\n",
    "            tokens = re.findall(r'\\b\\w+\\b|[.,!?;]', text.lower())\n",
    "            words.update(tokens)\n",
    "        \n",
    "        words = ['<UNK>', '<BOS>', '<EOS>'] + sorted(list(words))\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(words)}\n",
    "        self.idx_to_word = {i: word for word, i in self.word_to_idx.items()}\n",
    "        \n",
    "        # Update vocab size\n",
    "        self.vocab_size = len(words)\n",
    "        \n",
    "        # Reinitialize parameters with correct vocab size\n",
    "        self.embeddings = np.random.randn(self.vocab_size, self.embedding_dim) * 0.1\n",
    "        self.W2 = np.random.randn(self.hidden_dim, self.vocab_size) * 0.1\n",
    "        self.b2 = np.zeros(self.vocab_size)\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[int]:\n",
    "        \"\"\"Convert text to token indices.\"\"\"\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[.,!?;]', text.lower())\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            indices.append(self.word_to_idx.get(token, self.word_to_idx['<UNK>']))\n",
    "        return indices\n",
    "    \n",
    "    def softmax(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Softmax activation.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "    \n",
    "    def forward(self, context_indices: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # Embedding lookup\n",
    "        embedded = self.embeddings[context_indices]  # (batch, context_size, embedding_dim)\n",
    "        \n",
    "        # Flatten context embeddings\n",
    "        flattened = embedded.reshape(embedded.shape[0], -1)  # (batch, context_size * embedding_dim)\n",
    "        \n",
    "        # Hidden layer\n",
    "        hidden = np.tanh(flattened @ self.W1 + self.b1)  # (batch, hidden_dim)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = hidden @ self.W2 + self.b2  # (batch, vocab_size)\n",
    "        \n",
    "        # Softmax\n",
    "        probs = self.softmax(logits)\n",
    "        \n",
    "        return probs, hidden, embedded\n",
    "    \n",
    "    def create_training_data(self, texts: List[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Create training data from texts.\"\"\"\n",
    "        contexts = []\n",
    "        targets = []\n",
    "        \n",
    "        for text in texts:\n",
    "            indices = self.tokenize(text)\n",
    "            \n",
    "            # Add BOS tokens\n",
    "            padded = [self.word_to_idx['<BOS>']] * self.context_size + indices + [self.word_to_idx['<EOS>']]\n",
    "            \n",
    "            for i in range(len(padded) - self.context_size):\n",
    "                context = padded[i:i + self.context_size]\n",
    "                target = padded[i + self.context_size]\n",
    "                contexts.append(context)\n",
    "                targets.append(target)\n",
    "        \n",
    "        return np.array(contexts), np.array(targets)\n",
    "    \n",
    "    def train_step(self, contexts: np.ndarray, targets: np.ndarray, learning_rate: float = 0.01) -> float:\n",
    "        \"\"\"Single training step (simplified).\"\"\"\n",
    "        batch_size = contexts.shape[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        probs, hidden, embedded = self.forward(contexts)\n",
    "        \n",
    "        # Compute loss (cross-entropy)\n",
    "        target_probs = probs[np.arange(batch_size), targets]\n",
    "        loss = -np.mean(np.log(target_probs + 1e-10))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def generate(self, seed_text: str, max_length: int = 20) -> str:\n",
    "        \"\"\"Generate text using the model.\"\"\"\n",
    "        # Initialize context\n",
    "        if seed_text:\n",
    "            context = self.tokenize(seed_text)[-self.context_size:]\n",
    "        else:\n",
    "            context = [self.word_to_idx['<BOS>']] * self.context_size\n",
    "        \n",
    "        # Pad if necessary\n",
    "        while len(context) < self.context_size:\n",
    "            context = [self.word_to_idx['<BOS>']] + context\n",
    "        \n",
    "        generated = []\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Get probabilities for next word\n",
    "            context_array = np.array([context])\n",
    "            probs, _, _ = self.forward(context_array)\n",
    "            \n",
    "            # Sample next word\n",
    "            next_idx = np.random.choice(self.vocab_size, p=probs[0])\n",
    "            next_word = self.idx_to_word[next_idx]\n",
    "            \n",
    "            if next_word == '<EOS>':\n",
    "                break\n",
    "            \n",
    "            generated.append(next_word)\n",
    "            \n",
    "            # Update context\n",
    "            context = context[1:] + [next_idx]\n",
    "        \n",
    "        return ' '.join(generated)\n",
    "\n",
    "# Compare neural vs n-gram models\n",
    "def compare_neural_ngram():\n",
    "    \"\"\"Compare neural and n-gram language models.\"\"\"\n",
    "    \n",
    "    # Training data\n",
    "    training_texts = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"A quick brown fox ran through the forest.\",\n",
    "        \"The lazy dog slept under the tree.\",\n",
    "        \"Brown foxes are quick and clever animals.\",\n",
    "        \"Dogs and foxes are different animals.\",\n",
    "        \"The forest was full of trees and animals.\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize models\n",
    "    ngram_model = NGramLanguageModel(n=3, smoothing='laplace')\n",
    "    ngram_model.train(training_texts)\n",
    "    \n",
    "    neural_model = SimpleNeuralLM(\n",
    "        vocab_size=100,  # Will be updated\n",
    "        embedding_dim=16,\n",
    "        hidden_dim=32,\n",
    "        context_size=3\n",
    "    )\n",
    "    neural_model.build_vocab(training_texts)\n",
    "    \n",
    "    # Create training data for neural model\n",
    "    contexts, targets = neural_model.create_training_data(training_texts)\n",
    "    \n",
    "    # Train neural model (simplified)\n",
    "    losses = []\n",
    "    for epoch in range(100):\n",
    "        loss = neural_model.train_step(contexts, targets)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Generate samples\n",
    "    print(\"Model Comparison:\")\n",
    "    print(\"\\nN-gram samples:\")\n",
    "    for i in range(3):\n",
    "        sample = ngram_model.generate(max_length=15)\n",
    "        print(f\"  {i+1}: {sample}\")\n",
    "    \n",
    "    print(\"\\nNeural model samples:\")\n",
    "    for i in range(3):\n",
    "        sample = neural_model.generate(\"\", max_length=15)\n",
    "        print(f\"  {i+1}: {sample}\")\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.title('Neural LM Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cross-Entropy Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Visualize embeddings (2D projection)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    # Simple 2D projection of embeddings\n",
    "    embeddings_2d = neural_model.embeddings[:, :2]  # Take first 2 dimensions\n",
    "    \n",
    "    # Plot a few word embeddings\n",
    "    words_to_plot = ['the', 'fox', 'dog', 'quick', 'brown', 'lazy']\n",
    "    for word in words_to_plot:\n",
    "        if word in neural_model.word_to_idx:\n",
    "            idx = neural_model.word_to_idx[word]\n",
    "            x, y = embeddings_2d[idx]\n",
    "            plt.scatter(x, y, s=100)\n",
    "            plt.annotate(word, (x, y), xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.title('Word Embeddings (2D Projection)')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Model comparison\n",
    "    plt.subplot(1, 3, 3)\n",
    "    comparison_data = {\n",
    "        'N-gram': [len(ngram_model.vocab), len(ngram_model.ngram_counts), 0],\n",
    "        'Neural': [neural_model.vocab_size, contexts.shape[0], \n",
    "                  neural_model.embedding_dim * neural_model.vocab_size + \n",
    "                  neural_model.hidden_dim * neural_model.context_size * neural_model.embedding_dim]\n",
    "    }\n",
    "    \n",
    "    x = np.arange(3)\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, comparison_data['N-gram'], width, label='N-gram', alpha=0.7)\n",
    "    plt.bar(x + width/2, comparison_data['Neural'], width, label='Neural', alpha=0.7)\n",
    "    \n",
    "    plt.title('Model Comparison')\n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(x, ['Vocab Size', 'Training Examples', 'Parameters'])\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nKey Differences:\")\n",
    "    print(\"N-gram Models:\")\n",
    "    print(\"  + Simple and interpretable\")\n",
    "    print(\"  + Fast training and inference\")\n",
    "    print(\"  - Discrete, sparse representations\")\n",
    "    print(\"  - Limited context window\")\n",
    "    \n",
    "    print(\"\\nNeural Models:\")\n",
    "    print(\"  + Dense, learned representations\")\n",
    "    print(\"  + Can capture semantic similarities\")\n",
    "    print(\"  + Scalable to larger contexts\")\n",
    "    print(\"  - More complex training\")\n",
    "    print(\"  - Requires more data\")\n",
    "\n",
    "compare_neural_ngram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Autoregressive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class AutoregressiveModel:\n",
    "    \"\"\"Autoregressive language model demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, d_model: int = 64):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Simple transformer-like parameters\n",
    "        self.token_embedding = np.random.randn(vocab_size, d_model) * 0.1\n",
    "        self.pos_embedding = np.random.randn(100, d_model) * 0.1  # Max seq len 100\n",
    "        self.output_projection = np.random.randn(d_model, vocab_size) * 0.1\n",
    "        \n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "    \n",
    "    def build_vocab(self, texts: List[str]):\n",
    "        \"\"\"Build vocabulary.\"\"\"\n",
    "        words = set()\n",
    "        for text in texts:\n",
    "            tokens = re.findall(r'\\b\\w+\\b|[.,!?;]', text.lower())\n",
    "            words.update(tokens)\n",
    "        \n",
    "        words = ['<PAD>', '<BOS>', '<EOS>'] + sorted(list(words))\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(words)}\n",
    "        self.idx_to_word = {i: word for word, i in self.word_to_idx.items()}\n",
    "        \n",
    "        self.vocab_size = len(words)\n",
    "        # Reinitialize with correct vocab size\n",
    "        self.token_embedding = np.random.randn(self.vocab_size, self.d_model) * 0.1\n",
    "        self.output_projection = np.random.randn(self.d_model, self.vocab_size) * 0.1\n",
    "    \n",
    "    def create_causal_mask(self, seq_len: int) -> np.ndarray:\n",
    "        \"\"\"Create causal mask for autoregressive generation.\"\"\"\n",
    "        mask = np.triu(np.ones((seq_len, seq_len)), k=1)\n",
    "        return mask == 0  # True where attention is allowed\n",
    "    \n",
    "    def forward(self, input_ids: np.ndarray, use_cache: bool = False) -> np.ndarray:\n",
    "        \"\"\"Forward pass (simplified).\"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Token embeddings\n",
    "        token_embeds = self.token_embedding[input_ids]  # (batch, seq_len, d_model)\n",
    "        \n",
    "        # Positional embeddings\n",
    "        pos_embeds = self.pos_embedding[:seq_len]  # (seq_len, d_model)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        hidden_states = token_embeds + pos_embeds[None, :, :]  # (batch, seq_len, d_model)\n",
    "        \n",
    "        # Simple transformation (in real model, this would be transformer layers)\n",
    "        hidden_states = np.tanh(hidden_states)\n",
    "        \n",
    "        # Output projection\n",
    "        logits = hidden_states @ self.output_projection  # (batch, seq_len, vocab_size)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def generate_autoregressive(self, prompt: str, max_length: int = 20, \n",
    "                              temperature: float = 1.0) -> str:\n",
    "        \"\"\"Generate text autoregressively.\"\"\"\n",
    "        # Tokenize prompt\n",
    "        if prompt:\n",
    "            tokens = re.findall(r'\\b\\w+\\b|[.,!?;]', prompt.lower())\n",
    "            input_ids = [self.word_to_idx.get(token, 0) for token in tokens]\n",
    "        else:\n",
    "            input_ids = [self.word_to_idx['<BOS>']]\n",
    "        \n",
    "        generated_ids = input_ids.copy()\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Prepare input\n",
    "            current_input = np.array([generated_ids])\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = self.forward(current_input)\n",
    "            \n",
    "            # Get logits for next token (last position)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            \n",
    "            # Apply softmax\n",
    "            probs = np.exp(next_token_logits - np.max(next_token_logits))\n",
    "            probs = probs / np.sum(probs)\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token_id = np.random.choice(self.vocab_size, p=probs)\n",
    "            \n",
    "            # Check for EOS\n",
    "            if next_token_id == self.word_to_idx.get('<EOS>', -1):\n",
    "                break\n",
    "            \n",
    "            generated_ids.append(next_token_id)\n",
    "        \n",
    "        # Convert back to text\n",
    "        generated_tokens = [self.idx_to_word.get(idx, '<UNK>') for idx in generated_ids]\n",
    "        return ' '.join(generated_tokens)\n",
    "\n",
    "def demonstrate_autoregressive():\n",
    "    \"\"\"Demonstrate autoregressive generation.\"\"\"\n",
    "    \n",
    "    # Sample data\n",
    "    texts = [\n",
    "        \"The sun rises in the east.\",\n",
    "        \"Birds fly in the sky.\",\n",
    "        \"The ocean is deep and blue.\",\n",
    "        \"Mountains are tall and majestic.\",\n",
    "        \"Flowers bloom in spring.\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize model\n",
    "    model = AutoregressiveModel(vocab_size=100)\n",
    "    model.build_vocab(texts)\n",
    "    \n",
    "    # Demonstrate causal masking\n",
    "    seq_len = 8\n",
    "    causal_mask = model.create_causal_mask(seq_len)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Visualize causal mask\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.imshow(causal_mask, cmap='RdYlBu', aspect='auto')\n",
    "    plt.title('Causal Attention Mask')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Show autoregressive generation process\n",
    "    plt.subplot(2, 3, 2)\n",
    "    \n",
    "    # Simulate generation steps\n",
    "    steps = ['the', 'sun', 'rises', 'in', 'the']\n",
    "    step_probs = []\n",
    "    \n",
    "    for i, step in enumerate(steps):\n",
    "        # Create input up to current step\n",
    "        current_tokens = steps[:i+1]\n",
    "        input_ids = [model.word_to_idx.get(token, 0) for token in current_tokens]\n",
    "        \n",
    "        # Pad to same length for visualization\n",
    "        padded_input = input_ids + [0] * (len(steps) - len(input_ids))\n",
    "        step_probs.append(padded_input)\n",
    "    \n",
    "    step_matrix = np.array(step_probs)\n",
    "    plt.imshow(step_matrix, cmap='viridis', aspect='auto')\n",
    "    plt.title('Autoregressive Generation Steps')\n",
    "    plt.xlabel('Token Position')\n",
    "    plt.ylabel('Generation Step')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Generate samples with different temperatures\n",
    "    plt.subplot(2, 3, 3)\n",
    "    temperatures = [0.5, 1.0, 1.5, 2.0]\n",
    "    \n",
    "    # Simulate probability distributions for different temperatures\n",
    "    base_logits = np.array([2.0, 1.5, 1.0, 0.5, 0.2])\n",
    "    \n",
    "    for i, temp in enumerate(temperatures):\n",
    "        scaled_logits = base_logits / temp\n",
    "        probs = np.exp(scaled_logits - np.max(scaled_logits))\n",
    "        probs = probs / np.sum(probs)\n",
    "        \n",
    "        plt.plot(probs, label=f'T={temp}', marker='o')\n",
    "    \n",
    "    plt.title('Temperature Effect on Sampling')\n",
    "    plt.xlabel('Token Index')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Show generation examples\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.text(0.1, 0.9, \"Autoregressive Generation Examples:\", fontsize=12, weight='bold')\n",
    "    \n",
    "    y_pos = 0.8\n",
    "    prompts = [\"the\", \"birds\", \"ocean\"]\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        generated = model.generate_autoregressive(prompt, max_length=8, temperature=1.0)\n",
    "        plt.text(0.1, y_pos, f\"'{prompt}' → {generated}\", fontsize=10)\n",
    "        y_pos -= 0.15\n",
    "    \n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Compare different sampling strategies\n",
    "    plt.subplot(2, 3, 5)\n",
    "    \n",
    "    # Simulate different sampling methods\n",
    "    logits = np.array([3.0, 2.0, 1.0, 0.5, 0.1, -0.5, -1.0])\n",
    "    probs = np.exp(logits - np.max(logits))\n",
    "    probs = probs / np.sum(probs)\n",
    "    \n",
    "    # Greedy (argmax)\n",
    "    greedy = np.zeros_like(probs)\n",
    "    greedy[np.argmax(probs)] = 1.0\n",
    "    \n",
    "    # Top-k (k=3)\n",
    "    top_k = probs.copy()\n",
    "    top_k_indices = np.argsort(probs)[-3:]\n",
    "    mask = np.zeros_like(probs, dtype=bool)\n",
    "    mask[top_k_indices] = True\n",
    "    top_k[~mask] = 0\n",
    "    top_k = top_k / np.sum(top_k)\n",
    "    \n",
    "    x = np.arange(len(probs))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x - width, probs, width, label='Original', alpha=0.7)\n",
    "    plt.bar(x, greedy, width, label='Greedy', alpha=0.7)\n",
    "    plt.bar(x + width, top_k, width, label='Top-k (k=3)', alpha=0.7)\n",
    "    \n",
    "    plt.title('Sampling Strategies')\n",
    "    plt.xlabel('Token Index')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Show autoregressive property\n",
    "    plt.subplot(2, 3, 6)\n",
    "    \n",
    "    # Demonstrate that P(w1,w2,w3) = P(w1) * P(w2|w1) * P(w3|w1,w2)\n",
    "    sequence = \"the sun rises\"\n",
    "    tokens = sequence.split()\n",
    "    \n",
    "    # Simulate conditional probabilities\n",
    "    probs = [0.8, 0.6, 0.7]  # P(the), P(sun|the), P(rises|the,sun)\n",
    "    cumulative_prob = np.cumprod(probs)\n",
    "    \n",
    "    plt.plot(range(1, len(tokens)+1), probs, 'o-', label='Conditional P(wi|w<i)', linewidth=2)\n",
    "    plt.plot(range(1, len(tokens)+1), cumulative_prob, 's-', label='Joint P(w1...wi)', linewidth=2)\n",
    "    \n",
    "    plt.title('Autoregressive Factorization')\n",
    "    plt.xlabel('Token Position')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xticks(range(1, len(tokens)+1), tokens)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Autoregressive Language Modeling:\")\n",
    "    print(\"\\nKey Properties:\")\n",
    "    print(\"- Factorizes joint probability: P(w1...wn) = ∏P(wi|w<i)\")\n",
    "    print(\"- Uses causal masking to prevent future information leakage\")\n",
    "    print(\"- Generates text left-to-right, one token at a time\")\n",
    "    print(\"- Temperature controls randomness in sampling\")\n",
    "    \n",
    "    print(\"\\nAdvantages:\")\n",
    "    print(\"- Natural for text generation tasks\")\n",
    "    print(\"- Can generate variable-length sequences\")\n",
    "    print(\"- Straightforward training objective\")\n",
    "    \n",
    "    print(\"\\nApplications:\")\n",
    "    print(\"- GPT family models\")\n",
    "    print(\"- Text completion\")\n",
    "    print(\"- Creative writing\")\n",
    "    print(\"- Code generation\")\n",
    "\n",
    "demonstrate_autoregressive()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
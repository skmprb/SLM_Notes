{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Training Large Language Models\n",
    "\n",
    "This notebook covers practical aspects of training large language models.\n",
    "\n",
    "## Topics Covered:\n",
    "- Pretraining objectives\n",
    "- Next-token prediction\n",
    "- Training data collection and preprocessing\n",
    "- Batch processing and token batching\n",
    "- Mixed-precision training\n",
    "- Distributed training strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pretraining Objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class PretrainingObjectives:\n",
    "    \"\"\"Different pretraining objectives for language models.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def next_token_prediction_loss(logits: np.ndarray, targets: np.ndarray) -> float:\n",
    "        \"\"\"Compute next-token prediction loss (cross-entropy).\"\"\"\n",
    "        # Apply softmax to logits\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=-1, keepdims=True))\n",
    "        probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        batch_size, seq_len = targets.shape\n",
    "        loss = 0\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                target_prob = probs[i, j, targets[i, j]]\n",
    "                loss += -np.log(target_prob + 1e-10)\n",
    "        \n",
    "        return loss / (batch_size * seq_len)\n",
    "    \n",
    "    @staticmethod\n",
    "    def masked_language_modeling_loss(logits: np.ndarray, targets: np.ndarray, \n",
    "                                    mask: np.ndarray) -> float:\n",
    "        \"\"\"Compute MLM loss (only on masked positions).\"\"\"\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=-1, keepdims=True))\n",
    "        probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "        \n",
    "        loss = 0\n",
    "        masked_count = 0\n",
    "        \n",
    "        batch_size, seq_len = targets.shape\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                if mask[i, j]:  # Only compute loss on masked positions\n",
    "                    target_prob = probs[i, j, targets[i, j]]\n",
    "                    loss += -np.log(target_prob + 1e-10)\n",
    "                    masked_count += 1\n",
    "        \n",
    "        return loss / masked_count if masked_count > 0 else 0\n",
    "\n",
    "def demonstrate_pretraining_objectives():\n",
    "    \"\"\"Compare different pretraining objectives.\"\"\"\n",
    "    \n",
    "    # Sample data\n",
    "    batch_size, seq_len, vocab_size = 2, 8, 100\n",
    "    \n",
    "    # Random logits and targets\n",
    "    logits = np.random.randn(batch_size, seq_len, vocab_size)\n",
    "    targets = np.random.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    \n",
    "    # Create mask for MLM (15% of tokens masked)\n",
    "    mask = np.random.random((batch_size, seq_len)) < 0.15\n",
    "    \n",
    "    # Compute losses\n",
    "    ntp_loss = PretrainingObjectives.next_token_prediction_loss(logits, targets)\n",
    "    mlm_loss = PretrainingObjectives.masked_language_modeling_loss(logits, targets, mask)\n",
    "    \n",
    "    print(\"Pretraining Objectives Comparison:\")\n",
    "    print(f\"Next-Token Prediction Loss: {ntp_loss:.4f}\")\n",
    "    print(f\"Masked Language Modeling Loss: {mlm_loss:.4f}\")\n",
    "    \n",
    "    # Visualize masking pattern\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(mask, cmap='RdYlBu', aspect='auto')\n",
    "    plt.title('MLM Masking Pattern')\n",
    "    plt.xlabel('Sequence Position')\n",
    "    plt.ylabel('Batch Item')\n",
    "    plt.colorbar(label='Masked (1) / Not Masked (0)')\n",
    "    \n",
    "    # Show loss computation differences\n",
    "    plt.subplot(1, 2, 2)\n",
    "    \n",
    "    objectives = ['Next-Token\\nPrediction', 'Masked LM']\n",
    "    losses = [ntp_loss, mlm_loss]\n",
    "    coverage = [100, np.mean(mask) * 100]  # Percentage of tokens used\n",
    "    \n",
    "    x = np.arange(len(objectives))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1 = plt.gca()\n",
    "    bars1 = ax1.bar(x - width/2, losses, width, label='Loss', alpha=0.7)\n",
    "    ax1.set_ylabel('Loss Value')\n",
    "    ax1.set_title('Objective Comparison')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x + width/2, coverage, width, label='Token Coverage %', alpha=0.7, color='orange')\n",
    "    ax2.set_ylabel('Token Coverage (%)')\n",
    "    \n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(objectives)\n",
    "    \n",
    "    # Add legends\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nKey Differences:\")\n",
    "    print(\"Next-Token Prediction (GPT-style):\")\n",
    "    print(\"  - Uses all tokens for training\")\n",
    "    print(\"  - Autoregressive generation\")\n",
    "    print(\"  - Causal (left-to-right) attention\")\n",
    "    \n",
    "    print(\"\\nMasked Language Modeling (BERT-style):\")\n",
    "    print(\"  - Uses only masked tokens (~15%)\")\n",
    "    print(\"  - Bidirectional context\")\n",
    "    print(\"  - Better for understanding tasks\")\n",
    "\n",
    "demonstrate_pretraining_objectives()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"Data preprocessing pipeline for LLM training.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stats = defaultdict(int)\n",
    "    \n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        \"\"\"Basic text normalization.\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Handle special characters\n",
    "        text = re.sub(r\"[\\\"'„‚]\", '\"', text)  # Normalize quotes\n",
    "        text = re.sub(r'[–—]', '-', text)  # Normalize dashes\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def detect_language(self, text: str) -> str:\n",
    "        \"\"\"Simple language detection (simplified).\"\"\"\n",
    "        # Count character frequencies for basic detection\n",
    "        char_counts = defaultdict(int)\n",
    "        for char in text.lower():\n",
    "            if char.isalpha():\n",
    "                char_counts[char] += 1\n",
    "        \n",
    "        # Simple heuristics\n",
    "        total_chars = sum(char_counts.values())\n",
    "        if total_chars == 0:\n",
    "            return 'unknown'\n",
    "        \n",
    "        # Check for common English patterns\n",
    "        english_chars = char_counts['e'] + char_counts['t'] + char_counts['a']\n",
    "        if english_chars / total_chars > 0.25:\n",
    "            return 'en'\n",
    "        \n",
    "        return 'other'\n",
    "    \n",
    "    def quality_filter(self, text: str) -> bool:\n",
    "        \"\"\"Filter low-quality text.\"\"\"\n",
    "        # Length checks\n",
    "        if len(text) < 50 or len(text) > 10000:\n",
    "            self.stats['filtered_length'] += 1\n",
    "            return False\n",
    "        \n",
    "        # Character diversity\n",
    "        unique_chars = len(set(text.lower()))\n",
    "        if unique_chars < 10:\n",
    "            self.stats['filtered_diversity'] += 1\n",
    "            return False\n",
    "        \n",
    "        # Repetition check\n",
    "        words = text.split()\n",
    "        if len(words) > 0:\n",
    "            word_counts = defaultdict(int)\n",
    "            for word in words:\n",
    "                word_counts[word] += 1\n",
    "            \n",
    "            max_repetition = max(word_counts.values())\n",
    "            if max_repetition > len(words) * 0.3:  # More than 30% repetition\n",
    "                self.stats['filtered_repetition'] += 1\n",
    "                return False\n",
    "        \n",
    "        self.stats['passed_quality'] += 1\n",
    "        return True\n",
    "    \n",
    "    def deduplicate(self, texts: List[str], threshold: float = 0.8) -> List[str]:\n",
    "        \"\"\"Simple deduplication based on character overlap.\"\"\"\n",
    "        def jaccard_similarity(text1: str, text2: str) -> float:\n",
    "            set1 = set(text1.lower().split())\n",
    "            set2 = set(text2.lower().split())\n",
    "            intersection = len(set1.intersection(set2))\n",
    "            union = len(set1.union(set2))\n",
    "            return intersection / union if union > 0 else 0\n",
    "        \n",
    "        deduplicated = []\n",
    "        for text in texts:\n",
    "            is_duplicate = False\n",
    "            for existing in deduplicated:\n",
    "                if jaccard_similarity(text, existing) > threshold:\n",
    "                    is_duplicate = True\n",
    "                    self.stats['duplicates_removed'] += 1\n",
    "                    break\n",
    "            \n",
    "            if not is_duplicate:\n",
    "                deduplicated.append(text)\n",
    "        \n",
    "        return deduplicated\n",
    "    \n",
    "    def process_batch(self, texts: List[str]) -> List[str]:\n",
    "        \"\"\"Process a batch of texts.\"\"\"\n",
    "        processed = []\n",
    "        \n",
    "        for text in texts:\n",
    "            # Normalize\n",
    "            normalized = self.normalize_text(text)\n",
    "            \n",
    "            # Language detection\n",
    "            lang = self.detect_language(normalized)\n",
    "            if lang != 'en':\n",
    "                self.stats['filtered_language'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Quality filter\n",
    "            if not self.quality_filter(normalized):\n",
    "                continue\n",
    "            \n",
    "            processed.append(normalized)\n",
    "        \n",
    "        # Deduplicate\n",
    "        processed = self.deduplicate(processed)\n",
    "        \n",
    "        return processed\n",
    "\n",
    "def demonstrate_data_preprocessing():\n",
    "    \"\"\"Demonstrate data preprocessing pipeline.\"\"\"\n",
    "    \n",
    "    # Sample raw texts with various quality issues\n",
    "    raw_texts = [\n",
    "        \"This is a high-quality text with proper grammar and structure. It contains meaningful content.\",\n",
    "        \"short text\",  # Too short\n",
    "        \"This is a high-quality text with proper grammar and structure. It contains meaningful content.\",  # Duplicate\n",
    "        \"aaaaa aaaaa aaaaa aaaaa aaaaa aaaaa aaaaa aaaaa\",  # Too repetitive\n",
    "        \"Another good quality text that provides valuable information and insights about various topics.\",\n",
    "        \"MIXED    CASE   TEXT   WITH    WEIRD     SPACING!!!\",\n",
    "        \"The quick brown fox jumps over the lazy dog. This sentence contains every letter of the alphabet.\",\n",
    "        \"x\" * 15000,  # Too long\n",
    "        \"Bonjour, comment allez-vous? Je suis très bien.\",  # Non-English\n",
    "        \"Final text with good quality and reasonable length for training purposes.\"\n",
    "    ]\n",
    "    \n",
    "    # Process the data\n",
    "    preprocessor = DataPreprocessor()\n",
    "    processed_texts = preprocessor.process_batch(raw_texts)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Data Preprocessing Results:\")\n",
    "    print(f\"Input texts: {len(raw_texts)}\")\n",
    "    print(f\"Output texts: {len(processed_texts)}\")\n",
    "    print(f\"Retention rate: {len(processed_texts)/len(raw_texts)*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\nFiltering Statistics:\")\n",
    "    for reason, count in preprocessor.stats.items():\n",
    "        print(f\"  {reason}: {count}\")\n",
    "    \n",
    "    # Visualize preprocessing pipeline\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Pipeline flow\n",
    "    plt.subplot(2, 3, 1)\n",
    "    stages = ['Raw\\nTexts', 'Normalized', 'Language\\nFiltered', 'Quality\\nFiltered', 'Deduplicated']\n",
    "    counts = [\n",
    "        len(raw_texts),\n",
    "        len(raw_texts),  # Normalization doesn't remove texts\n",
    "        len(raw_texts) - preprocessor.stats['filtered_language'],\n",
    "        preprocessor.stats['passed_quality'],\n",
    "        len(processed_texts)\n",
    "    ]\n",
    "    \n",
    "    plt.plot(stages, counts, 'o-', linewidth=2, markersize=8)\n",
    "    plt.title('Data Processing Pipeline')\n",
    "    plt.ylabel('Number of Texts')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Text length distribution\n",
    "    plt.subplot(2, 3, 2)\n",
    "    raw_lengths = [len(text) for text in raw_texts]\n",
    "    processed_lengths = [len(text) for text in processed_texts]\n",
    "    \n",
    "    plt.hist(raw_lengths, bins=10, alpha=0.7, label='Raw', density=True)\n",
    "    plt.hist(processed_lengths, bins=10, alpha=0.7, label='Processed', density=True)\n",
    "    plt.title('Text Length Distribution')\n",
    "    plt.xlabel('Text Length (characters)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Quality metrics\n",
    "    plt.subplot(2, 3, 3)\n",
    "    filter_reasons = list(preprocessor.stats.keys())\n",
    "    filter_counts = list(preprocessor.stats.values())\n",
    "    \n",
    "    plt.pie(filter_counts, labels=filter_reasons, autopct='%1.1f%%')\n",
    "    plt.title('Filtering Breakdown')\n",
    "    \n",
    "    # Show sample processed texts\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.text(0.1, 0.9, \"Sample Processed Texts:\", fontsize=12, weight='bold')\n",
    "    \n",
    "    y_pos = 0.8\n",
    "    for i, text in enumerate(processed_texts[:3]):\n",
    "        truncated = text[:50] + \"...\" if len(text) > 50 else text\n",
    "        plt.text(0.1, y_pos, f\"{i+1}: {truncated}\", fontsize=9, wrap=True)\n",
    "        y_pos -= 0.2\n",
    "    \n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Character diversity analysis\n",
    "    plt.subplot(2, 3, 5)\n",
    "    diversities = [len(set(text.lower())) for text in processed_texts]\n",
    "    plt.hist(diversities, bins=5, alpha=0.7)\n",
    "    plt.title('Character Diversity')\n",
    "    plt.xlabel('Unique Characters')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Processing efficiency\n",
    "    plt.subplot(2, 3, 6)\n",
    "    efficiency_metrics = {\n",
    "        'Retention Rate': len(processed_texts)/len(raw_texts)*100,\n",
    "        'Quality Rate': preprocessor.stats['passed_quality']/len(raw_texts)*100,\n",
    "        'Dedup Rate': (1 - preprocessor.stats['duplicates_removed']/len(raw_texts))*100\n",
    "    }\n",
    "    \n",
    "    metrics = list(efficiency_metrics.keys())\n",
    "    values = list(efficiency_metrics.values())\n",
    "    \n",
    "    plt.bar(metrics, values, alpha=0.7)\n",
    "    plt.title('Processing Efficiency')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPreprocessing Pipeline Components:\")\n",
    "    print(\"1. Text Normalization: Standardize format and encoding\")\n",
    "    print(\"2. Language Detection: Filter non-target languages\")\n",
    "    print(\"3. Quality Filtering: Remove low-quality content\")\n",
    "    print(\"4. Deduplication: Remove duplicate or near-duplicate content\")\n",
    "    print(\"5. Format Validation: Ensure proper structure\")\n",
    "\n",
    "demonstrate_data_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Token Batching and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class TokenBatcher:\n",
    "    \"\"\"Efficient token batching with padding strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self, pad_token_id: int = 0):\n",
    "        self.pad_token_id = pad_token_id\n",
    "    \n",
    "    def static_batching(self, sequences: List[List[int]], batch_size: int, \n",
    "                       max_length: int) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"Static batching with fixed sequence length.\"\"\"\n",
    "        batches = []\n",
    "        \n",
    "        for i in range(0, len(sequences), batch_size):\n",
    "            batch_sequences = sequences[i:i + batch_size]\n",
    "            \n",
    "            # Pad sequences to max_length\n",
    "            padded_batch = []\n",
    "            attention_masks = []\n",
    "            \n",
    "            for seq in batch_sequences:\n",
    "                if len(seq) > max_length:\n",
    "                    # Truncate\n",
    "                    padded_seq = seq[:max_length]\n",
    "                    mask = [1] * max_length\n",
    "                else:\n",
    "                    # Pad\n",
    "                    padded_seq = seq + [self.pad_token_id] * (max_length - len(seq))\n",
    "                    mask = [1] * len(seq) + [0] * (max_length - len(seq))\n",
    "                \n",
    "                padded_batch.append(padded_seq)\n",
    "                attention_masks.append(mask)\n",
    "            \n",
    "            batches.append((np.array(padded_batch), np.array(attention_masks)))\n",
    "        \n",
    "        return batches\n",
    "    \n",
    "    def dynamic_batching(self, sequences: List[List[int]], \n",
    "                        max_tokens: int) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"Dynamic batching based on token count.\"\"\"\n",
    "        # Sort sequences by length for efficient packing\n",
    "        sorted_sequences = sorted(sequences, key=len)\n",
    "        \n",
    "        batches = []\n",
    "        current_batch = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for seq in sorted_sequences:\n",
    "            seq_length = len(seq)\n",
    "            \n",
    "            # Calculate tokens needed if we add this sequence\n",
    "            if current_batch:\n",
    "                max_len_in_batch = max(len(s) for s in current_batch + [seq])\n",
    "                tokens_needed = max_len_in_batch * (len(current_batch) + 1)\n",
    "            else:\n",
    "                tokens_needed = seq_length\n",
    "            \n",
    "            if tokens_needed <= max_tokens:\n",
    "                current_batch.append(seq)\n",
    "                current_tokens = tokens_needed\n",
    "            else:\n",
    "                # Finalize current batch\n",
    "                if current_batch:\n",
    "                    batches.append(self._create_padded_batch(current_batch))\n",
    "                \n",
    "                # Start new batch\n",
    "                current_batch = [seq]\n",
    "                current_tokens = seq_length\n",
    "        \n",
    "        # Add final batch\n",
    "        if current_batch:\n",
    "            batches.append(self._create_padded_batch(current_batch))\n",
    "        \n",
    "        return batches\n",
    "    \n",
    "    def sequence_packing(self, sequences: List[List[int]], \n",
    "                        max_length: int) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"Pack multiple sequences into single examples.\"\"\"\n",
    "        packed_sequences = []\n",
    "        current_packed = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for seq in sequences:\n",
    "            if current_length + len(seq) + 1 <= max_length:  # +1 for separator\n",
    "                if current_packed:\n",
    "                    current_packed.extend([self.pad_token_id])  # Separator\n",
    "                    current_length += 1\n",
    "                \n",
    "                current_packed.extend(seq)\n",
    "                current_length += len(seq)\n",
    "            else:\n",
    "                # Finalize current packed sequence\n",
    "                if current_packed:\n",
    "                    packed_sequences.append(current_packed)\n",
    "                \n",
    "                # Start new packed sequence\n",
    "                current_packed = seq.copy()\n",
    "                current_length = len(seq)\n",
    "        \n",
    "        # Add final packed sequence\n",
    "        if current_packed:\n",
    "            packed_sequences.append(current_packed)\n",
    "        \n",
    "        # Create batches from packed sequences\n",
    "        return self.static_batching(packed_sequences, batch_size=32, max_length=max_length)\n",
    "    \n",
    "    def _create_padded_batch(self, sequences: List[List[int]]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Create padded batch from sequences.\"\"\"\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "        \n",
    "        padded_batch = []\n",
    "        attention_masks = []\n",
    "        \n",
    "        for seq in sequences:\n",
    "            padded_seq = seq + [self.pad_token_id] * (max_len - len(seq))\n",
    "            mask = [1] * len(seq) + [0] * (max_len - len(seq))\n",
    "            \n",
    "            padded_batch.append(padded_seq)\n",
    "            attention_masks.append(mask)\n",
    "        \n",
    "        return np.array(padded_batch), np.array(attention_masks)\n",
    "\n",
    "def demonstrate_token_batching():\n",
    "    \"\"\"Demonstrate different batching strategies.\"\"\"\n",
    "    \n",
    "    # Generate sample sequences with varying lengths\n",
    "    np.random.seed(42)\n",
    "    sequences = []\n",
    "    for _ in range(20):\n",
    "        length = np.random.randint(10, 100)\n",
    "        seq = np.random.randint(1, 1000, length).tolist()\n",
    "        sequences.append(seq)\n",
    "    \n",
    "    batcher = TokenBatcher(pad_token_id=0)\n",
    "    \n",
    "    # Compare different batching strategies\n",
    "    static_batches = batcher.static_batching(sequences, batch_size=4, max_length=80)\n",
    "    dynamic_batches = batcher.dynamic_batching(sequences, max_tokens=320)\n",
    "    packed_batches = batcher.sequence_packing(sequences, max_length=150)\n",
    "    \n",
    "    # Calculate efficiency metrics\n",
    "    def calculate_efficiency(batches):\n",
    "        total_tokens = 0\n",
    "        total_padding = 0\n",
    "        \n",
    "        for batch, mask in batches:\n",
    "            total_tokens += np.sum(mask)\n",
    "            total_padding += np.sum(1 - mask)\n",
    "        \n",
    "        efficiency = total_tokens / (total_tokens + total_padding) * 100\n",
    "        return efficiency, total_tokens, total_padding\n",
    "    \n",
    "    static_eff, static_tokens, static_padding = calculate_efficiency(static_batches)\n",
    "    dynamic_eff, dynamic_tokens, dynamic_padding = calculate_efficiency(dynamic_batches)\n",
    "    packed_eff, packed_tokens, packed_padding = calculate_efficiency(packed_batches)\n",
    "    \n",
    "    print(\"Token Batching Comparison:\")\n",
    "    print(f\"\\nStatic Batching:\")\n",
    "    print(f\"  Batches: {len(static_batches)}\")\n",
    "    print(f\"  Efficiency: {static_eff:.1f}%\")\n",
    "    print(f\"  Tokens: {static_tokens}, Padding: {static_padding}\")\n",
    "    \n",
    "    print(f\"\\nDynamic Batching:\")\n",
    "    print(f\"  Batches: {len(dynamic_batches)}\")\n",
    "    print(f\"  Efficiency: {dynamic_eff:.1f}%\")\n",
    "    print(f\"  Tokens: {dynamic_tokens}, Padding: {dynamic_padding}\")\n",
    "    \n",
    "    print(f\"\\nSequence Packing:\")\n",
    "    print(f\"  Batches: {len(packed_batches)}\")\n",
    "    print(f\"  Efficiency: {packed_eff:.1f}%\")\n",
    "    print(f\"  Tokens: {packed_tokens}, Padding: {packed_padding}\")\n",
    "    \n",
    "    # Visualize batching strategies\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Sequence length distribution\n",
    "    plt.subplot(3, 3, 1)\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    plt.hist(lengths, bins=10, alpha=0.7)\n",
    "    plt.title('Sequence Length Distribution')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Efficiency comparison\n",
    "    plt.subplot(3, 3, 2)\n",
    "    strategies = ['Static', 'Dynamic', 'Packed']\n",
    "    efficiencies = [static_eff, dynamic_eff, packed_eff]\n",
    "    \n",
    "    bars = plt.bar(strategies, efficiencies, alpha=0.7)\n",
    "    plt.title('Batching Efficiency')\n",
    "    plt.ylabel('Efficiency (%)')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, eff in zip(bars, efficiencies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                f'{eff:.1f}%', ha='center')\n",
    "    \n",
    "    # Batch size distribution\n",
    "    plt.subplot(3, 3, 3)\n",
    "    batch_sizes = {\n",
    "        'Static': [len(batch) for batch, _ in static_batches],\n",
    "        'Dynamic': [len(batch) for batch, _ in dynamic_batches],\n",
    "        'Packed': [len(batch) for batch, _ in packed_batches]\n",
    "    }\n",
    "    \n",
    "    for strategy, sizes in batch_sizes.items():\n",
    "        plt.hist(sizes, alpha=0.5, label=strategy, bins=5)\n",
    "    \n",
    "    plt.title('Batch Size Distribution')\n",
    "    plt.xlabel('Batch Size')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Visualize sample batches\n",
    "    strategies_data = [\n",
    "        ('Static Batching', static_batches[0]),\n",
    "        ('Dynamic Batching', dynamic_batches[0]),\n",
    "        ('Packed Batching', packed_batches[0])\n",
    "    ]\n",
    "    \n",
    "    for i, (name, (batch, mask)) in enumerate(strategies_data):\n",
    "        plt.subplot(3, 3, 4 + i)\n",
    "        plt.imshow(mask, cmap='RdYlBu', aspect='auto')\n",
    "        plt.title(f'{name}\\nSample Batch Mask')\n",
    "        plt.xlabel('Sequence Position')\n",
    "        plt.ylabel('Batch Item')\n",
    "        plt.colorbar(label='Token (1) / Padding (0)')\n",
    "    \n",
    "    # Token utilization\n",
    "    plt.subplot(3, 3, 7)\n",
    "    utilization_data = {\n",
    "        'Static': [static_tokens, static_padding],\n",
    "        'Dynamic': [dynamic_tokens, dynamic_padding],\n",
    "        'Packed': [packed_tokens, packed_padding]\n",
    "    }\n",
    "    \n",
    "    x = np.arange(len(strategies))\n",
    "    width = 0.35\n",
    "    \n",
    "    tokens_data = [data[0] for data in utilization_data.values()]\n",
    "    padding_data = [data[1] for data in utilization_data.values()]\n",
    "    \n",
    "    plt.bar(x, tokens_data, width, label='Tokens', alpha=0.7)\n",
    "    plt.bar(x, padding_data, width, bottom=tokens_data, label='Padding', alpha=0.7)\n",
    "    \n",
    "    plt.title('Token vs Padding')\n",
    "    plt.xlabel('Strategy')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(x, strategies)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Memory usage comparison\n",
    "    plt.subplot(3, 3, 8)\n",
    "    memory_usage = {\n",
    "        'Static': sum(batch.size for batch, _ in static_batches),\n",
    "        'Dynamic': sum(batch.size for batch, _ in dynamic_batches),\n",
    "        'Packed': sum(batch.size for batch, _ in packed_batches)\n",
    "    }\n",
    "    \n",
    "    plt.bar(memory_usage.keys(), memory_usage.values(), alpha=0.7)\n",
    "    plt.title('Memory Usage')\n",
    "    plt.ylabel('Total Elements')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Batch count comparison\n",
    "    plt.subplot(3, 3, 9)\n",
    "    batch_counts = [len(static_batches), len(dynamic_batches), len(packed_batches)]\n",
    "    \n",
    "    plt.bar(strategies, batch_counts, alpha=0.7)\n",
    "    plt.title('Number of Batches')\n",
    "    plt.ylabel('Batch Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nBatching Strategy Trade-offs:\")\n",
    "    print(\"\\nStatic Batching:\")\n",
    "    print(\"  + Simple implementation\")\n",
    "    print(\"  + Predictable memory usage\")\n",
    "    print(\"  - High padding overhead\")\n",
    "    \n",
    "    print(\"\\nDynamic Batching:\")\n",
    "    print(\"  + Better padding efficiency\")\n",
    "    print(\"  + Adaptive batch sizes\")\n",
    "    print(\"  - More complex implementation\")\n",
    "    \n",
    "    print(\"\\nSequence Packing:\")\n",
    "    print(\"  + Highest efficiency\")\n",
    "    print(\"  + Minimal padding\")\n",
    "    print(\"  - Requires careful attention masking\")\n",
    "    print(\"  - More complex data loading\")\n",
    "\n",
    "demonstrate_token_batching()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Deployment and Serving\n",
    "\n",
    "## Overview\n",
    "\n",
    "Deploying LLMs in production requires careful consideration of performance, scalability, and reliability. This notebook covers:\n",
    "\n",
    "- **Model Serving**: REST APIs and streaming interfaces\n",
    "- **Load Balancing**: Traffic distribution and scaling strategies\n",
    "- **Performance Monitoring**: Latency, throughput, and error tracking\n",
    "- **Deployment Patterns**: Blue-green, canary, and A/B deployments\n",
    "\n",
    "Let's implement practical deployment and serving solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, deque\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import threading\n",
    "import queue\n",
    "import random\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Serving Infrastructure\n",
    "\n",
    "Let's implement a scalable model serving system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelEndpoint:\n",
    "    endpoint_id: str\n",
    "    model_name: str\n",
    "    version: str\n",
    "    max_concurrent: int\n",
    "    timeout_seconds: int\n",
    "    current_load: int = 0\n",
    "    total_requests: int = 0\n",
    "    successful_requests: int = 0\n",
    "    avg_latency: float = 0.0\n",
    "    status: str = 'healthy'\n",
    "\n",
    "class ModelServingSystem:\n",
    "    \"\"\"Scalable model serving with load balancing and monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.endpoints = {}\n",
    "        self.request_queue = queue.Queue()\n",
    "        self.metrics = {\n",
    "            'total_requests': 0,\n",
    "            'successful_requests': 0,\n",
    "            'failed_requests': 0,\n",
    "            'avg_latency': 0.0,\n",
    "            'requests_per_second': 0.0\n",
    "        }\n",
    "        self.latency_history = deque(maxlen=1000)\n",
    "        self.load_balancer = LoadBalancer()\n",
    "        self.monitoring_active = False\n",
    "    \n",
    "    def register_endpoint(self, endpoint: ModelEndpoint):\n",
    "        \"\"\"Register a new model endpoint\"\"\"\n",
    "        self.endpoints[endpoint.endpoint_id] = endpoint\n",
    "        self.load_balancer.add_endpoint(endpoint)\n",
    "        print(f\"Registered endpoint: {endpoint.endpoint_id} ({endpoint.model_name} v{endpoint.version})\")\n",
    "    \n",
    "    def serve_request(self, request_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Serve a single request\"\"\"\n",
    "        start_time = time.time()\n",
    "        request_id = f\"req_{int(time.time() * 1000000)}\"\n",
    "        \n",
    "        try:\n",
    "            # Select endpoint using load balancer\n",
    "            endpoint = self.load_balancer.select_endpoint(request_data)\n",
    "            \n",
    "            if not endpoint:\n",
    "                return {\n",
    "                    'request_id': request_id,\n",
    "                    'error': 'No available endpoints',\n",
    "                    'status': 'failed'\n",
    "                }\n",
    "            \n",
    "            # Process request\n",
    "            response = self._process_request(endpoint, request_data, request_id)\n",
    "            \n",
    "            # Update metrics\n",
    "            latency = time.time() - start_time\n",
    "            self._update_metrics(endpoint, latency, success=True)\n",
    "            \n",
    "            response.update({\n",
    "                'request_id': request_id,\n",
    "                'endpoint_id': endpoint.endpoint_id,\n",
    "                'latency': latency,\n",
    "                'status': 'success'\n",
    "            })\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            latency = time.time() - start_time\n",
    "            self._update_metrics(None, latency, success=False)\n",
    "            \n",
    "            return {\n",
    "                'request_id': request_id,\n",
    "                'error': str(e),\n",
    "                'latency': latency,\n",
    "                'status': 'failed'\n",
    "            }\n",
    "    \n",
    "    def _process_request(self, endpoint: ModelEndpoint, request_data: Dict, request_id: str) -> Dict:\n",
    "        \"\"\"Process request on specific endpoint\"\"\"\n",
    "        # Simulate model inference\n",
    "        prompt = request_data.get('prompt', '')\n",
    "        max_tokens = request_data.get('max_tokens', 100)\n",
    "        \n",
    "        # Simulate processing time based on request complexity\n",
    "        base_time = 0.1\n",
    "        complexity_factor = len(prompt) / 1000 + max_tokens / 1000\n",
    "        processing_time = base_time + complexity_factor * 0.5\n",
    "        \n",
    "        time.sleep(min(processing_time, 2.0))  # Cap simulation time\n",
    "        \n",
    "        # Generate mock response\n",
    "        response_text = f\"Response from {endpoint.model_name} to: {prompt[:50]}...\"\n",
    "        \n",
    "        return {\n",
    "            'text': response_text,\n",
    "            'model': endpoint.model_name,\n",
    "            'version': endpoint.version,\n",
    "            'tokens_generated': max_tokens,\n",
    "            'processing_time': processing_time\n",
    "        }\n",
    "    \n",
    "    def _update_metrics(self, endpoint: Optional[ModelEndpoint], latency: float, success: bool):\n",
    "        \"\"\"Update system and endpoint metrics\"\"\"\n",
    "        # System metrics\n",
    "        self.metrics['total_requests'] += 1\n",
    "        \n",
    "        if success:\n",
    "            self.metrics['successful_requests'] += 1\n",
    "        else:\n",
    "            self.metrics['failed_requests'] += 1\n",
    "        \n",
    "        # Update latency\n",
    "        self.latency_history.append(latency)\n",
    "        self.metrics['avg_latency'] = np.mean(list(self.latency_history))\n",
    "        \n",
    "        # Endpoint metrics\n",
    "        if endpoint:\n",
    "            endpoint.total_requests += 1\n",
    "            if success:\n",
    "                endpoint.successful_requests += 1\n",
    "            \n",
    "            # Update endpoint average latency\n",
    "            if endpoint.total_requests == 1:\n",
    "                endpoint.avg_latency = latency\n",
    "            else:\n",
    "                endpoint.avg_latency = (\n",
    "                    endpoint.avg_latency * (endpoint.total_requests - 1) + latency\n",
    "                ) / endpoint.total_requests\n",
    "    \n",
    "    def get_system_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive system metrics\"\"\"\n",
    "        metrics = self.metrics.copy()\n",
    "        \n",
    "        if metrics['total_requests'] > 0:\n",
    "            metrics['success_rate'] = metrics['successful_requests'] / metrics['total_requests']\n",
    "            metrics['error_rate'] = metrics['failed_requests'] / metrics['total_requests']\n",
    "        \n",
    "        # Calculate requests per second (last minute)\n",
    "        recent_requests = len([l for l in self.latency_history if l is not None])\n",
    "        metrics['requests_per_second'] = recent_requests / 60.0  # Approximate\n",
    "        \n",
    "        # Endpoint status\n",
    "        metrics['endpoint_status'] = {\n",
    "            eid: {\n",
    "                'status': ep.status,\n",
    "                'load': ep.current_load,\n",
    "                'success_rate': ep.successful_requests / max(ep.total_requests, 1),\n",
    "                'avg_latency': ep.avg_latency\n",
    "            } for eid, ep in self.endpoints.items()\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "class LoadBalancer:\n",
    "    \"\"\"Load balancer for distributing requests across endpoints\"\"\"\n",
    "    \n",
    "    def __init__(self, strategy='round_robin'):\n",
    "        self.strategy = strategy\n",
    "        self.endpoints = []\n",
    "        self.current_index = 0\n",
    "        \n",
    "        self.strategies = {\n",
    "            'round_robin': self._round_robin_select,\n",
    "            'least_loaded': self._least_loaded_select,\n",
    "            'weighted_random': self._weighted_random_select,\n",
    "            'latency_based': self._latency_based_select\n",
    "        }\n",
    "    \n",
    "    def add_endpoint(self, endpoint: ModelEndpoint):\n",
    "        \"\"\"Add endpoint to load balancer\"\"\"\n",
    "        self.endpoints.append(endpoint)\n",
    "    \n",
    "    def select_endpoint(self, request_data: Dict) -> Optional[ModelEndpoint]:\n",
    "        \"\"\"Select best endpoint for request\"\"\"\n",
    "        available_endpoints = [ep for ep in self.endpoints \n",
    "                             if ep.status == 'healthy' and ep.current_load < ep.max_concurrent]\n",
    "        \n",
    "        if not available_endpoints:\n",
    "            return None\n",
    "        \n",
    "        return self.strategies[self.strategy](available_endpoints, request_data)\n",
    "    \n",
    "    def _round_robin_select(self, endpoints: List[ModelEndpoint], request_data: Dict) -> ModelEndpoint:\n",
    "        \"\"\"Round-robin selection\"\"\"\n",
    "        selected = endpoints[self.current_index % len(endpoints)]\n",
    "        self.current_index += 1\n",
    "        return selected\n",
    "    \n",
    "    def _least_loaded_select(self, endpoints: List[ModelEndpoint], request_data: Dict) -> ModelEndpoint:\n",
    "        \"\"\"Select endpoint with least current load\"\"\"\n",
    "        return min(endpoints, key=lambda ep: ep.current_load)\n",
    "    \n",
    "    def _weighted_random_select(self, endpoints: List[ModelEndpoint], request_data: Dict) -> ModelEndpoint:\n",
    "        \"\"\"Weighted random selection based on capacity\"\"\"\n",
    "        weights = [ep.max_concurrent - ep.current_load for ep in endpoints]\n",
    "        total_weight = sum(weights)\n",
    "        \n",
    "        if total_weight == 0:\n",
    "            return random.choice(endpoints)\n",
    "        \n",
    "        r = random.uniform(0, total_weight)\n",
    "        cumulative = 0\n",
    "        \n",
    "        for i, weight in enumerate(weights):\n",
    "            cumulative += weight\n",
    "            if r <= cumulative:\n",
    "                return endpoints[i]\n",
    "        \n",
    "        return endpoints[-1]\n",
    "    \n",
    "    def _latency_based_select(self, endpoints: List[ModelEndpoint], request_data: Dict) -> ModelEndpoint:\n",
    "        \"\"\"Select endpoint with best latency performance\"\"\"\n",
    "        return min(endpoints, key=lambda ep: ep.avg_latency if ep.avg_latency > 0 else float('inf'))\n",
    "\n",
    "print(\"Model serving infrastructure implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model Serving System\n",
    "\n",
    "Let's test our serving system with multiple endpoints and load balancing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize serving system\n",
    "serving_system = ModelServingSystem()\n",
    "\n",
    "# Create test endpoints\n",
    "endpoints = [\n",
    "    ModelEndpoint('ep1', 'gpt-3.5-turbo', '1.0', max_concurrent=10, timeout_seconds=30),\n",
    "    ModelEndpoint('ep2', 'llama-2-7b', '1.0', max_concurrent=8, timeout_seconds=45),\n",
    "    ModelEndpoint('ep3', 'mistral-7b', '0.1', max_concurrent=12, timeout_seconds=25)\n",
    "]\n",
    "\n",
    "for endpoint in endpoints:\n",
    "    serving_system.register_endpoint(endpoint)\n",
    "\n",
    "# Test different load balancing strategies\n",
    "strategies = ['round_robin', 'least_loaded', 'weighted_random', 'latency_based']\n",
    "test_requests = [\n",
    "    {'prompt': 'What is machine learning?', 'max_tokens': 100},\n",
    "    {'prompt': 'Explain neural networks', 'max_tokens': 150},\n",
    "    {'prompt': 'How does AI work?', 'max_tokens': 120},\n",
    "    {'prompt': 'Define deep learning', 'max_tokens': 80},\n",
    "    {'prompt': 'What are transformers?', 'max_tokens': 200}\n",
    "]\n",
    "\n",
    "strategy_results = {}\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"\\nTesting {strategy} strategy...\")\n",
    "    \n",
    "    # Reset endpoint metrics\n",
    "    for endpoint in serving_system.endpoints.values():\n",
    "        endpoint.total_requests = 0\n",
    "        endpoint.successful_requests = 0\n",
    "        endpoint.avg_latency = 0.0\n",
    "    \n",
    "    serving_system.load_balancer.strategy = strategy\n",
    "    serving_system.metrics = {\n",
    "        'total_requests': 0, 'successful_requests': 0, 'failed_requests': 0,\n",
    "        'avg_latency': 0.0, 'requests_per_second': 0.0\n",
    "    }\n",
    "    serving_system.latency_history.clear()\n",
    "    \n",
    "    # Process test requests\n",
    "    responses = []\n",
    "    for request in test_requests:\n",
    "        response = serving_system.serve_request(request)\n",
    "        responses.append(response)\n",
    "        print(f\"  Request processed: {response['status']} (latency: {response.get('latency', 0):.3f}s)\")\n",
    "    \n",
    "    # Get metrics for this strategy\n",
    "    metrics = serving_system.get_system_metrics()\n",
    "    strategy_results[strategy] = {\n",
    "        'success_rate': metrics['success_rate'],\n",
    "        'avg_latency': metrics['avg_latency'],\n",
    "        'endpoint_distribution': {eid: ep['load'] for eid, ep in metrics['endpoint_status'].items()}\n",
    "    }\n",
    "    \n",
    "    print(f\"  Success rate: {metrics['success_rate']:.1%}\")\n",
    "    print(f\"  Average latency: {metrics['avg_latency']:.3f}s\")\n",
    "    print(f\"  Load distribution: {strategy_results[strategy]['endpoint_distribution']}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Success rates by strategy\n",
    "plt.subplot(2, 3, 1)\n",
    "success_rates = [strategy_results[s]['success_rate'] for s in strategies]\n",
    "plt.bar(strategies, success_rates, color='lightgreen')\n",
    "plt.xlabel('Load Balancing Strategy')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.title('Success Rates by Strategy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Average latencies by strategy\n",
    "plt.subplot(2, 3, 2)\n",
    "latencies = [strategy_results[s]['avg_latency'] for s in strategies]\n",
    "plt.bar(strategies, latencies, color='skyblue')\n",
    "plt.xlabel('Load Balancing Strategy')\n",
    "plt.ylabel('Average Latency (s)')\n",
    "plt.title('Latency by Strategy')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Load distribution for round_robin\n",
    "plt.subplot(2, 3, 3)\n",
    "rr_distribution = strategy_results['round_robin']['endpoint_distribution']\n",
    "plt.pie(rr_distribution.values(), labels=rr_distribution.keys(), autopct='%1.1f%%')\n",
    "plt.title('Load Distribution (Round Robin)')\n",
    "\n",
    "# Load distribution for least_loaded\n",
    "plt.subplot(2, 3, 4)\n",
    "ll_distribution = strategy_results['least_loaded']['endpoint_distribution']\n",
    "plt.pie(ll_distribution.values(), labels=ll_distribution.keys(), autopct='%1.1f%%')\n",
    "plt.title('Load Distribution (Least Loaded)')\n",
    "\n",
    "# Strategy comparison radar chart\n",
    "plt.subplot(2, 3, 5)\n",
    "metrics_comparison = np.array([\n",
    "    success_rates,\n",
    "    [1/l for l in latencies]  # Inverse latency (higher is better)\n",
    "])\n",
    "\n",
    "x = np.arange(len(strategies))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, success_rates, width, label='Success Rate', alpha=0.8)\n",
    "plt.bar(x + width/2, [l/max(latencies) for l in latencies], width, label='Latency (normalized)', alpha=0.8)\n",
    "plt.xlabel('Strategy')\n",
    "plt.ylabel('Performance Score')\n",
    "plt.title('Strategy Performance Comparison')\n",
    "plt.xticks(x, strategies, rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# Endpoint performance comparison\n",
    "plt.subplot(2, 3, 6)\n",
    "endpoint_names = [ep.model_name for ep in serving_system.endpoints.values()]\n",
    "endpoint_latencies = [ep.avg_latency for ep in serving_system.endpoints.values()]\n",
    "endpoint_success_rates = [ep.successful_requests / max(ep.total_requests, 1) \n",
    "                         for ep in serving_system.endpoints.values()]\n",
    "\n",
    "plt.scatter(endpoint_latencies, endpoint_success_rates, s=100, alpha=0.7)\n",
    "for i, name in enumerate(endpoint_names):\n",
    "    plt.annotate(name, (endpoint_latencies[i], endpoint_success_rates[i]), \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "plt.xlabel('Average Latency (s)')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.title('Endpoint Performance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== SERVING SYSTEM ANALYSIS ===\")\n",
    "best_strategy = min(strategies, key=lambda s: strategy_results[s]['avg_latency'])\n",
    "print(f\"Best latency strategy: {best_strategy} ({strategy_results[best_strategy]['avg_latency']:.3f}s)\")\n",
    "\n",
    "most_reliable = max(strategies, key=lambda s: strategy_results[s]['success_rate'])\n",
    "print(f\"Most reliable strategy: {most_reliable} ({strategy_results[most_reliable]['success_rate']:.1%})\")\n",
    "\n",
    "print(f\"\\nEndpoint Performance:\")\n",
    "for endpoint in serving_system.endpoints.values():\n",
    "    print(f\"  {endpoint.model_name}: {endpoint.avg_latency:.3f}s avg latency, {endpoint.successful_requests}/{endpoint.total_requests} success\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
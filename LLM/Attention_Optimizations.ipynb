{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Optimizations for LLMs\n",
    "\n",
    "## Overview\n",
    "\n",
    "Attention mechanisms are the computational bottleneck in transformer models. This notebook covers:\n",
    "\n",
    "- **FlashAttention**: Memory-efficient attention computation\n",
    "- **Sparse Attention**: Reducing quadratic complexity with structured sparsity\n",
    "- **Linear Attention**: Approximating attention with linear complexity\n",
    "- **Performance Analysis**: Benchmarking different attention variants\n",
    "\n",
    "Let's implement and compare various attention optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FlashAttention Implementation\n",
    "\n",
    "FlashAttention reduces memory usage by computing attention in blocks and avoiding materialization of the full attention matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashAttention(nn.Module):\n",
    "    \"\"\"Memory-efficient FlashAttention implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, block_size=64):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.block_size = block_size\n",
    "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
    "        \n",
    "        self.qkv_proj = nn.Linear(d_model, 3 * d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        qkv = self.qkv_proj(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # Apply FlashAttention algorithm\n",
    "        out = self.flash_attention_forward(q, k, v, mask)\n",
    "        \n",
    "        # Reshape and project output\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out_proj(out)\n",
    "    \n",
    "    def flash_attention_forward(self, q, k, v, mask=None):\n",
    "        \"\"\"FlashAttention forward pass with block-wise computation\"\"\"\n",
    "        B, H, T, D = q.shape\n",
    "        \n",
    "        # Initialize output and statistics\n",
    "        out = torch.zeros_like(v)\n",
    "        l = torch.zeros(B, H, T, 1, device=q.device)\n",
    "        m = torch.full((B, H, T, 1), -float('inf'), device=q.device)\n",
    "        \n",
    "        # Process in blocks\n",
    "        for i in range(0, T, self.block_size):\n",
    "            end_i = min(i + self.block_size, T)\n",
    "            q_block = q[:, :, i:end_i, :]\n",
    "            \n",
    "            for j in range(0, T, self.block_size):\n",
    "                end_j = min(j + self.block_size, T)\n",
    "                k_block = k[:, :, j:end_j, :]\n",
    "                v_block = v[:, :, j:end_j, :]\n",
    "                \n",
    "                # Compute attention scores for this block\n",
    "                scores = torch.matmul(q_block, k_block.transpose(-2, -1)) * self.scale\n",
    "                \n",
    "                # Apply causal mask if needed\n",
    "                if mask is not None or j >= i:  # Causal attention\n",
    "                    causal_mask = torch.triu(torch.ones(end_i - i, end_j - j), diagonal=j - i + 1)\n",
    "                    scores.masked_fill_(causal_mask.bool().to(scores.device), -float('inf'))\n",
    "                \n",
    "                # Online softmax computation\n",
    "                m_block = torch.max(scores, dim=-1, keepdim=True)[0]\n",
    "                scores_exp = torch.exp(scores - m_block)\n",
    "                l_block = torch.sum(scores_exp, dim=-1, keepdim=True)\n",
    "                \n",
    "                # Update global statistics\n",
    "                m_prev = m[:, :, i:end_i, :]\n",
    "                l_prev = l[:, :, i:end_i, :]\n",
    "                \n",
    "                m_new = torch.max(m_prev, m_block)\n",
    "                alpha = torch.exp(m_prev - m_new)\n",
    "                beta = torch.exp(m_block - m_new)\n",
    "                \n",
    "                l_new = alpha * l_prev + beta * l_block\n",
    "                \n",
    "                # Update output\n",
    "                out_block = torch.matmul(scores_exp, v_block)\n",
    "                out[:, :, i:end_i, :] = (alpha * out[:, :, i:end_i, :] + beta * out_block) / l_new\n",
    "                \n",
    "                # Update statistics\n",
    "                m[:, :, i:end_i, :] = m_new\n",
    "                l[:, :, i:end_i, :] = l_new\n",
    "        \n",
    "        return out\n",
    "\n",
    "class SparseAttention(nn.Module):\n",
    "    \"\"\"Sparse attention with configurable sparsity patterns\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, sparsity_pattern='local', window_size=128):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.sparsity_pattern = sparsity_pattern\n",
    "        self.window_size = window_size\n",
    "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
    "        \n",
    "        self.qkv_proj = nn.Linear(d_model, 3 * d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        qkv = self.qkv_proj(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        q = q.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # Create sparsity mask\n",
    "        mask = self.create_sparsity_mask(T)\n",
    "        \n",
    "        # Compute sparse attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        scores.masked_fill_(~mask, -float('inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out_proj(out)\n",
    "    \n",
    "    def create_sparsity_mask(self, seq_len):\n",
    "        \"\"\"Create sparsity mask based on pattern\"\"\"\n",
    "        if self.sparsity_pattern == 'local':\n",
    "            return self.local_mask(seq_len)\n",
    "        elif self.sparsity_pattern == 'strided':\n",
    "            return self.strided_mask(seq_len)\n",
    "        elif self.sparsity_pattern == 'random':\n",
    "            return self.random_mask(seq_len)\n",
    "        else:\n",
    "            return torch.ones(seq_len, seq_len, dtype=torch.bool)\n",
    "    \n",
    "    def local_mask(self, seq_len):\n",
    "        \"\"\"Local attention within window\"\"\"\n",
    "        mask = torch.zeros(seq_len, seq_len, dtype=torch.bool)\n",
    "        for i in range(seq_len):\n",
    "            start = max(0, i - self.window_size // 2)\n",
    "            end = min(seq_len, i + self.window_size // 2 + 1)\n",
    "            mask[i, start:end] = True\n",
    "        return mask\n",
    "    \n",
    "    def strided_mask(self, seq_len):\n",
    "        \"\"\"Strided attention pattern\"\"\"\n",
    "        mask = torch.zeros(seq_len, seq_len, dtype=torch.bool)\n",
    "        stride = self.window_size // 4\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            # Local attention\n",
    "            local_start = max(0, i - self.window_size // 2)\n",
    "            local_end = min(seq_len, i + self.window_size // 2 + 1)\n",
    "            mask[i, local_start:local_end] = True\n",
    "            \n",
    "            # Strided attention\n",
    "            for j in range(0, i, stride):\n",
    "                if j < seq_len:\n",
    "                    mask[i, j] = True\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def random_mask(self, seq_len, sparsity=0.1):\n",
    "        \"\"\"Random sparsity pattern\"\"\"\n",
    "        mask = torch.rand(seq_len, seq_len) < sparsity\n",
    "        # Ensure causal mask\n",
    "        causal = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool))\n",
    "        return mask & causal\n",
    "\n",
    "print(\"Attention optimization modules implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Attention Optimizations\n",
    "\n",
    "Let's benchmark different attention mechanisms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_attention(attention_module, seq_lengths, d_model=512, n_heads=8, batch_size=4):\n",
    "    \"\"\"Benchmark attention module across different sequence lengths\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        # Create random input\n",
    "        x = torch.randn(batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(3):\n",
    "            _ = attention_module(x)\n",
    "        \n",
    "        # Benchmark\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(10):\n",
    "            output = attention_module(x)\n",
    "        \n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        end_time = time.time()\n",
    "        \n",
    "        avg_time = (end_time - start_time) / 10\n",
    "        \n",
    "        results.append({\n",
    "            'seq_len': seq_len,\n",
    "            'time': avg_time,\n",
    "            'memory': torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "        })\n",
    "        \n",
    "        print(f\"Seq len {seq_len}: {avg_time:.4f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test different attention mechanisms\n",
    "d_model, n_heads = 512, 8\n",
    "seq_lengths = [128, 256, 512, 1024]\n",
    "\n",
    "print(\"Benchmarking Standard Attention...\")\n",
    "standard_attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
    "standard_results = []\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    x = torch.randn(4, seq_len, d_model)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for _ in range(10):\n",
    "        output, _ = standard_attn(x, x, x)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / 10\n",
    "    standard_results.append({'seq_len': seq_len, 'time': avg_time})\n",
    "    print(f\"Seq len {seq_len}: {avg_time:.4f}s\")\n",
    "\n",
    "print(\"\\nBenchmarking FlashAttention...\")\n",
    "flash_attn = FlashAttention(d_model, n_heads, block_size=64)\n",
    "flash_results = benchmark_attention(flash_attn, seq_lengths)\n",
    "\n",
    "print(\"\\nBenchmarking Sparse Attention (Local)...\")\n",
    "sparse_attn_local = SparseAttention(d_model, n_heads, 'local', window_size=64)\n",
    "sparse_local_results = benchmark_attention(sparse_attn_local, seq_lengths)\n",
    "\n",
    "print(\"\\nBenchmarking Sparse Attention (Strided)...\")\n",
    "sparse_attn_strided = SparseAttention(d_model, n_heads, 'strided', window_size=64)\n",
    "sparse_strided_results = benchmark_attention(sparse_attn_strided, seq_lengths)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Performance comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot([r['seq_len'] for r in standard_results], [r['time'] for r in standard_results], 'o-', label='Standard', linewidth=2)\n",
    "plt.plot([r['seq_len'] for r in flash_results], [r['time'] for r in flash_results], 's-', label='FlashAttention', linewidth=2)\n",
    "plt.plot([r['seq_len'] for r in sparse_local_results], [r['time'] for r in sparse_local_results], '^-', label='Sparse (Local)', linewidth=2)\n",
    "plt.plot([r['seq_len'] for r in sparse_strided_results], [r['time'] for r in sparse_strided_results], 'd-', label='Sparse (Strided)', linewidth=2)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Attention Performance Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Complexity analysis\n",
    "plt.subplot(2, 2, 2)\n",
    "seq_lens = np.array([r['seq_len'] for r in standard_results])\n",
    "standard_times = np.array([r['time'] for r in standard_results])\n",
    "flash_times = np.array([r['time'] for r in flash_results])\n",
    "\n",
    "# Fit quadratic and linear curves\n",
    "quadratic_fit = np.polyfit(seq_lens, standard_times, 2)\n",
    "linear_fit = np.polyfit(seq_lens, flash_times, 1)\n",
    "\n",
    "x_smooth = np.linspace(seq_lens.min(), seq_lens.max(), 100)\n",
    "plt.plot(seq_lens, standard_times, 'o', label='Standard (Measured)', markersize=8)\n",
    "plt.plot(x_smooth, np.polyval(quadratic_fit, x_smooth), '--', label='O(nÂ²) Fit', alpha=0.7)\n",
    "plt.plot(seq_lens, flash_times, 's', label='FlashAttention (Measured)', markersize=8)\n",
    "plt.plot(x_smooth, np.polyval(linear_fit, x_smooth), '--', label='O(n) Fit', alpha=0.7)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Complexity Analysis')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Speedup analysis\n",
    "plt.subplot(2, 2, 3)\n",
    "flash_speedup = [s['time'] / f['time'] for s, f in zip(standard_results, flash_results)]\n",
    "sparse_local_speedup = [s['time'] / sp['time'] for s, sp in zip(standard_results, sparse_local_results)]\n",
    "sparse_strided_speedup = [s['time'] / sp['time'] for s, sp in zip(standard_results, sparse_strided_results)]\n",
    "\n",
    "plt.bar(np.arange(len(seq_lengths)) - 0.2, flash_speedup, 0.2, label='FlashAttention', alpha=0.8)\n",
    "plt.bar(np.arange(len(seq_lengths)), sparse_local_speedup, 0.2, label='Sparse (Local)', alpha=0.8)\n",
    "plt.bar(np.arange(len(seq_lengths)) + 0.2, sparse_strided_speedup, 0.2, label='Sparse (Strided)', alpha=0.8)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Speedup vs Standard')\n",
    "plt.title('Speedup Comparison')\n",
    "plt.xticks(range(len(seq_lengths)), seq_lengths)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Attention pattern visualization\n",
    "plt.subplot(2, 2, 4)\n",
    "seq_len = 64\n",
    "local_mask = sparse_attn_local.create_sparsity_mask(seq_len).float().numpy()\n",
    "plt.imshow(local_mask, cmap='Blues', aspect='auto')\n",
    "plt.title('Local Sparse Attention Pattern')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== PERFORMANCE SUMMARY ===\")\n",
    "print(f\"At sequence length 1024:\")\n",
    "print(f\"  Standard Attention: {standard_results[-1]['time']:.4f}s\")\n",
    "print(f\"  FlashAttention: {flash_results[-1]['time']:.4f}s ({standard_results[-1]['time']/flash_results[-1]['time']:.1f}x speedup)\")\n",
    "print(f\"  Sparse Local: {sparse_local_results[-1]['time']:.4f}s ({standard_results[-1]['time']/sparse_local_results[-1]['time']:.1f}x speedup)\")\n",
    "print(f\"  Sparse Strided: {sparse_strided_results[-1]['time']:.4f}s ({standard_results[-1]['time']/sparse_strided_results[-1]['time']:.1f}x speedup)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
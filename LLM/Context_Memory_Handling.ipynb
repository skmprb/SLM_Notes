{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context and Memory Handling for LLMs\n",
    "\n",
    "## Overview\n",
    "\n",
    "Efficient context and memory management is essential for long conversations and complex tasks. This notebook covers:\n",
    "\n",
    "- **Context Window Management**: Sliding windows and context compression\n",
    "- **Memory Systems**: Short-term and long-term memory mechanisms\n",
    "- **Context Retrieval**: Relevant context selection and ranking\n",
    "- **Memory Optimization**: Efficient storage and retrieval strategies\n",
    "\n",
    "Let's implement practical context and memory management systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, defaultdict\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import heapq\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Context Window Management\n",
    "\n",
    "Efficient management of context windows for long conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ContextItem:\n",
    "    content: str\n",
    "    timestamp: datetime\n",
    "    importance: float\n",
    "    token_count: int\n",
    "    item_type: str  # 'user', 'assistant', 'system'\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "class ContextWindowManager:\n",
    "    \"\"\"Manages context windows with intelligent truncation and compression\"\"\"\n",
    "    \n",
    "    def __init__(self, max_tokens=4096, compression_ratio=0.7):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.compression_ratio = compression_ratio\n",
    "        self.context_items = deque()\n",
    "        self.total_tokens = 0\n",
    "        \n",
    "        # Importance scoring weights\n",
    "        self.importance_weights = {\n",
    "            'recency': 0.3,\n",
    "            'relevance': 0.4,\n",
    "            'user_interaction': 0.2,\n",
    "            'system_importance': 0.1\n",
    "        }\n",
    "    \n",
    "    def add_context(self, content: str, item_type: str, importance: float = 0.5, metadata: Dict = None):\n",
    "        \"\"\"Add new context item\"\"\"\n",
    "        token_count = self.estimate_tokens(content)\n",
    "        \n",
    "        item = ContextItem(\n",
    "            content=content,\n",
    "            timestamp=datetime.now(),\n",
    "            importance=importance,\n",
    "            token_count=token_count,\n",
    "            item_type=item_type,\n",
    "            metadata=metadata or {}\n",
    "        )\n",
    "        \n",
    "        self.context_items.append(item)\n",
    "        self.total_tokens += token_count\n",
    "        \n",
    "        # Manage context size\n",
    "        self._manage_context_size()\n",
    "    \n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate token count (simplified)\"\"\"\n",
    "        return len(text.split()) * 1.3  # Rough approximation\n",
    "    \n",
    "    def _manage_context_size(self):\n",
    "        \"\"\"Manage context size through intelligent truncation\"\"\"\n",
    "        if self.total_tokens <= self.max_tokens:\n",
    "            return\n",
    "        \n",
    "        # Calculate target size after compression\n",
    "        target_tokens = int(self.max_tokens * self.compression_ratio)\n",
    "        \n",
    "        # Score all items for retention\n",
    "        scored_items = []\n",
    "        for item in self.context_items:\n",
    "            score = self._calculate_retention_score(item)\n",
    "            scored_items.append((score, item))\n",
    "        \n",
    "        # Sort by score (descending)\n",
    "        scored_items.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Select items to keep\n",
    "        kept_items = []\n",
    "        kept_tokens = 0\n",
    "        \n",
    "        for score, item in scored_items:\n",
    "            if kept_tokens + item.token_count <= target_tokens:\n",
    "                kept_items.append(item)\n",
    "                kept_tokens += item.token_count\n",
    "            elif len(kept_items) == 0:  # Always keep at least one item\n",
    "                kept_items.append(item)\n",
    "                kept_tokens += item.token_count\n",
    "                break\n",
    "        \n",
    "        # Update context\n",
    "        self.context_items = deque(sorted(kept_items, key=lambda x: x.timestamp))\n",
    "        self.total_tokens = kept_tokens\n",
    "    \n",
    "    def _calculate_retention_score(self, item: ContextItem) -> float:\n",
    "        \"\"\"Calculate retention score for context item\"\"\"\n",
    "        now = datetime.now()\n",
    "        \n",
    "        # Recency score (exponential decay)\n",
    "        time_diff = (now - item.timestamp).total_seconds() / 3600  # Hours\n",
    "        recency_score = np.exp(-time_diff / 24)  # Decay over 24 hours\n",
    "        \n",
    "        # Type-based importance\n",
    "        type_importance = {\n",
    "            'system': 0.9,\n",
    "            'user': 0.8,\n",
    "            'assistant': 0.6\n",
    "        }\n",
    "        \n",
    "        # Calculate weighted score\n",
    "        score = (\n",
    "            self.importance_weights['recency'] * recency_score +\n",
    "            self.importance_weights['relevance'] * item.importance +\n",
    "            self.importance_weights['user_interaction'] * type_importance.get(item.item_type, 0.5) +\n",
    "            self.importance_weights['system_importance'] * (1.0 if item.item_type == 'system' else 0.0)\n",
    "        )\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def get_context_window(self) -> str:\n",
    "        \"\"\"Get current context window as string\"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        for item in self.context_items:\n",
    "            prefix = f\"[{item.item_type.upper()}]\"\n",
    "            context_parts.append(f\"{prefix} {item.content}\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def get_context_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of current context\"\"\"\n",
    "        type_counts = defaultdict(int)\n",
    "        type_tokens = defaultdict(int)\n",
    "        \n",
    "        for item in self.context_items:\n",
    "            type_counts[item.item_type] += 1\n",
    "            type_tokens[item.item_type] += item.token_count\n",
    "        \n",
    "        return {\n",
    "            'total_items': len(self.context_items),\n",
    "            'total_tokens': self.total_tokens,\n",
    "            'utilization': self.total_tokens / self.max_tokens,\n",
    "            'type_distribution': dict(type_counts),\n",
    "            'token_distribution': dict(type_tokens),\n",
    "            'oldest_item': self.context_items[0].timestamp if self.context_items else None,\n",
    "            'newest_item': self.context_items[-1].timestamp if self.context_items else None\n",
    "        }\n",
    "\n",
    "class MemorySystem:\n",
    "    \"\"\"Multi-tier memory system for LLM conversations\"\"\"\n",
    "    \n",
    "    def __init__(self, short_term_size=50, long_term_size=1000):\n",
    "        self.short_term_memory = deque(maxlen=short_term_size)\n",
    "        self.long_term_memory = []\n",
    "        self.long_term_size = long_term_size\n",
    "        \n",
    "        # Memory indices for fast retrieval\n",
    "        self.keyword_index = defaultdict(list)\n",
    "        self.temporal_index = defaultdict(list)\n",
    "        \n",
    "        # Memory statistics\n",
    "        self.access_counts = defaultdict(int)\n",
    "        self.last_access = defaultdict(datetime)\n",
    "    \n",
    "    def store_memory(self, content: str, memory_type: str, keywords: List[str] = None, importance: float = 0.5):\n",
    "        \"\"\"Store new memory\"\"\"\n",
    "        memory_id = len(self.short_term_memory) + len(self.long_term_memory)\n",
    "        \n",
    "        memory_item = {\n",
    "            'id': memory_id,\n",
    "            'content': content,\n",
    "            'type': memory_type,\n",
    "            'keywords': keywords or self.extract_keywords(content),\n",
    "            'importance': importance,\n",
    "            'timestamp': datetime.now(),\n",
    "            'access_count': 0,\n",
    "            'last_accessed': datetime.now()\n",
    "        }\n",
    "        \n",
    "        # Add to short-term memory\n",
    "        self.short_term_memory.append(memory_item)\n",
    "        \n",
    "        # Update indices\n",
    "        self._update_indices(memory_item)\n",
    "        \n",
    "        # Consolidate to long-term if needed\n",
    "        self._consolidate_memory()\n",
    "        \n",
    "        return memory_id\n",
    "    \n",
    "    def extract_keywords(self, content: str) -> List[str]:\n",
    "        \"\"\"Extract keywords from content (simplified)\"\"\"\n",
    "        # Simple keyword extraction\n",
    "        words = content.lower().split()\n",
    "        # Filter out common words and keep meaningful terms\n",
    "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}\n",
    "        keywords = [word for word in words if len(word) > 3 and word not in stop_words]\n",
    "        return list(set(keywords))[:10]  # Limit to 10 keywords\n",
    "    \n",
    "    def _update_indices(self, memory_item):\n",
    "        \"\"\"Update memory indices\"\"\"\n",
    "        memory_id = memory_item['id']\n",
    "        \n",
    "        # Keyword index\n",
    "        for keyword in memory_item['keywords']:\n",
    "            self.keyword_index[keyword].append(memory_id)\n",
    "        \n",
    "        # Temporal index (by day)\n",
    "        day_key = memory_item['timestamp'].strftime('%Y-%m-%d')\n",
    "        self.temporal_index[day_key].append(memory_id)\n",
    "    \n",
    "    def _consolidate_memory(self):\n",
    "        \"\"\"Move important short-term memories to long-term storage\"\"\"\n",
    "        if len(self.short_term_memory) < self.short_term_memory.maxlen:\n",
    "            return\n",
    "        \n",
    "        # Score memories for long-term storage\n",
    "        candidates = list(self.short_term_memory)\n",
    "        scored_candidates = []\n",
    "        \n",
    "        for memory in candidates:\n",
    "            score = self._calculate_consolidation_score(memory)\n",
    "            scored_candidates.append((score, memory))\n",
    "        \n",
    "        # Sort by score and select top candidates\n",
    "        scored_candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Move top 20% to long-term memory\n",
    "        consolidation_count = max(1, len(scored_candidates) // 5)\n",
    "        \n",
    "        for i in range(consolidation_count):\n",
    "            _, memory = scored_candidates[i]\n",
    "            self.long_term_memory.append(memory)\n",
    "        \n",
    "        # Manage long-term memory size\n",
    "        if len(self.long_term_memory) > self.long_term_size:\n",
    "            # Remove least important memories\n",
    "            self.long_term_memory.sort(key=lambda x: self._calculate_consolidation_score(x), reverse=True)\n",
    "            self.long_term_memory = self.long_term_memory[:self.long_term_size]\n",
    "    \n",
    "    def _calculate_consolidation_score(self, memory):\n",
    "        \"\"\"Calculate score for memory consolidation\"\"\"\n",
    "        # Factors: importance, access frequency, recency\n",
    "        importance_score = memory['importance']\n",
    "        access_score = min(memory['access_count'] / 10.0, 1.0)  # Normalize access count\n",
    "        \n",
    "        # Recency score (recent memories get slight boost)\n",
    "        hours_old = (datetime.now() - memory['timestamp']).total_seconds() / 3600\n",
    "        recency_score = np.exp(-hours_old / 168)  # Decay over a week\n",
    "        \n",
    "        return importance_score * 0.5 + access_score * 0.3 + recency_score * 0.2\n",
    "    \n",
    "    def retrieve_memories(self, query: str, max_results: int = 5) -> List[Dict]:\n",
    "        \"\"\"Retrieve relevant memories based on query\"\"\"\n",
    "        query_keywords = self.extract_keywords(query)\n",
    "        \n",
    "        # Search in both short-term and long-term memory\n",
    "        all_memories = list(self.short_term_memory) + self.long_term_memory\n",
    "        \n",
    "        # Score memories for relevance\n",
    "        scored_memories = []\n",
    "        \n",
    "        for memory in all_memories:\n",
    "            relevance_score = self._calculate_relevance_score(memory, query_keywords)\n",
    "            if relevance_score > 0:\n",
    "                scored_memories.append((relevance_score, memory))\n",
    "        \n",
    "        # Sort by relevance and return top results\n",
    "        scored_memories.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for i in range(min(max_results, len(scored_memories))):\n",
    "            score, memory = scored_memories[i]\n",
    "            \n",
    "            # Update access statistics\n",
    "            memory['access_count'] += 1\n",
    "            memory['last_accessed'] = datetime.now()\n",
    "            \n",
    "            results.append({\n",
    "                'memory': memory,\n",
    "                'relevance_score': score\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _calculate_relevance_score(self, memory, query_keywords):\n",
    "        \"\"\"Calculate relevance score between memory and query\"\"\"\n",
    "        memory_keywords = set(memory['keywords'])\n",
    "        query_keyword_set = set(query_keywords)\n",
    "        \n",
    "        # Keyword overlap\n",
    "        overlap = len(memory_keywords.intersection(query_keyword_set))\n",
    "        if overlap == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Jaccard similarity\n",
    "        jaccard = overlap / len(memory_keywords.union(query_keyword_set))\n",
    "        \n",
    "        # Boost score based on memory importance and recency\n",
    "        importance_boost = memory['importance']\n",
    "        access_boost = min(memory['access_count'] / 5.0, 1.0)\n",
    "        \n",
    "        return jaccard * (1 + importance_boost + access_boost)\n",
    "    \n",
    "    def get_memory_statistics(self):\n",
    "        \"\"\"Get memory system statistics\"\"\"\n",
    "        return {\n",
    "            'short_term_count': len(self.short_term_memory),\n",
    "            'long_term_count': len(self.long_term_memory),\n",
    "            'total_memories': len(self.short_term_memory) + len(self.long_term_memory),\n",
    "            'keyword_index_size': len(self.keyword_index),\n",
    "            'temporal_index_size': len(self.temporal_index),\n",
    "            'most_accessed_keywords': self._get_top_keywords(10)\n",
    "        }\n",
    "    \n",
    "    def _get_top_keywords(self, top_k):\n",
    "        \"\"\"Get most frequently occurring keywords\"\"\"\n",
    "        keyword_counts = {k: len(v) for k, v in self.keyword_index.items()}\n",
    "        sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_keywords[:top_k]\n",
    "\n",
    "print(\"Context and memory management systems implemented!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
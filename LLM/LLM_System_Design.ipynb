{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM System Design - Practical Implementation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates practical implementations of LLM system design patterns including:\n",
    "\n",
    "- **End-to-end pipelines**: Complete request processing workflows\n",
    "- **Caching strategies**: Multi-level and semantic caching\n",
    "- **Cost optimization**: Resource allocation and budget management\n",
    "- **Orchestration**: Workflow and multi-agent coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, deque\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. End-to-End LLM Pipeline\n",
    "\n",
    "Let's implement a complete LLM processing pipeline with validation, caching, and monitoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMPipeline:\n",
    "    \"\"\"Complete LLM processing pipeline with caching and monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.cache = {}\n",
    "        self.metrics = {\n",
    "            'total_requests': 0,\n",
    "            'cache_hits': 0,\n",
    "            'cache_misses': 0,\n",
    "            'total_inference_time': 0,\n",
    "            'errors': 0\n",
    "        }\n",
    "        self.request_history = deque(maxlen=1000)\n",
    "    \n",
    "    def process_request(self, request):\n",
    "        \"\"\"Process a complete LLM request through the pipeline\"\"\"\n",
    "        request_id = self.generate_request_id()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # 1. Request validation\n",
    "            validated_request = self.validate_request(request)\n",
    "            \n",
    "            # 2. Preprocessing\n",
    "            processed_input = self.preprocess_input(validated_request)\n",
    "            \n",
    "            # 3. Cache lookup\n",
    "            cache_key = self.generate_cache_key(processed_input)\n",
    "            cached_response = self.cache.get(cache_key)\n",
    "            \n",
    "            if cached_response:\n",
    "                self.metrics['cache_hits'] += 1\n",
    "                response = self.format_response(cached_response, from_cache=True)\n",
    "            else:\n",
    "                self.metrics['cache_misses'] += 1\n",
    "                \n",
    "                # 4. Model inference\n",
    "                inference_start = time.time()\n",
    "                model_output = self.simulate_model_inference(processed_input)\n",
    "                inference_time = time.time() - inference_start\n",
    "                \n",
    "                self.metrics['total_inference_time'] += inference_time\n",
    "                \n",
    "                # 5. Post-processing\n",
    "                processed_output = self.postprocess_output(model_output)\n",
    "                \n",
    "                # 6. Cache storage\n",
    "                self.cache[cache_key] = processed_output\n",
    "                \n",
    "                # 7. Response formatting\n",
    "                response = self.format_response(processed_output, inference_time=inference_time)\n",
    "            \n",
    "            # 8. Logging and monitoring\n",
    "            total_time = time.time() - start_time\n",
    "            self.log_request(request_id, request, response, total_time)\n",
    "            \n",
    "            self.metrics['total_requests'] += 1\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.metrics['errors'] += 1\n",
    "            return self.handle_error(e, request_id)\n",
    "    \n",
    "    def validate_request(self, request):\n",
    "        \"\"\"Validate incoming request\"\"\"\n",
    "        required_fields = ['prompt']\n",
    "        \n",
    "        for field in required_fields:\n",
    "            if field not in request:\n",
    "                raise ValueError(f\"Missing required field: {field}\")\n",
    "        \n",
    "        # Validate prompt length\n",
    "        max_length = self.config.get('max_prompt_length', 4000)\n",
    "        if len(request['prompt']) > max_length:\n",
    "            raise ValueError(f\"Prompt exceeds maximum length of {max_length}\")\n",
    "        \n",
    "        # Set defaults\n",
    "        validated = request.copy()\n",
    "        validated.setdefault('max_tokens', 100)\n",
    "        validated.setdefault('temperature', 0.7)\n",
    "        validated.setdefault('top_p', 0.9)\n",
    "        \n",
    "        return validated\n",
    "    \n",
    "    def preprocess_input(self, request):\n",
    "        \"\"\"Preprocess input for model\"\"\"\n",
    "        processed = {\n",
    "            'prompt': request['prompt'].strip(),\n",
    "            'max_tokens': min(request['max_tokens'], 512),  # Cap max tokens\n",
    "            'temperature': max(0.0, min(2.0, request['temperature'])),  # Clamp temperature\n",
    "            'top_p': max(0.0, min(1.0, request['top_p']))  # Clamp top_p\n",
    "        }\n",
    "        \n",
    "        # Add preprocessing timestamp\n",
    "        processed['processed_at'] = datetime.now().isoformat()\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def generate_cache_key(self, processed_input):\n",
    "        \"\"\"Generate cache key for processed input\"\"\"\n",
    "        key_components = [\n",
    "            processed_input['prompt'],\n",
    "            str(processed_input['temperature']),\n",
    "            str(processed_input['max_tokens']),\n",
    "            str(processed_input['top_p'])\n",
    "        ]\n",
    "        \n",
    "        key_string = '|'.join(key_components)\n",
    "        return hashlib.md5(key_string.encode()).hexdigest()\n",
    "    \n",
    "    def simulate_model_inference(self, processed_input):\n",
    "        \"\"\"Simulate model inference (replace with actual model call)\"\"\"\n",
    "        # Simulate inference time based on prompt length\n",
    "        prompt_length = len(processed_input['prompt'])\n",
    "        base_time = 0.1  # Base inference time\n",
    "        length_factor = prompt_length / 1000  # Additional time per 1000 chars\n",
    "        \n",
    "        inference_time = base_time + length_factor\n",
    "        time.sleep(min(inference_time, 2.0))  # Cap simulation time\n",
    "        \n",
    "        # Generate mock response\n",
    "        response_templates = [\n",
    "            \"This is a helpful response to your query about {topic}.\",\n",
    "            \"Based on your question, I can provide the following information: {info}.\",\n",
    "            \"Here's what I understand about {topic}: {explanation}.\"\n",
    "        ]\n",
    "        \n",
    "        template = np.random.choice(response_templates)\n",
    "        mock_response = template.format(\n",
    "            topic=\"the requested topic\",\n",
    "            info=\"relevant details\",\n",
    "            explanation=\"a comprehensive explanation\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'text': mock_response,\n",
    "            'tokens_generated': len(mock_response.split()),\n",
    "            'finish_reason': 'completed'\n",
    "        }\n",
    "    \n",
    "    def postprocess_output(self, model_output):\n",
    "        \"\"\"Post-process model output\"\"\"\n",
    "        processed = model_output.copy()\n",
    "        \n",
    "        # Clean up text\n",
    "        processed['text'] = processed['text'].strip()\n",
    "        \n",
    "        # Add metadata\n",
    "        processed['processed_at'] = datetime.now().isoformat()\n",
    "        processed['word_count'] = len(processed['text'].split())\n",
    "        processed['character_count'] = len(processed['text'])\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def format_response(self, output, from_cache=False, inference_time=None):\n",
    "        \"\"\"Format final response\"\"\"\n",
    "        response = {\n",
    "            'text': output['text'],\n",
    "            'metadata': {\n",
    "                'tokens_generated': output.get('tokens_generated', 0),\n",
    "                'word_count': output.get('word_count', 0),\n",
    "                'character_count': output.get('character_count', 0),\n",
    "                'from_cache': from_cache,\n",
    "                'inference_time': inference_time,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def generate_request_id(self):\n",
    "        \"\"\"Generate unique request ID\"\"\"\n",
    "        timestamp = str(int(time.time() * 1000000))\n",
    "        return f\"req_{timestamp}\"\n",
    "    \n",
    "    def log_request(self, request_id, request, response, total_time):\n",
    "        \"\"\"Log request for monitoring\"\"\"\n",
    "        log_entry = {\n",
    "            'request_id': request_id,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'prompt_length': len(request['prompt']),\n",
    "            'response_length': len(response['text']),\n",
    "            'total_time': total_time,\n",
    "            'from_cache': response['metadata']['from_cache']\n",
    "        }\n",
    "        \n",
    "        self.request_history.append(log_entry)\n",
    "    \n",
    "    def handle_error(self, error, request_id):\n",
    "        \"\"\"Handle pipeline errors\"\"\"\n",
    "        error_response = {\n",
    "            'error': True,\n",
    "            'message': str(error),\n",
    "            'request_id': request_id,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return error_response\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"Get pipeline performance metrics\"\"\"\n",
    "        metrics = self.metrics.copy()\n",
    "        \n",
    "        if metrics['total_requests'] > 0:\n",
    "            metrics['cache_hit_rate'] = metrics['cache_hits'] / metrics['total_requests']\n",
    "            metrics['error_rate'] = metrics['errors'] / metrics['total_requests']\n",
    "            \n",
    "        if metrics['cache_misses'] > 0:\n",
    "            metrics['avg_inference_time'] = metrics['total_inference_time'] / metrics['cache_misses']\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline_config = {\n",
    "    'max_prompt_length': 4000,\n",
    "    'cache_ttl': 3600  # 1 hour\n",
    "}\n",
    "\n",
    "pipeline = LLMPipeline(pipeline_config)\n",
    "print(\"LLM Pipeline initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Pipeline\n",
    "\n",
    "Let's test our pipeline with various requests and observe caching behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test requests\n",
    "test_requests = [\n",
    "    {'prompt': 'What is machine learning?', 'max_tokens': 100},\n",
    "    {'prompt': 'Explain neural networks', 'max_tokens': 150},\n",
    "    {'prompt': 'What is machine learning?', 'max_tokens': 100},  # Duplicate for cache test\n",
    "    {'prompt': 'How does deep learning work?', 'max_tokens': 200},\n",
    "    {'prompt': 'What is machine learning?', 'max_tokens': 100},  # Another duplicate\n",
    "]\n",
    "\n",
    "print(\"Processing test requests...\\n\")\n",
    "\n",
    "responses = []\n",
    "for i, request in enumerate(test_requests, 1):\n",
    "    print(f\"Request {i}: {request['prompt'][:50]}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = pipeline.process_request(request)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    responses.append(response)\n",
    "    \n",
    "    if 'error' not in response:\n",
    "        print(f\"  Response: {response['text'][:100]}...\")\n",
    "        print(f\"  From cache: {response['metadata']['from_cache']}\")\n",
    "        print(f\"  Total time: {end_time - start_time:.3f}s\")\n",
    "    else:\n",
    "        print(f\"  Error: {response['message']}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Display pipeline metrics\n",
    "metrics = pipeline.get_metrics()\n",
    "print(\"=== PIPELINE METRICS ===\")\n",
    "for key, value in metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Level Caching System\n",
    "\n",
    "Let's implement a sophisticated caching system with multiple levels and semantic similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLevelCache:\n",
    "    \"\"\"Multi-level caching system with L1 (memory), L2 (disk simulation), L3 (database simulation)\"\"\"\n",
    "    \n",
    "    def __init__(self, l1_size=100, l2_size=500, l3_size=2000):\n",
    "        # L1: In-memory cache (fastest, smallest)\n",
    "        self.l1_cache = {}\n",
    "        self.l1_access_order = deque(maxlen=l1_size)\n",
    "        self.l1_size = l1_size\n",
    "        \n",
    "        # L2: Simulated disk cache (medium speed, medium size)\n",
    "        self.l2_cache = {}\n",
    "        self.l2_access_order = deque(maxlen=l2_size)\n",
    "        self.l2_size = l2_size\n",
    "        \n",
    "        # L3: Simulated database cache (slowest, largest)\n",
    "        self.l3_cache = {}\n",
    "        self.l3_access_order = deque(maxlen=l3_size)\n",
    "        self.l3_size = l3_size\n",
    "        \n",
    "        # Cache statistics\n",
    "        self.stats = {\n",
    "            'l1_hits': 0, 'l1_misses': 0,\n",
    "            'l2_hits': 0, 'l2_misses': 0,\n",
    "            'l3_hits': 0, 'l3_misses': 0,\n",
    "            'total_requests': 0\n",
    "        }\n",
    "    \n",
    "    def get(self, key):\n",
    "        \"\"\"Get value from multi-level cache\"\"\"\n",
    "        self.stats['total_requests'] += 1\n",
    "        \n",
    "        # Try L1 cache first\n",
    "        if key in self.l1_cache:\n",
    "            self.stats['l1_hits'] += 1\n",
    "            self._update_access_order(key, 1)\n",
    "            return self.l1_cache[key]\n",
    "        \n",
    "        self.stats['l1_misses'] += 1\n",
    "        \n",
    "        # Try L2 cache\n",
    "        if key in self.l2_cache:\n",
    "            self.stats['l2_hits'] += 1\n",
    "            value = self.l2_cache[key]\n",
    "            # Promote to L1\n",
    "            self._promote_to_l1(key, value)\n",
    "            time.sleep(0.01)  # Simulate L2 access time\n",
    "            return value\n",
    "        \n",
    "        self.stats['l2_misses'] += 1\n",
    "        \n",
    "        # Try L3 cache\n",
    "        if key in self.l3_cache:\n",
    "            self.stats['l3_hits'] += 1\n",
    "            value = self.l3_cache[key]\n",
    "            # Promote to L2 and L1\n",
    "            self._promote_to_l2(key, value)\n",
    "            self._promote_to_l1(key, value)\n",
    "            time.sleep(0.05)  # Simulate L3 access time\n",
    "            return value\n",
    "        \n",
    "        self.stats['l3_misses'] += 1\n",
    "        return None\n",
    "    \n",
    "    def set(self, key, value, ttl=None):\n",
    "        \"\"\"Set value in all cache levels\"\"\"\n",
    "        # Add TTL if specified\n",
    "        if ttl:\n",
    "            expiry_time = time.time() + ttl\n",
    "            cache_value = {'value': value, 'expires_at': expiry_time}\n",
    "        else:\n",
    "            cache_value = {'value': value, 'expires_at': None}\n",
    "        \n",
    "        # Store in all levels\n",
    "        self._set_l1(key, cache_value)\n",
    "        self._set_l2(key, cache_value)\n",
    "        self._set_l3(key, cache_value)\n",
    "    \n",
    "    def _set_l1(self, key, value):\n",
    "        \"\"\"Set value in L1 cache with LRU eviction\"\"\"\n",
    "        if len(self.l1_cache) >= self.l1_size and key not in self.l1_cache:\n",
    "            # Evict least recently used\n",
    "            if self.l1_access_order:\n",
    "                lru_key = self.l1_access_order.popleft()\n",
    "                self.l1_cache.pop(lru_key, None)\n",
    "        \n",
    "        self.l1_cache[key] = value\n",
    "        self._update_access_order(key, 1)\n",
    "    \n",
    "    def _set_l2(self, key, value):\n",
    "        \"\"\"Set value in L2 cache\"\"\"\n",
    "        if len(self.l2_cache) >= self.l2_size and key not in self.l2_cache:\n",
    "            if self.l2_access_order:\n",
    "                lru_key = self.l2_access_order.popleft()\n",
    "                self.l2_cache.pop(lru_key, None)\n",
    "        \n",
    "        self.l2_cache[key] = value\n",
    "        self._update_access_order(key, 2)\n",
    "    \n",
    "    def _set_l3(self, key, value):\n",
    "        \"\"\"Set value in L3 cache\"\"\"\n",
    "        if len(self.l3_cache) >= self.l3_size and key not in self.l3_cache:\n",
    "            if self.l3_access_order:\n",
    "                lru_key = self.l3_access_order.popleft()\n",
    "                self.l3_cache.pop(lru_key, None)\n",
    "        \n",
    "        self.l3_cache[key] = value\n",
    "        self._update_access_order(key, 3)\n",
    "    \n",
    "    def _promote_to_l1(self, key, value):\n",
    "        \"\"\"Promote value to L1 cache\"\"\"\n",
    "        self._set_l1(key, value)\n",
    "    \n",
    "    def _promote_to_l2(self, key, value):\n",
    "        \"\"\"Promote value to L2 cache\"\"\"\n",
    "        self._set_l2(key, value)\n",
    "    \n",
    "    def _update_access_order(self, key, level):\n",
    "        \"\"\"Update access order for LRU\"\"\"\n",
    "        if level == 1:\n",
    "            if key in self.l1_access_order:\n",
    "                self.l1_access_order.remove(key)\n",
    "            self.l1_access_order.append(key)\n",
    "        elif level == 2:\n",
    "            if key in self.l2_access_order:\n",
    "                self.l2_access_order.remove(key)\n",
    "            self.l2_access_order.append(key)\n",
    "        elif level == 3:\n",
    "            if key in self.l3_access_order:\n",
    "                self.l3_access_order.remove(key)\n",
    "            self.l3_access_order.append(key)\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        stats = self.stats.copy()\n",
    "        \n",
    "        if stats['total_requests'] > 0:\n",
    "            stats['l1_hit_rate'] = stats['l1_hits'] / stats['total_requests']\n",
    "            stats['l2_hit_rate'] = stats['l2_hits'] / stats['total_requests']\n",
    "            stats['l3_hit_rate'] = stats['l3_hits'] / stats['total_requests']\n",
    "            stats['overall_hit_rate'] = (stats['l1_hits'] + stats['l2_hits'] + stats['l3_hits']) / stats['total_requests']\n",
    "        \n",
    "        stats['cache_sizes'] = {\n",
    "            'l1': len(self.l1_cache),\n",
    "            'l2': len(self.l2_cache),\n",
    "            'l3': len(self.l3_cache)\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Initialize multi-level cache\n",
    "cache = MultiLevelCache(l1_size=5, l2_size=10, l3_size=20)  # Small sizes for demo\n",
    "print(\"Multi-level cache initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Multi-Level Cache\n",
    "\n",
    "Let's test the cache with various access patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cache with various patterns\n",
    "print(\"Testing multi-level cache...\\n\")\n",
    "\n",
    "# Store some initial data\n",
    "test_data = {\n",
    "    'key1': 'First value',\n",
    "    'key2': 'Second value', \n",
    "    'key3': 'Third value',\n",
    "    'key4': 'Fourth value',\n",
    "    'key5': 'Fifth value',\n",
    "    'key6': 'Sixth value',\n",
    "    'key7': 'Seventh value'\n",
    "}\n",
    "\n",
    "print(\"Storing initial data...\")\n",
    "for key, value in test_data.items():\n",
    "    cache.set(key, value)\n",
    "    print(f\"Stored {key}: {value}\")\n",
    "\n",
    "print(\"\\nTesting cache retrieval patterns...\")\n",
    "\n",
    "# Test different access patterns\n",
    "access_patterns = [\n",
    "    ('key1', 'First access - should be in L1'),\n",
    "    ('key1', 'Second access - should be L1 hit'),\n",
    "    ('key2', 'Access key2 - should be in L1'),\n",
    "    ('key8', 'Non-existent key - should be miss'),\n",
    "    ('key3', 'Access key3 - might be in L2/L3'),\n",
    "    ('key1', 'Access key1 again - should be L1 hit'),\n",
    "]\n",
    "\n",
    "for key, description in access_patterns:\n",
    "    start_time = time.time()\n",
    "    result = cache.get(key)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"{description}:\")\n",
    "    print(f\"  Key: {key}\")\n",
    "    print(f\"  Result: {result['value'] if result else 'Not found'}\")\n",
    "    print(f\"  Access time: {(end_time - start_time)*1000:.2f}ms\")\n",
    "    print()\n",
    "\n",
    "# Display cache statistics\n",
    "stats = cache.get_stats()\n",
    "print(\"=== CACHE STATISTICS ===\")\n",
    "print(f\"Total requests: {stats['total_requests']}\")\n",
    "print(f\"L1 hits: {stats['l1_hits']} (rate: {stats.get('l1_hit_rate', 0):.2%})\")\n",
    "print(f\"L2 hits: {stats['l2_hits']} (rate: {stats.get('l2_hit_rate', 0):.2%})\")\n",
    "print(f\"L3 hits: {stats['l3_hits']} (rate: {stats.get('l3_hit_rate', 0):.2%})\")\n",
    "print(f\"Overall hit rate: {stats.get('overall_hit_rate', 0):.2%}\")\n",
    "print(f\"Cache sizes: L1={stats['cache_sizes']['l1']}, L2={stats['cache_sizes']['l2']}, L3={stats['cache_sizes']['l3']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cost Optimization System\n",
    "\n",
    "Let's implement a cost-aware system that optimizes resource allocation and tracks budgets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostOptimizationSystem:\n",
    "    \"\"\"Cost optimization and budget management system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.user_budgets = {}\n",
    "        self.model_costs = {\n",
    "            'gpt-4': {'cost_per_token': 0.00003, 'quality_score': 0.95, 'speed': 1.0},\n",
    "            'gpt-3.5': {'cost_per_token': 0.000002, 'quality_score': 0.85, 'speed': 2.0},\n",
    "            'local-llama': {'cost_per_token': 0.0000001, 'quality_score': 0.75, 'speed': 0.5}\n",
    "        }\n",
    "        self.usage_history = defaultdict(list)\n",
    "    \n",
    "    def set_user_budget(self, user_id, daily_limit, monthly_limit):\n",
    "        \"\"\"Set budget limits for a user\"\"\"\n",
    "        self.user_budgets[user_id] = {\n",
    "            'daily_limit': daily_limit,\n",
    "            'monthly_limit': monthly_limit,\n",
    "            'daily_usage': 0.0,\n",
    "            'monthly_usage': 0.0,\n",
    "            'last_reset': datetime.now().date()\n",
    "        }\n",
    "    \n",
    "    def estimate_request_cost(self, prompt, max_tokens, model='gpt-3.5'):\n",
    "        \"\"\"Estimate cost for a request\"\"\"\n",
    "        if model not in self.model_costs:\n",
    "            raise ValueError(f\"Unknown model: {model}\")\n",
    "        \n",
    "        # Estimate input tokens (rough approximation)\n",
    "        input_tokens = len(prompt.split()) * 1.3  # Account for tokenization\n",
    "        \n",
    "        # Total tokens (input + output)\n",
    "        total_tokens = input_tokens + max_tokens\n",
    "        \n",
    "        cost_per_token = self.model_costs[model]['cost_per_token']\n",
    "        estimated_cost = total_tokens * cost_per_token\n",
    "        \n",
    "        return {\n",
    "            'estimated_cost': estimated_cost,\n",
    "            'input_tokens': input_tokens,\n",
    "            'max_output_tokens': max_tokens,\n",
    "            'total_tokens': total_tokens,\n",
    "            'model': model\n",
    "        }\n",
    "    \n",
    "    def select_optimal_model(self, prompt, max_tokens, user_id, quality_threshold=0.8):\n",
    "        \"\"\"Select optimal model based on budget and quality requirements\"\"\"\n",
    "        if user_id not in self.user_budgets:\n",
    "            return 'gpt-3.5'  # Default model\n",
    "        \n",
    "        budget_info = self.user_budgets[user_id]\n",
    "        available_budget = min(\n",
    "            budget_info['daily_limit'] - budget_info['daily_usage'],\n",
    "            budget_info['monthly_limit'] - budget_info['monthly_usage']\n",
    "        )\n",
    "        \n",
    "        # Evaluate each model\n",
    "        model_options = []\n",
    "        \n",
    "        for model_name, model_info in self.model_costs.items():\n",
    "            cost_estimate = self.estimate_request_cost(prompt, max_tokens, model_name)\n",
    "            \n",
    "            if cost_estimate['estimated_cost'] <= available_budget:\n",
    "                # Calculate utility score (quality per dollar)\n",
    "                utility_score = model_info['quality_score'] / cost_estimate['estimated_cost']\n",
    "                \n",
    "                model_options.append({\n",
    "                    'model': model_name,\n",
    "                    'cost': cost_estimate['estimated_cost'],\n",
    "                    'quality': model_info['quality_score'],\n",
    "                    'utility_score': utility_score,\n",
    "                    'meets_quality_threshold': model_info['quality_score'] >= quality_threshold\n",
    "                })\n",
    "        \n",
    "        if not model_options:\n",
    "            return None  # No affordable options\n",
    "        \n",
    "        # Filter by quality threshold first\n",
    "        quality_options = [opt for opt in model_options if opt['meets_quality_threshold']]\n",
    "        \n",
    "        if quality_options:\n",
    "            # Select highest utility among quality options\n",
    "            best_option = max(quality_options, key=lambda x: x['utility_score'])\n",
    "        else:\n",
    "            # If no options meet quality threshold, select cheapest\n",
    "            best_option = min(model_options, key=lambda x: x['cost'])\n",
    "        \n",
    "        return best_option['model']\n",
    "    \n",
    "    def process_request_with_cost_optimization(self, user_id, prompt, max_tokens, quality_threshold=0.8):\n",
    "        \"\"\"Process request with cost optimization\"\"\"\n",
    "        # Select optimal model\n",
    "        selected_model = self.select_optimal_model(prompt, max_tokens, user_id, quality_threshold)\n",
    "        \n",
    "        if not selected_model:\n",
    "            return {\n",
    "                'error': 'Insufficient budget for any available model',\n",
    "                'available_budget': self.get_available_budget(user_id)\n",
    "            }\n",
    "        \n",
    "        # Estimate actual cost\n",
    "        cost_estimate = self.estimate_request_cost(prompt, max_tokens, selected_model)\n",
    "        \n",
    "        # Simulate processing (in practice, call actual model)\n",
    "        processing_result = self.simulate_model_processing(prompt, max_tokens, selected_model)\n",
    "        \n",
    "        # Calculate actual cost based on actual tokens used\n",
    "        actual_tokens = processing_result['actual_tokens']\n",
    "        actual_cost = actual_tokens * self.model_costs[selected_model]['cost_per_token']\n",
    "        \n",
    "        # Deduct from budget\n",
    "        self.deduct_cost(user_id, actual_cost)\n",
    "        \n",
    "        # Record usage\n",
    "        self.record_usage(user_id, {\n",
    "            'timestamp': datetime.now(),\n",
    "            'model': selected_model,\n",
    "            'prompt_length': len(prompt),\n",
    "            'tokens_used': actual_tokens,\n",
    "            'cost': actual_cost,\n",
    "            'estimated_cost': cost_estimate['estimated_cost']\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            'response': processing_result['response'],\n",
    "            'model_used': selected_model,\n",
    "            'cost_info': {\n",
    "                'estimated_cost': cost_estimate['estimated_cost'],\n",
    "                'actual_cost': actual_cost,\n",
    "                'tokens_used': actual_tokens,\n",
    "                'remaining_budget': self.get_available_budget(user_id)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def simulate_model_processing(self, prompt, max_tokens, model):\n",
    "        \"\"\"Simulate model processing\"\"\"\n",
    "        # Simulate different response lengths based on model\n",
    "        model_info = self.model_costs[model]\n",
    "        \n",
    "        # Higher quality models tend to give more comprehensive responses\n",
    "        response_length_factor = model_info['quality_score']\n",
    "        actual_output_tokens = int(max_tokens * response_length_factor * np.random.uniform(0.7, 1.0))\n",
    "        \n",
    "        input_tokens = len(prompt.split()) * 1.3\n",
    "        total_tokens = input_tokens + actual_output_tokens\n",
    "        \n",
    "        # Simulate processing time\n",
    "        processing_time = (total_tokens / 1000) / model_info['speed']\n",
    "        time.sleep(min(processing_time, 1.0))  # Cap simulation time\n",
    "        \n",
    "        response = f\"This is a {model} response to: {prompt[:50]}... (Quality: {model_info['quality_score']:.2f})\"\n",
    "        \n",
    "        return {\n",
    "            'response': response,\n",
    "            'actual_tokens': total_tokens,\n",
    "            'processing_time': processing_time\n",
    "        }\n",
    "    \n",
    "    def deduct_cost(self, user_id, cost):\n",
    "        \"\"\"Deduct cost from user budget\"\"\"\n",
    "        if user_id in self.user_budgets:\n",
    "            budget = self.user_budgets[user_id]\n",
    "            \n",
    "            # Check if we need to reset daily usage\n",
    "            today = datetime.now().date()\n",
    "            if budget['last_reset'] < today:\n",
    "                budget['daily_usage'] = 0.0\n",
    "                budget['last_reset'] = today\n",
    "            \n",
    "            budget['daily_usage'] += cost\n",
    "            budget['monthly_usage'] += cost\n",
    "    \n",
    "    def get_available_budget(self, user_id):\n",
    "        \"\"\"Get available budget for user\"\"\"\n",
    "        if user_id not in self.user_budgets:\n",
    "            return float('inf')\n",
    "        \n",
    "        budget = self.user_budgets[user_id]\n",
    "        daily_remaining = budget['daily_limit'] - budget['daily_usage']\n",
    "        monthly_remaining = budget['monthly_limit'] - budget['monthly_usage']\n",
    "        \n",
    "        return min(daily_remaining, monthly_remaining)\n",
    "    \n",
    "    def record_usage(self, user_id, usage_record):\n",
    "        \"\"\"Record usage for analytics\"\"\"\n",
    "        self.usage_history[user_id].append(usage_record)\n",
    "    \n",
    "    def get_usage_analytics(self, user_id):\n",
    "        \"\"\"Get usage analytics for user\"\"\"\n",
    "        if user_id not in self.usage_history:\n",
    "            return {'total_requests': 0, 'total_cost': 0.0}\n",
    "        \n",
    "        usage_records = self.usage_history[user_id]\n",
    "        \n",
    "        analytics = {\n",
    "            'total_requests': len(usage_records),\n",
    "            'total_cost': sum(record['cost'] for record in usage_records),\n",
    "            'avg_cost_per_request': np.mean([record['cost'] for record in usage_records]),\n",
    "            'model_usage': Counter(record['model'] for record in usage_records),\n",
    "            'cost_by_model': defaultdict(float)\n",
    "        }\n",
    "        \n",
    "        for record in usage_records:\n",
    "            analytics['cost_by_model'][record['model']] += record['cost']\n",
    "        \n",
    "        return analytics\n",
    "\n",
    "# Initialize cost optimization system\n",
    "cost_optimizer = CostOptimizationSystem()\n",
    "print(\"Cost optimization system initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Cost Optimization\n",
    "\n",
    "Let's test the cost optimization system with different users and budgets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up test users with different budgets\n",
    "cost_optimizer.set_user_budget('user_premium', daily_limit=10.0, monthly_limit=200.0)\n",
    "cost_optimizer.set_user_budget('user_standard', daily_limit=1.0, monthly_limit=20.0)\n",
    "cost_optimizer.set_user_budget('user_budget', daily_limit=0.1, monthly_limit=2.0)\n",
    "\n",
    "print(\"Set up user budgets:\")\n",
    "print(\"- Premium user: $10/day, $200/month\")\n",
    "print(\"- Standard user: $1/day, $20/month\")\n",
    "print(\"- Budget user: $0.10/day, $2/month\")\n",
    "print()\n",
    "\n",
    "# Test requests\n",
    "test_requests = [\n",
    "    {\n",
    "        'user_id': 'user_premium',\n",
    "        'prompt': 'Write a comprehensive analysis of machine learning trends in 2024',\n",
    "        'max_tokens': 500,\n",
    "        'quality_threshold': 0.9\n",
    "    },\n",
    "    {\n",
    "        'user_id': 'user_standard',\n",
    "        'prompt': 'Explain the basics of neural networks',\n",
    "        'max_tokens': 200,\n",
    "        'quality_threshold': 0.8\n",
    "    },\n",
    "    {\n",
    "        'user_id': 'user_budget',\n",
    "        'prompt': 'What is AI?',\n",
    "        'max_tokens': 100,\n",
    "        'quality_threshold': 0.7\n",
    "    },\n",
    "    {\n",
    "        'user_id': 'user_budget',\n",
    "        'prompt': 'Explain quantum computing in detail',\n",
    "        'max_tokens': 300,\n",
    "        'quality_threshold': 0.9  # High quality requirement\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Processing requests with cost optimization...\\n\")\n",
    "\n",
    "for i, request in enumerate(test_requests, 1):\n",
    "    print(f\"Request {i} from {request['user_id']}:\")\n",
    "    print(f\"  Prompt: {request['prompt'][:60]}...\")\n",
    "    print(f\"  Max tokens: {request['max_tokens']}\")\n",
    "    print(f\"  Quality threshold: {request['quality_threshold']}\")\n",
    "    \n",
    "    # Process request\n",
    "    result = cost_optimizer.process_request_with_cost_optimization(\n",
    "        request['user_id'],\n",
    "        request['prompt'],\n",
    "        request['max_tokens'],\n",
    "        request['quality_threshold']\n",
    "    )\n",
    "    \n",
    "    if 'error' in result:\n",
    "        print(f\"  ERROR: {result['error']}\")\n",
    "        print(f\"  Available budget: ${result['available_budget']:.4f}\")\n",
    "    else:\n",
    "        print(f\"  Model selected: {result['model_used']}\")\n",
    "        print(f\"  Estimated cost: ${result['cost_info']['estimated_cost']:.4f}\")\n",
    "        print(f\"  Actual cost: ${result['cost_info']['actual_cost']:.4f}\")\n",
    "        print(f\"  Tokens used: {result['cost_info']['tokens_used']:.0f}\")\n",
    "        print(f\"  Remaining budget: ${result['cost_info']['remaining_budget']:.4f}\")\n",
    "        print(f\"  Response: {result['response'][:80]}...\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Display usage analytics\n",
    "print(\"=== USAGE ANALYTICS ===\")\n",
    "for user_id in ['user_premium', 'user_standard', 'user_budget']:\n",
    "    analytics = cost_optimizer.get_usage_analytics(user_id)\n",
    "    print(f\"\\n{user_id.upper()}:\")\n",
    "    print(f\"  Total requests: {analytics['total_requests']}\")\n",
    "    print(f\"  Total cost: ${analytics['total_cost']:.4f}\")\n",
    "    if analytics['total_requests'] > 0:\n",
    "        print(f\"  Average cost per request: ${analytics['avg_cost_per_request']:.4f}\")\n",
    "        print(f\"  Model usage: {dict(analytics['model_usage'])}\")\n",
    "        print(f\"  Cost by model: {dict(analytics['cost_by_model'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
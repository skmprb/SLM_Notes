{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting and Inference Control for LLMs\n",
    "\n",
    "## Overview\n",
    "\n",
    "Advanced prompting techniques enable precise control over LLM behavior and outputs. This notebook covers:\n",
    "\n",
    "- **Prompt Engineering**: Template optimization and dynamic prompt construction\n",
    "- **In-Context Learning**: Few-shot learning and example selection strategies\n",
    "- **Chain-of-Thought**: Reasoning enhancement and self-consistency methods\n",
    "- **Tool Calling**: Function integration and orchestration frameworks\n",
    "\n",
    "Let's implement practical prompting and inference control systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from typing import Dict, List, Any, Optional, Callable\n",
    "from collections import defaultdict, Counter\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Prompt Engineering System\n",
    "\n",
    "Let's implement a comprehensive prompt engineering framework with templates and optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PromptTemplate:\n",
    "    name: str\n",
    "    template: str\n",
    "    variables: List[str]\n",
    "    task_type: str\n",
    "    effectiveness_score: float = 0.0\n",
    "    usage_count: int = 0\n",
    "\n",
    "class PromptEngineeringSystem:\n",
    "    \"\"\"Advanced prompt engineering with template optimization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.templates = {}\n",
    "        self.performance_history = defaultdict(list)\n",
    "        self.optimization_strategies = {\n",
    "            'clarity': self.optimize_for_clarity,\n",
    "            'specificity': self.optimize_for_specificity,\n",
    "            'structure': self.optimize_for_structure,\n",
    "            'examples': self.optimize_with_examples\n",
    "        }\n",
    "        self.load_default_templates()\n",
    "    \n",
    "    def load_default_templates(self):\n",
    "        \"\"\"Load default prompt templates for common tasks\"\"\"\n",
    "        default_templates = [\n",
    "            PromptTemplate(\n",
    "                name=\"basic_qa\",\n",
    "                template=\"Question: {question}\\nAnswer:\",\n",
    "                variables=[\"question\"],\n",
    "                task_type=\"question_answering\"\n",
    "            ),\n",
    "            PromptTemplate(\n",
    "                name=\"structured_qa\",\n",
    "                template=\"Context: {context}\\n\\nQuestion: {question}\\n\\nPlease provide a clear and accurate answer based on the context above.\\n\\nAnswer:\",\n",
    "                variables=[\"context\", \"question\"],\n",
    "                task_type=\"question_answering\"\n",
    "            ),\n",
    "            PromptTemplate(\n",
    "                name=\"classification\",\n",
    "                template=\"Classify the following text into one of these categories: {categories}\\n\\nText: {text}\\n\\nCategory:\",\n",
    "                variables=[\"categories\", \"text\"],\n",
    "                task_type=\"classification\"\n",
    "            ),\n",
    "            PromptTemplate(\n",
    "                name=\"reasoning\",\n",
    "                template=\"Problem: {problem}\\n\\nLet's think step by step:\\n1.\",\n",
    "                variables=[\"problem\"],\n",
    "                task_type=\"reasoning\"\n",
    "            ),\n",
    "            PromptTemplate(\n",
    "                name=\"creative_writing\",\n",
    "                template=\"Write a {style} {type} about {topic}. Make it {tone} and approximately {length} words.\\n\\n\",\n",
    "                variables=[\"style\", \"type\", \"topic\", \"tone\", \"length\"],\n",
    "                task_type=\"generation\"\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        for template in default_templates:\n",
    "            self.templates[template.name] = template\n",
    "    \n",
    "    def create_prompt(self, template_name, variables, optimization_level=\"basic\"):\n",
    "        \"\"\"Create optimized prompt from template\"\"\"\n",
    "        if template_name not in self.templates:\n",
    "            raise ValueError(f\"Template '{template_name}' not found\")\n",
    "        \n",
    "        template = self.templates[template_name]\n",
    "        \n",
    "        # Basic prompt creation\n",
    "        prompt = template.template.format(**variables)\n",
    "        \n",
    "        # Apply optimizations\n",
    "        if optimization_level != \"basic\":\n",
    "            prompt = self.apply_optimizations(prompt, template, optimization_level)\n",
    "        \n",
    "        # Track usage\n",
    "        template.usage_count += 1\n",
    "        \n",
    "        return {\n",
    "            'prompt': prompt,\n",
    "            'template_name': template_name,\n",
    "            'variables': variables,\n",
    "            'optimization_level': optimization_level\n",
    "        }\n",
    "    \n",
    "    def apply_optimizations(self, prompt, template, optimization_level):\n",
    "        \"\"\"Apply various optimization strategies\"\"\"\n",
    "        optimized_prompt = prompt\n",
    "        \n",
    "        if optimization_level == \"enhanced\":\n",
    "            # Apply clarity and structure optimizations\n",
    "            optimized_prompt = self.optimization_strategies['clarity'](optimized_prompt)\n",
    "            optimized_prompt = self.optimization_strategies['structure'](optimized_prompt)\n",
    "        \n",
    "        elif optimization_level == \"advanced\":\n",
    "            # Apply all optimizations\n",
    "            for strategy in self.optimization_strategies.values():\n",
    "                optimized_prompt = strategy(optimized_prompt)\n",
    "        \n",
    "        return optimized_prompt\n",
    "    \n",
    "    def optimize_for_clarity(self, prompt):\n",
    "        \"\"\"Optimize prompt for clarity\"\"\"\n",
    "        # Add clear instructions\n",
    "        if not prompt.strip().endswith(\":\"):\n",
    "            prompt += \"\\n\\nPlease provide a clear and detailed response.\"\n",
    "        \n",
    "        # Add formatting hints\n",
    "        if \"classify\" in prompt.lower():\n",
    "            prompt += \"\\n\\nProvide only the category name.\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def optimize_for_specificity(self, prompt):\n",
    "        \"\"\"Optimize prompt for specificity\"\"\"\n",
    "        # Add specific constraints\n",
    "        specificity_additions = [\n",
    "            \"Be specific and precise in your response.\",\n",
    "            \"Include relevant details and examples where appropriate.\",\n",
    "            \"Focus on accuracy and completeness.\"\n",
    "        ]\n",
    "        \n",
    "        addition = random.choice(specificity_additions)\n",
    "        return prompt + f\"\\n\\n{addition}\"\n",
    "    \n",
    "    def optimize_for_structure(self, prompt):\n",
    "        \"\"\"Optimize prompt structure\"\"\"\n",
    "        # Add structural elements\n",
    "        if \"step by step\" not in prompt.lower():\n",
    "            if any(word in prompt.lower() for word in [\"analyze\", \"explain\", \"describe\"]):\n",
    "                prompt += \"\\n\\nStructure your response clearly with numbered points or sections.\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def optimize_with_examples(self, prompt):\n",
    "        \"\"\"Add examples to prompt\"\"\"\n",
    "        # This is a simplified version - in practice, you'd have a database of examples\n",
    "        if \"classify\" in prompt.lower():\n",
    "            example = \"\\n\\nExample:\\nText: 'The movie was fantastic!'\\nCategory: Positive\"\n",
    "            prompt = prompt.replace(\"Category:\", example + \"\\n\\nNow classify:\\nCategory:\")\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def evaluate_prompt_performance(self, template_name, variables, response, quality_score):\n",
    "        \"\"\"Evaluate and record prompt performance\"\"\"\n",
    "        performance_record = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'template_name': template_name,\n",
    "            'variables': variables,\n",
    "            'quality_score': quality_score,\n",
    "            'response_length': len(response),\n",
    "            'response_quality': self.assess_response_quality(response)\n",
    "        }\n",
    "        \n",
    "        self.performance_history[template_name].append(performance_record)\n",
    "        \n",
    "        # Update template effectiveness\n",
    "        if template_name in self.templates:\n",
    "            template = self.templates[template_name]\n",
    "            scores = [record['quality_score'] for record in self.performance_history[template_name]]\n",
    "            template.effectiveness_score = np.mean(scores)\n",
    "        \n",
    "        return performance_record\n",
    "    \n",
    "    def assess_response_quality(self, response):\n",
    "        \"\"\"Simple response quality assessment\"\"\"\n",
    "        quality_indicators = {\n",
    "            'length_appropriate': 50 <= len(response) <= 1000,\n",
    "            'has_structure': any(marker in response for marker in ['1.', '2.', '-', '•']),\n",
    "            'complete_sentences': response.count('.') >= 1,\n",
    "            'no_repetition': len(set(response.split())) / len(response.split()) > 0.7 if response.split() else False\n",
    "        }\n",
    "        \n",
    "        return sum(quality_indicators.values()) / len(quality_indicators)\n",
    "    \n",
    "    def get_best_template(self, task_type):\n",
    "        \"\"\"Get best performing template for task type\"\"\"\n",
    "        candidates = [t for t in self.templates.values() if t.task_type == task_type]\n",
    "        \n",
    "        if not candidates:\n",
    "            return None\n",
    "        \n",
    "        # Sort by effectiveness score\n",
    "        candidates.sort(key=lambda x: x.effectiveness_score, reverse=True)\n",
    "        return candidates[0]\n",
    "    \n",
    "    def get_performance_report(self):\n",
    "        \"\"\"Generate performance report\"\"\"\n",
    "        report = {\n",
    "            'total_templates': len(self.templates),\n",
    "            'total_evaluations': sum(len(history) for history in self.performance_history.values()),\n",
    "            'template_performance': {},\n",
    "            'best_templates': {}\n",
    "        }\n",
    "        \n",
    "        # Template performance\n",
    "        for name, template in self.templates.items():\n",
    "            report['template_performance'][name] = {\n",
    "                'effectiveness_score': template.effectiveness_score,\n",
    "                'usage_count': template.usage_count,\n",
    "                'task_type': template.task_type\n",
    "            }\n",
    "        \n",
    "        # Best templates by task type\n",
    "        task_types = set(t.task_type for t in self.templates.values())\n",
    "        for task_type in task_types:\n",
    "            best_template = self.get_best_template(task_type)\n",
    "            if best_template:\n",
    "                report['best_templates'][task_type] = best_template.name\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Initialize prompt engineering system\n",
    "prompt_system = PromptEngineeringSystem()\n",
    "print(\"Prompt engineering system initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Prompt Engineering\n",
    "\n",
    "Let's test our prompt engineering system with various tasks and optimization levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different prompt engineering scenarios\n",
    "test_scenarios = [\n",
    "    {\n",
    "        'template': 'basic_qa',\n",
    "        'variables': {'question': 'What is machine learning?'},\n",
    "        'optimization': 'basic'\n",
    "    },\n",
    "    {\n",
    "        'template': 'structured_qa',\n",
    "        'variables': {\n",
    "            'context': 'Machine learning is a subset of AI that enables computers to learn from data.',\n",
    "            'question': 'How does machine learning relate to artificial intelligence?'\n",
    "        },\n",
    "        'optimization': 'enhanced'\n",
    "    },\n",
    "    {\n",
    "        'template': 'classification',\n",
    "        'variables': {\n",
    "            'categories': 'Positive, Negative, Neutral',\n",
    "            'text': 'This product is amazing and works perfectly!'\n",
    "        },\n",
    "        'optimization': 'advanced'\n",
    "    },\n",
    "    {\n",
    "        'template': 'reasoning',\n",
    "        'variables': {'problem': 'If a train travels 60 mph for 2 hours, how far does it go?'},\n",
    "        'optimization': 'enhanced'\n",
    "    },\n",
    "    {\n",
    "        'template': 'creative_writing',\n",
    "        'variables': {\n",
    "            'style': 'descriptive',\n",
    "            'type': 'short story',\n",
    "            'topic': 'a robot learning to paint',\n",
    "            'tone': 'inspiring',\n",
    "            'length': '200'\n",
    "        },\n",
    "        'optimization': 'advanced'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Testing prompt engineering with different optimization levels...\\n\")\n",
    "\n",
    "prompt_results = []\n",
    "\n",
    "for i, scenario in enumerate(test_scenarios, 1):\n",
    "    print(f\"Test {i}: {scenario['template']} (optimization: {scenario['optimization']})\")\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt_result = prompt_system.create_prompt(\n",
    "        scenario['template'],\n",
    "        scenario['variables'],\n",
    "        scenario['optimization']\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated Prompt:\")\n",
    "    print(f\"{prompt_result['prompt']}\")\n",
    "    print()\n",
    "    \n",
    "    # Simulate response and evaluation\n",
    "    simulated_response = f\"This is a simulated response for {scenario['template']} task.\"\n",
    "    quality_score = random.uniform(0.6, 0.95)  # Simulate quality assessment\n",
    "    \n",
    "    # Evaluate performance\n",
    "    performance = prompt_system.evaluate_prompt_performance(\n",
    "        scenario['template'],\n",
    "        scenario['variables'],\n",
    "        simulated_response,\n",
    "        quality_score\n",
    "    )\n",
    "    \n",
    "    prompt_results.append({\n",
    "        'template': scenario['template'],\n",
    "        'optimization': scenario['optimization'],\n",
    "        'prompt_length': len(prompt_result['prompt']),\n",
    "        'quality_score': quality_score,\n",
    "        'response_quality': performance['response_quality']\n",
    "    })\n",
    "    \n",
    "    print(f\"Quality Score: {quality_score:.3f}\")\n",
    "    print(f\"Response Quality: {performance['response_quality']:.3f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Generate performance report\n",
    "report = prompt_system.get_performance_report()\n",
    "\n",
    "print(\"\\n=== PROMPT ENGINEERING REPORT ===\")\n",
    "print(f\"Total templates: {report['total_templates']}\")\n",
    "print(f\"Total evaluations: {report['total_evaluations']}\")\n",
    "\n",
    "print(\"\\nTemplate Performance:\")\n",
    "for name, perf in report['template_performance'].items():\n",
    "    print(f\"  {name}: {perf['effectiveness_score']:.3f} (used {perf['usage_count']} times)\")\n",
    "\n",
    "print(\"\\nBest Templates by Task:\")\n",
    "for task_type, template_name in report['best_templates'].items():\n",
    "    print(f\"  {task_type}: {template_name}\")\n",
    "\n",
    "# Visualize results\n",
    "df_prompts = pd.DataFrame(prompt_results)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Quality scores by template\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.bar(df_prompts['template'], df_prompts['quality_score'], color='skyblue')\n",
    "plt.xlabel('Template')\n",
    "plt.ylabel('Quality Score')\n",
    "plt.title('Quality Scores by Template')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Optimization level impact\n",
    "plt.subplot(2, 3, 2)\n",
    "opt_quality = df_prompts.groupby('optimization')['quality_score'].mean()\n",
    "plt.bar(opt_quality.index, opt_quality.values, color=['lightcoral', 'lightgreen', 'gold'])\n",
    "plt.xlabel('Optimization Level')\n",
    "plt.ylabel('Average Quality Score')\n",
    "plt.title('Impact of Optimization Level')\n",
    "\n",
    "# Prompt length vs quality\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.scatter(df_prompts['prompt_length'], df_prompts['quality_score'], alpha=0.7, s=60)\n",
    "plt.xlabel('Prompt Length (characters)')\n",
    "plt.ylabel('Quality Score')\n",
    "plt.title('Prompt Length vs Quality')\n",
    "\n",
    "# Template usage distribution\n",
    "plt.subplot(2, 3, 4)\n",
    "template_usage = [perf['usage_count'] for perf in report['template_performance'].values()]\n",
    "template_names = list(report['template_performance'].keys())\n",
    "plt.pie(template_usage, labels=template_names, autopct='%1.1f%%')\n",
    "plt.title('Template Usage Distribution')\n",
    "\n",
    "# Effectiveness scores\n",
    "plt.subplot(2, 3, 5)\n",
    "effectiveness_scores = [perf['effectiveness_score'] for perf in report['template_performance'].values()]\n",
    "plt.bar(template_names, effectiveness_scores, color='lightgreen')\n",
    "plt.xlabel('Template')\n",
    "plt.ylabel('Effectiveness Score')\n",
    "plt.title('Template Effectiveness')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Quality improvement by optimization\n",
    "plt.subplot(2, 3, 6)\n",
    "basic_quality = df_prompts[df_prompts['optimization'] == 'basic']['quality_score'].mean()\n",
    "enhanced_quality = df_prompts[df_prompts['optimization'] == 'enhanced']['quality_score'].mean()\n",
    "advanced_quality = df_prompts[df_prompts['optimization'] == 'advanced']['quality_score'].mean()\n",
    "\n",
    "improvements = [0, enhanced_quality - basic_quality, advanced_quality - basic_quality]\n",
    "levels = ['Basic', 'Enhanced', 'Advanced']\n",
    "plt.bar(levels, improvements, color=['gray', 'orange', 'red'])\n",
    "plt.xlabel('Optimization Level')\n",
    "plt.ylabel('Quality Improvement')\n",
    "plt.title('Quality Improvement by Optimization')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== OPTIMIZATION ANALYSIS ===\")\n",
    "print(f\"Average quality improvement with enhanced optimization: {enhanced_quality - basic_quality:.3f}\")\n",
    "print(f\"Average quality improvement with advanced optimization: {advanced_quality - basic_quality:.3f}\")\n",
    "print(f\"Most effective template: {max(report['template_performance'].items(), key=lambda x: x[1]['effectiveness_score'])[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. In-Context Learning System\n",
    "\n",
    "Let's implement an advanced in-context learning system with dynamic example selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InContextLearningSystem:\n",
    "    \"\"\"Advanced in-context learning with dynamic example selection\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.example_database = defaultdict(list)\n",
    "        self.selection_strategies = {\n",
    "            'random': self.select_random_examples,\n",
    "            'similarity': self.select_similar_examples,\n",
    "            'diversity': self.select_diverse_examples,\n",
    "            'difficulty': self.select_by_difficulty,\n",
    "            'performance': self.select_by_performance\n",
    "        }\n",
    "        self.performance_tracker = defaultdict(list)\n",
    "    \n",
    "    def add_example(self, task_type, input_text, output_text, metadata=None):\n",
    "        \"\"\"Add example to the database\"\"\"\n",
    "        example = {\n",
    "            'input': input_text,\n",
    "            'output': output_text,\n",
    "            'metadata': metadata or {},\n",
    "            'difficulty': self.estimate_difficulty(input_text, output_text),\n",
    "            'performance_score': 0.0,\n",
    "            'usage_count': 0\n",
    "        }\n",
    "        \n",
    "        self.example_database[task_type].append(example)\n",
    "        return len(self.example_database[task_type]) - 1  # Return index\n",
    "    \n",
    "    def create_few_shot_prompt(self, task_type, query, num_examples=3, strategy='similarity'):\n",
    "        \"\"\"Create few-shot prompt with selected examples\"\"\"\n",
    "        if task_type not in self.example_database:\n",
    "            return f\"Task: {query}\\nAnswer:\"\n",
    "        \n",
    "        # Select examples using specified strategy\n",
    "        selected_examples = self.selection_strategies[strategy](\n",
    "            task_type, query, num_examples\n",
    "        )\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt_parts = []\n",
    "        \n",
    "        # Add examples\n",
    "        for i, example in enumerate(selected_examples, 1):\n",
    "            prompt_parts.append(f\"Example {i}:\")\n",
    "            prompt_parts.append(f\"Input: {example['input']}\")\n",
    "            prompt_parts.append(f\"Output: {example['output']}\")\n",
    "            prompt_parts.append(\"\")  # Empty line\n",
    "        \n",
    "        # Add current query\n",
    "        prompt_parts.append(f\"Now solve this:\")\n",
    "        prompt_parts.append(f\"Input: {query}\")\n",
    "        prompt_parts.append(f\"Output:\")\n",
    "        \n",
    "        return \"\\n\".join(prompt_parts)\n",
    "    \n",
    "    def select_random_examples(self, task_type, query, num_examples):\n",
    "        \"\"\"Select random examples\"\"\"\n",
    "        examples = self.example_database[task_type]\n",
    "        return random.sample(examples, min(num_examples, len(examples)))\n",
    "    \n",
    "    def select_similar_examples(self, task_type, query, num_examples):\n",
    "        \"\"\"Select examples similar to query\"\"\"\n",
    "        examples = self.example_database[task_type]\n",
    "        \n",
    "        # Simple similarity based on word overlap\n",
    "        query_words = set(query.lower().split())\n",
    "        \n",
    "        similarities = []\n",
    "        for example in examples:\n",
    "            example_words = set(example['input'].lower().split())\n",
    "            similarity = len(query_words.intersection(example_words)) / len(query_words.union(example_words))\n",
    "            similarities.append((similarity, example))\n",
    "        \n",
    "        # Sort by similarity and select top examples\n",
    "        similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "        return [ex for _, ex in similarities[:num_examples]]\n",
    "    \n",
    "    def select_diverse_examples(self, task_type, query, num_examples):\n",
    "        \"\"\"Select diverse examples to cover different patterns\"\"\"\n",
    "        examples = self.example_database[task_type]\n",
    "        \n",
    "        if len(examples) <= num_examples:\n",
    "            return examples\n",
    "        \n",
    "        selected = []\n",
    "        remaining = examples.copy()\n",
    "        \n",
    "        # Select first example randomly\n",
    "        first_example = random.choice(remaining)\n",
    "        selected.append(first_example)\n",
    "        remaining.remove(first_example)\n",
    "        \n",
    "        # Select remaining examples to maximize diversity\n",
    "        while len(selected) < num_examples and remaining:\n",
    "            best_candidate = None\n",
    "            best_diversity_score = -1\n",
    "            \n",
    "            for candidate in remaining:\n",
    "                # Calculate diversity score (minimum similarity to selected examples)\n",
    "                diversity_score = min(\n",
    "                    self.calculate_similarity(candidate, selected_ex)\n",
    "                    for selected_ex in selected\n",
    "                )\n",
    "                \n",
    "                if diversity_score > best_diversity_score:\n",
    "                    best_diversity_score = diversity_score\n",
    "                    best_candidate = candidate\n",
    "            \n",
    "            if best_candidate:\n",
    "                selected.append(best_candidate)\n",
    "                remaining.remove(best_candidate)\n",
    "        \n",
    "        return selected\n",
    "    \n",
    "    def select_by_difficulty(self, task_type, query, num_examples):\n",
    "        \"\"\"Select examples by difficulty progression\"\"\"\n",
    "        examples = self.example_database[task_type]\n",
    "        \n",
    "        # Sort by difficulty\n",
    "        sorted_examples = sorted(examples, key=lambda x: x['difficulty'])\n",
    "        \n",
    "        # Select examples with progressive difficulty\n",
    "        if len(sorted_examples) <= num_examples:\n",
    "            return sorted_examples\n",
    "        \n",
    "        # Select evenly spaced examples across difficulty range\n",
    "        indices = np.linspace(0, len(sorted_examples) - 1, num_examples, dtype=int)\n",
    "        return [sorted_examples[i] for i in indices]\n",
    "    \n",
    "    def select_by_performance(self, task_type, query, num_examples):\n",
    "        \"\"\"Select examples that have shown good performance\"\"\"\n",
    "        examples = self.example_database[task_type]\n",
    "        \n",
    "        # Sort by performance score\n",
    "        sorted_examples = sorted(examples, key=lambda x: x['performance_score'], reverse=True)\n",
    "        \n",
    "        return sorted_examples[:num_examples]\n",
    "    \n",
    "    def calculate_similarity(self, example1, example2):\n",
    "        \"\"\"Calculate similarity between two examples\"\"\"\n",
    "        words1 = set(example1['input'].lower().split())\n",
    "        words2 = set(example2['input'].lower().split())\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "        \n",
    "        return len(words1.intersection(words2)) / len(words1.union(words2))\n",
    "    \n",
    "    def estimate_difficulty(self, input_text, output_text):\n",
    "        \"\"\"Estimate difficulty of an example\"\"\"\n",
    "        # Simple heuristics for difficulty estimation\n",
    "        input_complexity = len(input_text.split()) / 10.0  # Normalize by average sentence length\n",
    "        output_complexity = len(output_text.split()) / 10.0\n",
    "        \n",
    "        # Check for complex patterns\n",
    "        complexity_indicators = [\n",
    "            len(re.findall(r'\\d+', input_text)) > 2,  # Multiple numbers\n",
    "            len(input_text.split(',')) > 3,  # Multiple clauses\n",
    "            any(word in input_text.lower() for word in ['analyze', 'compare', 'evaluate', 'synthesize'])\n",
    "        ]\n",
    "        \n",
    "        complexity_bonus = sum(complexity_indicators) * 0.2\n",
    "        \n",
    "        return min(1.0, input_complexity + output_complexity + complexity_bonus)\n",
    "    \n",
    "    def update_example_performance(self, task_type, example_indices, performance_scores):\n",
    "        \"\"\"Update performance scores for examples\"\"\"\n",
    "        for idx, score in zip(example_indices, performance_scores):\n",
    "            if 0 <= idx < len(self.example_database[task_type]):\n",
    "                example = self.example_database[task_type][idx]\n",
    "                example['usage_count'] += 1\n",
    "                \n",
    "                # Update running average of performance\n",
    "                current_score = example['performance_score']\n",
    "                usage_count = example['usage_count']\n",
    "                example['performance_score'] = (current_score * (usage_count - 1) + score) / usage_count\n",
    "    \n",
    "    def get_learning_analytics(self):\n",
    "        \"\"\"Get analytics about in-context learning performance\"\"\"\n",
    "        analytics = {\n",
    "            'total_examples': sum(len(examples) for examples in self.example_database.values()),\n",
    "            'task_types': list(self.example_database.keys()),\n",
    "            'examples_per_task': {task: len(examples) for task, examples in self.example_database.items()},\n",
    "            'difficulty_distribution': {},\n",
    "            'performance_distribution': {},\n",
    "            'usage_statistics': {}\n",
    "        }\n",
    "        \n",
    "        # Analyze difficulty and performance distributions\n",
    "        all_examples = []\n",
    "        for examples in self.example_database.values():\n",
    "            all_examples.extend(examples)\n",
    "        \n",
    "        if all_examples:\n",
    "            difficulties = [ex['difficulty'] for ex in all_examples]\n",
    "            performances = [ex['performance_score'] for ex in all_examples]\n",
    "            usage_counts = [ex['usage_count'] for ex in all_examples]\n",
    "            \n",
    "            analytics['difficulty_distribution'] = {\n",
    "                'mean': np.mean(difficulties),\n",
    "                'std': np.std(difficulties),\n",
    "                'min': np.min(difficulties),\n",
    "                'max': np.max(difficulties)\n",
    "            }\n",
    "            \n",
    "            analytics['performance_distribution'] = {\n",
    "                'mean': np.mean(performances),\n",
    "                'std': np.std(performances),\n",
    "                'min': np.min(performances),\n",
    "                'max': np.max(performances)\n",
    "            }\n",
    "            \n",
    "            analytics['usage_statistics'] = {\n",
    "                'mean_usage': np.mean(usage_counts),\n",
    "                'total_usage': np.sum(usage_counts),\n",
    "                'unused_examples': sum(1 for count in usage_counts if count == 0)\n",
    "            }\n",
    "        \n",
    "        return analytics\n",
    "\n",
    "# Initialize in-context learning system\n",
    "icl_system = InContextLearningSystem()\n",
    "print(\"In-context learning system initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing In-Context Learning\n",
    "\n",
    "Let's test our in-context learning system with different example selection strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add example data for different tasks\n",
    "def populate_example_database():\n",
    "    \"\"\"Populate the example database with sample data\"\"\"\n",
    "    \n",
    "    # Math word problems\n",
    "    math_examples = [\n",
    "        (\"A store has 15 apples and sells 7. How many are left?\", \"15 - 7 = 8 apples\"),\n",
    "        (\"If a car travels 60 miles in 2 hours, what is its speed?\", \"60 miles ÷ 2 hours = 30 mph\"),\n",
    "        (\"A rectangle has length 8 and width 5. What is its area?\", \"8 × 5 = 40 square units\"),\n",
    "        (\"John has 3 bags with 4 marbles each. How many marbles total?\", \"3 × 4 = 12 marbles\"),\n",
    "        (\"A pizza is cut into 8 slices. If 3 are eaten, what fraction remains?\", \"5/8 of the pizza remains\")\n",
    "    ]\n",
    "    \n",
    "    # Sentiment analysis\n",
    "    sentiment_examples = [\n",
    "        (\"This movie is absolutely fantastic!\", \"Positive\"),\n",
    "        (\"I hate waiting in long lines.\", \"Negative\"),\n",
    "        (\"The weather is okay today.\", \"Neutral\"),\n",
    "        (\"Best purchase I've ever made!\", \"Positive\"),\n",
    "        (\"This product is terrible and broke immediately.\", \"Negative\"),\n",
    "        (\"It's an average restaurant, nothing special.\", \"Neutral\")\n",
    "    ]\n",
    "    \n",
    "    # Text summarization\n",
    "    summary_examples = [\n",
    "        (\"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\", \n",
    "         \"Machine learning automates data analysis using AI to help systems learn from data and make decisions.\"),\n",
    "        (\"Climate change refers to long-term shifts in global or regional climate patterns. It is largely attributed to increased levels of atmospheric carbon dioxide produced by the use of fossil fuels.\",\n",
    "         \"Climate change involves long-term shifts in climate patterns, mainly caused by increased atmospheric CO2 from fossil fuels.\"),\n",
    "        (\"Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods from carbon dioxide and water. It generally involves the green pigment chlorophyll and generates oxygen as a byproduct.\",\n",
    "         \"Photosynthesis is how plants use sunlight, CO2, and water to make food using chlorophyll, producing oxygen.\")\n",
    "    ]\n",
    "    \n",
    "    # Add examples to database\n",
    "    for input_text, output_text in math_examples:\n",
    "        icl_system.add_example('math', input_text, output_text)\n",
    "    \n",
    "    for input_text, output_text in sentiment_examples:\n",
    "        icl_system.add_example('sentiment', input_text, output_text)\n",
    "    \n",
    "    for input_text, output_text in summary_examples:\n",
    "        icl_system.add_example('summarization', input_text, output_text)\n",
    "\n",
    "# Populate database\n",
    "populate_example_database()\n",
    "\n",
    "# Test different selection strategies\n",
    "test_queries = [\n",
    "    ('math', 'A bakery makes 24 cupcakes and sells 9. How many cupcakes are left?'),\n",
    "    ('sentiment', 'This book is incredibly boring and poorly written.'),\n",
    "    ('summarization', 'Artificial intelligence is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of intelligent agents.')\n",
    "]\n",
    "\n",
    "strategies = ['random', 'similarity', 'diversity', 'difficulty', 'performance']\n",
    "\n",
    "print(\"Testing different example selection strategies...\\n\")\n",
    "\n",
    "icl_results = []\n",
    "\n",
    "for task_type, query in test_queries:\n",
    "    print(f\"Task: {task_type}\")\n",
    "    print(f\"Query: {query[:80]}...\")\n",
    "    print()\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        prompt = icl_system.create_few_shot_prompt(task_type, query, num_examples=2, strategy=strategy)\n",
    "        \n",
    "        print(f\"Strategy: {strategy}\")\n",
    "        print(f\"Prompt length: {len(prompt)} characters\")\n",
    "        print(f\"First 200 chars: {prompt[:200]}...\")\n",
    "        \n",
    "        # Simulate performance evaluation\n",
    "        performance_score = random.uniform(0.6, 0.95)\n",
    "        \n",
    "        icl_results.append({\n",
    "            'task_type': task_type,\n",
    "            'strategy': strategy,\n",
    "            'prompt_length': len(prompt),\n",
    "            'performance_score': performance_score\n",
    "        })\n",
    "        \n",
    "        print(f\"Simulated performance: {performance_score:.3f}\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Get analytics\n",
    "analytics = icl_system.get_learning_analytics()\n",
    "\n",
    "print(\"\\n=== IN-CONTEXT LEARNING ANALYTICS ===\")\n",
    "print(f\"Total examples: {analytics['total_examples']}\")\n",
    "print(f\"Task types: {analytics['task_types']}\")\n",
    "print(f\"Examples per task: {analytics['examples_per_task']}\")\n",
    "print(f\"Average difficulty: {analytics['difficulty_distribution']['mean']:.3f}\")\n",
    "print(f\"Average performance: {analytics['performance_distribution']['mean']:.3f}\")\n",
    "\n",
    "# Visualize results\n",
    "df_icl = pd.DataFrame(icl_results)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Performance by strategy\n",
    "plt.subplot(2, 3, 1)\n",
    "strategy_performance = df_icl.groupby('strategy')['performance_score'].mean()\n",
    "plt.bar(strategy_performance.index, strategy_performance.values, color='lightblue')\n",
    "plt.xlabel('Selection Strategy')\n",
    "plt.ylabel('Average Performance')\n",
    "plt.title('Performance by Selection Strategy')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Performance by task type\n",
    "plt.subplot(2, 3, 2)\n",
    "task_performance = df_icl.groupby('task_type')['performance_score'].mean()\n",
    "plt.bar(task_performance.index, task_performance.values, color='lightgreen')\n",
    "plt.xlabel('Task Type')\n",
    "plt.ylabel('Average Performance')\n",
    "plt.title('Performance by Task Type')\n",
    "\n",
    "# Prompt length distribution\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(df_icl['prompt_length'], bins=10, alpha=0.7, color='salmon')\n",
    "plt.xlabel('Prompt Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Prompt Length Distribution')\n",
    "\n",
    "# Strategy effectiveness heatmap\n",
    "plt.subplot(2, 3, 4)\n",
    "pivot_data = df_icl.pivot_table(values='performance_score', index='task_type', columns='strategy', aggfunc='mean')\n",
    "im = plt.imshow(pivot_data.values, cmap='RdYlGn', aspect='auto')\n",
    "plt.colorbar(im)\n",
    "plt.xlabel('Strategy')\n",
    "plt.ylabel('Task Type')\n",
    "plt.title('Strategy Effectiveness Heatmap')\n",
    "plt.xticks(range(len(pivot_data.columns)), pivot_data.columns, rotation=45)\n",
    "plt.yticks(range(len(pivot_data.index)), pivot_data.index)\n",
    "\n",
    "# Examples per task distribution\n",
    "plt.subplot(2, 3, 5)\n",
    "task_counts = list(analytics['examples_per_task'].values())\n",
    "task_names = list(analytics['examples_per_task'].keys())\n",
    "plt.pie(task_counts, labels=task_names, autopct='%1.1f%%')\n",
    "plt.title('Examples per Task Distribution')\n",
    "\n",
    "# Performance vs prompt length\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.scatter(df_icl['prompt_length'], df_icl['performance_score'], alpha=0.6, s=50)\n",
    "plt.xlabel('Prompt Length')\n",
    "plt.ylabel('Performance Score')\n",
    "plt.title('Performance vs Prompt Length')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== STRATEGY ANALYSIS ===\")\n",
    "best_strategy = strategy_performance.idxmax()\n",
    "best_performance = strategy_performance.max()\n",
    "print(f\"Best performing strategy: {best_strategy} ({best_performance:.3f})\")\n",
    "print(f\"Strategy performance range: {strategy_performance.min():.3f} - {strategy_performance.max():.3f}\")\n",
    "print(f\"Average prompt length: {df_icl['prompt_length'].mean():.0f} characters\")\n",
    "print(f\"Most effective task type: {task_performance.idxmax()} ({task_performance.max():.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
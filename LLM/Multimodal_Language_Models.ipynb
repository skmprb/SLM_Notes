{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Language Models\n",
    "\n",
    "## Overview\n",
    "\n",
    "Multimodal LLMs can process and generate content across different modalities. This notebook covers:\n",
    "\n",
    "- **Vision-Language Models**: Text-image understanding and generation\n",
    "- **Audio-Language Models**: Speech and text integration\n",
    "- **Cross-Modal Attention**: Attention mechanisms across modalities\n",
    "- **Multimodal Fusion**: Combining information from different sources\n",
    "\n",
    "Let's implement practical multimodal architectures and processing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vision-Language Model\n",
    "\n",
    "Architecture for processing both text and images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEncoder(nn.Module):\n",
    "    \"\"\"Simple vision encoder for image features\"\"\"\n",
    "    \n",
    "    def __init__(self, image_size=224, patch_size=16, d_model=768):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Calculate number of patches\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            3, d_model, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        \n",
    "        # Position embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, d_model))\n",
    "        \n",
    "        # CLS token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead=12, batch_first=True),\n",
    "            num_layers=6\n",
    "        )\n",
    "    \n",
    "    def forward(self, images):\n",
    "        B = images.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(images)  # [B, d_model, H/P, W/P]\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, d_model]\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Apply transformer\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class CrossModalAttention(nn.Module):\n",
    "    \"\"\"Cross-modal attention between text and vision\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        # Projection layers\n",
    "        self.text_to_vision_attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
    "        self.vision_to_text_attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
    "        \n",
    "        # Layer norms\n",
    "        self.text_norm = nn.LayerNorm(d_model)\n",
    "        self.vision_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Feed-forward networks\n",
    "        self.text_ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "        \n",
    "        self.vision_ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "    \n",
    "    def forward(self, text_features, vision_features):\n",
    "        # Text attending to vision\n",
    "        text_attn_out, _ = self.text_to_vision_attn(\n",
    "            text_features, vision_features, vision_features\n",
    "        )\n",
    "        text_features = self.text_norm(text_features + text_attn_out)\n",
    "        text_features = text_features + self.text_ffn(text_features)\n",
    "        \n",
    "        # Vision attending to text\n",
    "        vision_attn_out, _ = self.vision_to_text_attn(\n",
    "            vision_features, text_features, text_features\n",
    "        )\n",
    "        vision_features = self.vision_norm(vision_features + vision_attn_out)\n",
    "        vision_features = vision_features + self.vision_ffn(vision_features)\n",
    "        \n",
    "        return text_features, vision_features\n",
    "\n",
    "class VisionLanguageModel(nn.Module):\n",
    "    \"\"\"Complete vision-language model\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=768, n_heads=12, n_layers=6):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Text components\n",
    "        self.text_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.text_pos_embed = nn.Parameter(torch.randn(1, 512, d_model))  # Max seq len 512\n",
    "        \n",
    "        # Vision components\n",
    "        self.vision_encoder = VisionEncoder(d_model=d_model)\n",
    "        \n",
    "        # Cross-modal layers\n",
    "        self.cross_modal_layers = nn.ModuleList([\n",
    "            CrossModalAttention(d_model, n_heads) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output heads\n",
    "        self.text_head = nn.Linear(d_model, vocab_size)\n",
    "        self.vision_head = nn.Linear(d_model, 1000)  # ImageNet classes\n",
    "        \n",
    "        # Multimodal fusion\n",
    "        self.fusion_layer = nn.Linear(2 * d_model, d_model)\n",
    "        self.multimodal_head = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, text_tokens=None, images=None, task='multimodal'):\n",
    "        text_features = None\n",
    "        vision_features = None\n",
    "        \n",
    "        # Process text\n",
    "        if text_tokens is not None:\n",
    "            B, T = text_tokens.shape\n",
    "            text_features = self.text_embed(text_tokens)\n",
    "            text_features = text_features + self.text_pos_embed[:, :T, :]\n",
    "        \n",
    "        # Process images\n",
    "        if images is not None:\n",
    "            vision_features = self.vision_encoder(images)\n",
    "        \n",
    "        # Cross-modal processing\n",
    "        if text_features is not None and vision_features is not None:\n",
    "            for layer in self.cross_modal_layers:\n",
    "                text_features, vision_features = layer(text_features, vision_features)\n",
    "        \n",
    "        # Task-specific outputs\n",
    "        if task == 'text_generation' and text_features is not None:\n",
    "            return self.text_head(text_features)\n",
    "        elif task == 'image_classification' and vision_features is not None:\n",
    "            return self.vision_head(vision_features[:, 0])  # CLS token\n",
    "        elif task == 'multimodal' and text_features is not None and vision_features is not None:\n",
    "            # Fuse modalities\n",
    "            text_pooled = text_features.mean(dim=1)  # Pool text features\n",
    "            vision_pooled = vision_features[:, 0]     # CLS token\n",
    "            \n",
    "            fused = torch.cat([text_pooled, vision_pooled], dim=-1)\n",
    "            fused = self.fusion_layer(fused)\n",
    "            \n",
    "            return self.multimodal_head(fused)\n",
    "        \n",
    "        return None\n",
    "\n",
    "class AudioLanguageModel(nn.Module):\n",
    "    \"\"\"Audio-language model for speech and text\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=768, n_heads=12):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Audio encoder (simplified)\n",
    "        self.audio_encoder = nn.Sequential(\n",
    "            nn.Conv1d(80, 256, kernel_size=3, padding=1),  # Mel-spectrogram input\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(512, d_model, kernel_size=3, padding=1),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        # Text components\n",
    "        self.text_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.text_pos_embed = nn.Parameter(torch.randn(1, 512, d_model))\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        self.audio_text_attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
    "        self.text_audio_attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
    "        \n",
    "        # Output heads\n",
    "        self.speech_recognition_head = nn.Linear(d_model, vocab_size)\n",
    "        self.text_to_speech_head = nn.Linear(d_model, 80)  # Mel-spectrogram output\n",
    "        \n",
    "        # Layer norms\n",
    "        self.audio_norm = nn.LayerNorm(d_model)\n",
    "        self.text_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, audio_features=None, text_tokens=None, task='speech_recognition'):\n",
    "        audio_repr = None\n",
    "        text_repr = None\n",
    "        \n",
    "        # Process audio\n",
    "        if audio_features is not None:\n",
    "            # audio_features: [B, mel_bins, time_steps]\n",
    "            audio_encoded = self.audio_encoder(audio_features)  # [B, d_model, 1]\n",
    "            audio_repr = audio_encoded.squeeze(-1).unsqueeze(1)  # [B, 1, d_model]\n",
    "        \n",
    "        # Process text\n",
    "        if text_tokens is not None:\n",
    "            B, T = text_tokens.shape\n",
    "            text_repr = self.text_embed(text_tokens)\n",
    "            text_repr = text_repr + self.text_pos_embed[:, :T, :]\n",
    "        \n",
    "        # Cross-modal processing\n",
    "        if audio_repr is not None and text_repr is not None:\n",
    "            # Audio attending to text\n",
    "            audio_attn_out, _ = self.audio_text_attn(audio_repr, text_repr, text_repr)\n",
    "            audio_repr = self.audio_norm(audio_repr + audio_attn_out)\n",
    "            \n",
    "            # Text attending to audio\n",
    "            text_attn_out, _ = self.text_audio_attn(text_repr, audio_repr, audio_repr)\n",
    "            text_repr = self.text_norm(text_repr + text_attn_out)\n",
    "        \n",
    "        # Task-specific outputs\n",
    "        if task == 'speech_recognition' and audio_repr is not None:\n",
    "            return self.speech_recognition_head(audio_repr)\n",
    "        elif task == 'text_to_speech' and text_repr is not None:\n",
    "            return self.text_to_speech_head(text_repr)\n",
    "        \n",
    "        return None\n",
    "\n",
    "print(\"Multimodal model architectures implemented!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
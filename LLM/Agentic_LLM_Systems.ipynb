{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic LLM Systems - Practical Implementation\n",
    "\n",
    "## Overview\n",
    "\n",
    "Agentic LLM systems enable autonomous reasoning, planning, and action-taking capabilities. This notebook covers:\n",
    "\n",
    "- **Planning Systems**: Hierarchical and dynamic planning algorithms\n",
    "- **Reflection Mechanisms**: Self-assessment and meta-cognitive processes\n",
    "- **ReAct Pattern**: Reasoning and Acting in synergistic loops\n",
    "- **Multi-Agent Systems**: Coordination and collaboration between agents\n",
    "\n",
    "Let's build practical implementations of these agentic capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, deque\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import asyncio\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hierarchical Planning System\n",
    "\n",
    "Let's implement a hierarchical planning system that can break down complex goals into manageable sub-tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskStatus(Enum):\n",
    "    PENDING = \"pending\"\n",
    "    IN_PROGRESS = \"in_progress\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n",
    "    BLOCKED = \"blocked\"\n",
    "\n",
    "@dataclass\n",
    "class Task:\n",
    "    id: str\n",
    "    description: str\n",
    "    priority: int\n",
    "    estimated_duration: float\n",
    "    dependencies: List[str]\n",
    "    status: TaskStatus = TaskStatus.PENDING\n",
    "    parent_task: Optional[str] = None\n",
    "    subtasks: List[str] = None\n",
    "    result: Optional[Dict] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.subtasks is None:\n",
    "            self.subtasks = []\n",
    "\n",
    "class HierarchicalPlanner:\n",
    "    \"\"\"Hierarchical task planning system for complex goal decomposition\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tasks = {}\n",
    "        self.execution_history = []\n",
    "        self.planning_strategies = {\n",
    "            'research': self.plan_research_task,\n",
    "            'analysis': self.plan_analysis_task,\n",
    "            'creation': self.plan_creation_task,\n",
    "            'problem_solving': self.plan_problem_solving_task\n",
    "        }\n",
    "    \n",
    "    def create_plan(self, goal_description, task_type='general', max_depth=3):\n",
    "        \"\"\"Create hierarchical plan for achieving a goal\"\"\"\n",
    "        print(f\"Creating plan for: {goal_description}\")\n",
    "        \n",
    "        # Create root task\n",
    "        root_task = Task(\n",
    "            id=\"root\",\n",
    "            description=goal_description,\n",
    "            priority=1,\n",
    "            estimated_duration=0,\n",
    "            dependencies=[]\n",
    "        )\n",
    "        \n",
    "        self.tasks[\"root\"] = root_task\n",
    "        \n",
    "        # Decompose into subtasks\n",
    "        self.decompose_task(\"root\", task_type, current_depth=0, max_depth=max_depth)\n",
    "        \n",
    "        # Create execution order\n",
    "        execution_plan = self.create_execution_order()\n",
    "        \n",
    "        return {\n",
    "            'root_task': root_task,\n",
    "            'all_tasks': self.tasks,\n",
    "            'execution_order': execution_plan,\n",
    "            'total_tasks': len(self.tasks),\n",
    "            'estimated_duration': self.calculate_total_duration()\n",
    "        }\n",
    "    \n",
    "    def decompose_task(self, task_id, task_type, current_depth=0, max_depth=3):\n",
    "        \"\"\"Recursively decompose task into subtasks\"\"\"\n",
    "        if current_depth >= max_depth:\n",
    "            return\n",
    "        \n",
    "        task = self.tasks[task_id]\n",
    "        \n",
    "        # Use appropriate planning strategy\n",
    "        if task_type in self.planning_strategies:\n",
    "            subtasks = self.planning_strategies[task_type](task.description, current_depth)\n",
    "        else:\n",
    "            subtasks = self.plan_generic_task(task.description, current_depth)\n",
    "        \n",
    "        # Create subtask objects\n",
    "        for i, subtask_desc in enumerate(subtasks):\n",
    "            subtask_id = f\"{task_id}_sub_{i+1}\"\n",
    "            \n",
    "            subtask = Task(\n",
    "                id=subtask_id,\n",
    "                description=subtask_desc,\n",
    "                priority=task.priority + 1,\n",
    "                estimated_duration=random.uniform(0.5, 3.0),  # Simulate duration\n",
    "                dependencies=[],\n",
    "                parent_task=task_id\n",
    "            )\n",
    "            \n",
    "            self.tasks[subtask_id] = subtask\n",
    "            task.subtasks.append(subtask_id)\n",
    "            \n",
    "            # Recursively decompose if not at max depth\n",
    "            if current_depth < max_depth - 1 and len(subtasks) <= 3:  # Limit branching\n",
    "                self.decompose_task(subtask_id, task_type, current_depth + 1, max_depth)\n",
    "    \n",
    "    def plan_research_task(self, description, depth):\n",
    "        \"\"\"Planning strategy for research tasks\"\"\"\n",
    "        if depth == 0:\n",
    "            return [\n",
    "                \"Define research questions and scope\",\n",
    "                \"Gather and review relevant sources\",\n",
    "                \"Analyze and synthesize findings\",\n",
    "                \"Document conclusions and insights\"\n",
    "            ]\n",
    "        elif depth == 1:\n",
    "            if \"questions\" in description.lower():\n",
    "                return [\"Identify key topics\", \"Formulate specific questions\", \"Prioritize research areas\"]\n",
    "            elif \"sources\" in description.lower():\n",
    "                return [\"Search academic databases\", \"Identify expert opinions\", \"Collect relevant data\"]\n",
    "            elif \"analyze\" in description.lower():\n",
    "                return [\"Compare different perspectives\", \"Identify patterns\", \"Draw connections\"]\n",
    "        return [\"Execute research step\", \"Verify results\", \"Document findings\"]\n",
    "    \n",
    "    def plan_analysis_task(self, description, depth):\n",
    "        \"\"\"Planning strategy for analysis tasks\"\"\"\n",
    "        if depth == 0:\n",
    "            return [\n",
    "                \"Collect and prepare data\",\n",
    "                \"Apply analytical methods\",\n",
    "                \"Interpret results\",\n",
    "                \"Generate recommendations\"\n",
    "            ]\n",
    "        elif depth == 1:\n",
    "            if \"data\" in description.lower():\n",
    "                return [\"Clean and validate data\", \"Organize data structure\", \"Check data quality\"]\n",
    "            elif \"methods\" in description.lower():\n",
    "                return [\"Select appropriate techniques\", \"Run analysis\", \"Validate results\"]\n",
    "        return [\"Perform analysis step\", \"Check accuracy\", \"Record outcomes\"]\n",
    "    \n",
    "    def plan_creation_task(self, description, depth):\n",
    "        \"\"\"Planning strategy for creation tasks\"\"\"\n",
    "        if depth == 0:\n",
    "            return [\n",
    "                \"Plan and design approach\",\n",
    "                \"Create initial version\",\n",
    "                \"Review and refine\",\n",
    "                \"Finalize and deliver\"\n",
    "            ]\n",
    "        elif depth == 1:\n",
    "            if \"plan\" in description.lower():\n",
    "                return [\"Define requirements\", \"Create outline\", \"Set milestones\"]\n",
    "            elif \"create\" in description.lower():\n",
    "                return [\"Build core components\", \"Add details\", \"Test functionality\"]\n",
    "        return [\"Work on component\", \"Test and validate\", \"Integrate with whole\"]\n",
    "    \n",
    "    def plan_problem_solving_task(self, description, depth):\n",
    "        \"\"\"Planning strategy for problem-solving tasks\"\"\"\n",
    "        if depth == 0:\n",
    "            return [\n",
    "                \"Understand and define the problem\",\n",
    "                \"Generate potential solutions\",\n",
    "                \"Evaluate and select best solution\",\n",
    "                \"Implement and test solution\"\n",
    "            ]\n",
    "        elif depth == 1:\n",
    "            if \"understand\" in description.lower():\n",
    "                return [\"Gather problem details\", \"Identify constraints\", \"Define success criteria\"]\n",
    "            elif \"generate\" in description.lower():\n",
    "                return [\"Brainstorm ideas\", \"Research existing solutions\", \"Create novel approaches\"]\n",
    "        return [\"Work on solution aspect\", \"Test approach\", \"Refine as needed\"]\n",
    "    \n",
    "    def plan_generic_task(self, description, depth):\n",
    "        \"\"\"Generic planning strategy\"\"\"\n",
    "        return [\n",
    "            f\"Prepare for: {description[:30]}...\",\n",
    "            f\"Execute: {description[:30]}...\",\n",
    "            f\"Verify: {description[:30]}...\"\n",
    "        ]\n",
    "    \n",
    "    def create_execution_order(self):\n",
    "        \"\"\"Create topologically sorted execution order\"\"\"\n",
    "        # Simple depth-first traversal for demonstration\n",
    "        execution_order = []\n",
    "        visited = set()\n",
    "        \n",
    "        def dfs(task_id):\n",
    "            if task_id in visited:\n",
    "                return\n",
    "            \n",
    "            visited.add(task_id)\n",
    "            task = self.tasks[task_id]\n",
    "            \n",
    "            # Visit subtasks first (depth-first)\n",
    "            for subtask_id in task.subtasks:\n",
    "                dfs(subtask_id)\n",
    "            \n",
    "            # Add current task after subtasks\n",
    "            if not task.subtasks:  # Only add leaf tasks to execution order\n",
    "                execution_order.append(task_id)\n",
    "        \n",
    "        dfs(\"root\")\n",
    "        return execution_order\n",
    "    \n",
    "    def calculate_total_duration(self):\n",
    "        \"\"\"Calculate total estimated duration\"\"\"\n",
    "        leaf_tasks = [task for task in self.tasks.values() if not task.subtasks]\n",
    "        return sum(task.estimated_duration for task in leaf_tasks)\n",
    "    \n",
    "    def execute_plan(self, execution_order, simulate=True):\n",
    "        \"\"\"Execute the planned tasks\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        print(f\"Executing plan with {len(execution_order)} tasks...\\n\")\n",
    "        \n",
    "        for i, task_id in enumerate(execution_order, 1):\n",
    "            task = self.tasks[task_id]\n",
    "            \n",
    "            print(f\"Step {i}: {task.description}\")\n",
    "            \n",
    "            if simulate:\n",
    "                # Simulate task execution\n",
    "                time.sleep(min(task.estimated_duration * 0.1, 0.5))  # Speed up for demo\n",
    "                \n",
    "                # Simulate success/failure\n",
    "                success_probability = 0.9  # 90% success rate\n",
    "                success = random.random() < success_probability\n",
    "                \n",
    "                if success:\n",
    "                    task.status = TaskStatus.COMPLETED\n",
    "                    result = f\"Successfully completed: {task.description[:50]}...\"\n",
    "                    print(f\"  ✓ {result}\")\n",
    "                else:\n",
    "                    task.status = TaskStatus.FAILED\n",
    "                    result = f\"Failed to complete: {task.description[:50]}...\"\n",
    "                    print(f\"  ✗ {result}\")\n",
    "                \n",
    "                task.result = {'success': success, 'message': result}\n",
    "                results.append(task.result)\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_plan_summary(self):\n",
    "        \"\"\"Get summary of the current plan\"\"\"\n",
    "        total_tasks = len(self.tasks)\n",
    "        leaf_tasks = [task for task in self.tasks.values() if not task.subtasks]\n",
    "        completed_tasks = [task for task in self.tasks.values() if task.status == TaskStatus.COMPLETED]\n",
    "        \n",
    "        return {\n",
    "            'total_tasks': total_tasks,\n",
    "            'leaf_tasks': len(leaf_tasks),\n",
    "            'completed_tasks': len(completed_tasks),\n",
    "            'completion_rate': len(completed_tasks) / len(leaf_tasks) if leaf_tasks else 0,\n",
    "            'estimated_duration': self.calculate_total_duration()\n",
    "        }\n",
    "\n",
    "# Initialize the planner\n",
    "planner = HierarchicalPlanner()\n",
    "print(\"Hierarchical planner initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Hierarchical Planning\n",
    "\n",
    "Let's test our planner with different types of complex goals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different planning scenarios\n",
    "test_goals = [\n",
    "    {\n",
    "        'goal': 'Research the impact of artificial intelligence on healthcare',\n",
    "        'type': 'research',\n",
    "        'description': 'Complex research task'\n",
    "    },\n",
    "    {\n",
    "        'goal': 'Analyze customer satisfaction data to improve service quality',\n",
    "        'type': 'analysis',\n",
    "        'description': 'Data analysis task'\n",
    "    },\n",
    "    {\n",
    "        'goal': 'Create a comprehensive marketing strategy for a new product launch',\n",
    "        'type': 'creation',\n",
    "        'description': 'Creative planning task'\n",
    "    }\n",
    "]\n",
    "\n",
    "planning_results = []\n",
    "\n",
    "for i, test_case in enumerate(test_goals, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEST {i}: {test_case['description']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create fresh planner for each test\n",
    "    test_planner = HierarchicalPlanner()\n",
    "    \n",
    "    # Create plan\n",
    "    plan = test_planner.create_plan(\n",
    "        test_case['goal'], \n",
    "        test_case['type'], \n",
    "        max_depth=3\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nPlan created successfully!\")\n",
    "    print(f\"Total tasks: {plan['total_tasks']}\")\n",
    "    print(f\"Execution steps: {len(plan['execution_order'])}\")\n",
    "    print(f\"Estimated duration: {plan['estimated_duration']:.1f} hours\")\n",
    "    \n",
    "    # Show task hierarchy\n",
    "    print(f\"\\nTask Hierarchy:\")\n",
    "    test_planner.print_task_hierarchy(\"root\", indent=0)\n",
    "    \n",
    "    # Execute plan\n",
    "    print(f\"\\nExecuting plan...\")\n",
    "    execution_results = test_planner.execute_plan(plan['execution_order'], simulate=True)\n",
    "    \n",
    "    # Get summary\n",
    "    summary = test_planner.get_plan_summary()\n",
    "    \n",
    "    print(f\"\\nExecution Summary:\")\n",
    "    print(f\"  Completion rate: {summary['completion_rate']:.1%}\")\n",
    "    print(f\"  Tasks completed: {summary['completed_tasks']}/{summary['leaf_tasks']}\")\n",
    "    \n",
    "    planning_results.append({\n",
    "        'goal_type': test_case['type'],\n",
    "        'total_tasks': summary['total_tasks'],\n",
    "        'completion_rate': summary['completion_rate'],\n",
    "        'estimated_duration': summary['estimated_duration']\n",
    "    })\n",
    "\n",
    "# Add method to print hierarchy\n",
    "def print_task_hierarchy(self, task_id, indent=0):\n",
    "    \"\"\"Print task hierarchy in tree format\"\"\"\n",
    "    task = self.tasks[task_id]\n",
    "    prefix = \"  \" * indent + (\"├─ \" if indent > 0 else \"\")\n",
    "    status_symbol = {\n",
    "        TaskStatus.COMPLETED: \"✓\",\n",
    "        TaskStatus.FAILED: \"✗\",\n",
    "        TaskStatus.IN_PROGRESS: \"⟳\",\n",
    "        TaskStatus.PENDING: \"○\"\n",
    "    }.get(task.status, \"○\")\n",
    "    \n",
    "    print(f\"{prefix}{status_symbol} {task.description}\")\n",
    "    \n",
    "    for subtask_id in task.subtasks:\n",
    "        self.print_task_hierarchy(subtask_id, indent + 1)\n",
    "\n",
    "# Add the method to the class\n",
    "HierarchicalPlanner.print_task_hierarchy = print_task_hierarchy\n",
    "\n",
    "# Summary visualization\n",
    "df_results = pd.DataFrame(planning_results)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Task complexity by type\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.bar(df_results['goal_type'], df_results['total_tasks'], color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.title('Task Complexity by Goal Type')\n",
    "plt.xlabel('Goal Type')\n",
    "plt.ylabel('Total Tasks Generated')\n",
    "\n",
    "# Completion rates\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.bar(df_results['goal_type'], df_results['completion_rate'], color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.title('Completion Rates by Goal Type')\n",
    "plt.xlabel('Goal Type')\n",
    "plt.ylabel('Completion Rate')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Duration estimates\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.bar(df_results['goal_type'], df_results['estimated_duration'], color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.title('Estimated Duration by Goal Type')\n",
    "plt.xlabel('Goal Type')\n",
    "plt.ylabel('Duration (hours)')\n",
    "\n",
    "# Planning efficiency (tasks per hour)\n",
    "plt.subplot(2, 2, 4)\n",
    "efficiency = df_results['total_tasks'] / df_results['estimated_duration']\n",
    "plt.bar(df_results['goal_type'], efficiency, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.title('Planning Efficiency (Tasks/Hour)')\n",
    "plt.xlabel('Goal Type')\n",
    "plt.ylabel('Tasks per Hour')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== PLANNING ANALYSIS ===\")\n",
    "print(f\"Average tasks per goal: {df_results['total_tasks'].mean():.1f}\")\n",
    "print(f\"Average completion rate: {df_results['completion_rate'].mean():.1%}\")\n",
    "print(f\"Average duration: {df_results['estimated_duration'].mean():.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reflection and Self-Assessment System\n",
    "\n",
    "Let's implement a reflection system that allows agents to assess their own performance and improve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectionSystem:\n",
    "    \"\"\"Self-reflection and meta-cognitive system for agents\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reflection_history = []\n",
    "        self.performance_metrics = defaultdict(list)\n",
    "        self.learning_insights = []\n",
    "        self.improvement_strategies = []\n",
    "        \n",
    "        # Reflection frameworks\n",
    "        self.reflection_frameworks = {\n",
    "            'performance': self.reflect_on_performance,\n",
    "            'process': self.reflect_on_process,\n",
    "            'outcome': self.reflect_on_outcome,\n",
    "            'learning': self.reflect_on_learning\n",
    "        }\n",
    "    \n",
    "    def conduct_reflection(self, task_results, reflection_type='comprehensive'):\n",
    "        \"\"\"Conduct comprehensive reflection on task execution\"\"\"\n",
    "        reflection_session = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'task_results': task_results,\n",
    "            'reflection_type': reflection_type,\n",
    "            'insights': {},\n",
    "            'action_items': [],\n",
    "            'confidence_assessment': 0.0\n",
    "        }\n",
    "        \n",
    "        if reflection_type == 'comprehensive':\n",
    "            # Run all reflection frameworks\n",
    "            for framework_name, framework_func in self.reflection_frameworks.items():\n",
    "                insights = framework_func(task_results)\n",
    "                reflection_session['insights'][framework_name] = insights\n",
    "        else:\n",
    "            # Run specific reflection framework\n",
    "            if reflection_type in self.reflection_frameworks:\n",
    "                insights = self.reflection_frameworks[reflection_type](task_results)\n",
    "                reflection_session['insights'][reflection_type] = insights\n",
    "        \n",
    "        # Generate action items\n",
    "        reflection_session['action_items'] = self.generate_action_items(reflection_session['insights'])\n",
    "        \n",
    "        # Assess confidence in reflection\n",
    "        reflection_session['confidence_assessment'] = self.assess_reflection_confidence(reflection_session)\n",
    "        \n",
    "        # Store reflection\n",
    "        self.reflection_history.append(reflection_session)\n",
    "        \n",
    "        return reflection_session\n",
    "    \n",
    "    def reflect_on_performance(self, task_results):\n",
    "        \"\"\"Reflect on performance metrics and outcomes\"\"\"\n",
    "        successful_tasks = sum(1 for result in task_results if result.get('success', False))\n",
    "        total_tasks = len(task_results)\n",
    "        success_rate = successful_tasks / total_tasks if total_tasks > 0 else 0\n",
    "        \n",
    "        # Analyze performance patterns\n",
    "        performance_insights = {\n",
    "            'success_rate': success_rate,\n",
    "            'total_tasks': total_tasks,\n",
    "            'successful_tasks': successful_tasks,\n",
    "            'performance_level': self.categorize_performance(success_rate),\n",
    "            'strengths': [],\n",
    "            'weaknesses': [],\n",
    "            'trends': self.analyze_performance_trends()\n",
    "        }\n",
    "        \n",
    "        # Identify strengths and weaknesses\n",
    "        if success_rate > 0.8:\n",
    "            performance_insights['strengths'].append(\"High task completion rate\")\n",
    "        elif success_rate < 0.6:\n",
    "            performance_insights['weaknesses'].append(\"Low task completion rate\")\n",
    "        \n",
    "        # Analyze failure patterns\n",
    "        failed_tasks = [result for result in task_results if not result.get('success', False)]\n",
    "        if failed_tasks:\n",
    "            failure_analysis = self.analyze_failures(failed_tasks)\n",
    "            performance_insights['failure_patterns'] = failure_analysis\n",
    "        \n",
    "        return performance_insights\n",
    "    \n",
    "    def reflect_on_process(self, task_results):\n",
    "        \"\"\"Reflect on the process and methodology used\"\"\"\n",
    "        process_insights = {\n",
    "            'efficiency_assessment': self.assess_process_efficiency(task_results),\n",
    "            'bottlenecks': self.identify_bottlenecks(task_results),\n",
    "            'optimization_opportunities': [],\n",
    "            'process_quality': 'good'  # Simplified assessment\n",
    "        }\n",
    "        \n",
    "        # Identify optimization opportunities\n",
    "        if len(task_results) > 5:\n",
    "            process_insights['optimization_opportunities'].append(\"Consider task batching for efficiency\")\n",
    "        \n",
    "        # Assess process consistency\n",
    "        consistency_score = self.assess_process_consistency(task_results)\n",
    "        process_insights['consistency_score'] = consistency_score\n",
    "        \n",
    "        if consistency_score < 0.7:\n",
    "            process_insights['optimization_opportunities'].append(\"Improve process standardization\")\n",
    "        \n",
    "        return process_insights\n",
    "    \n",
    "    def reflect_on_outcome(self, task_results):\n",
    "        \"\"\"Reflect on the outcomes and their alignment with goals\"\"\"\n",
    "        outcome_insights = {\n",
    "            'goal_alignment': self.assess_goal_alignment(task_results),\n",
    "            'quality_assessment': self.assess_outcome_quality(task_results),\n",
    "            'impact_evaluation': self.evaluate_impact(task_results),\n",
    "            'satisfaction_level': 'moderate'  # Simplified assessment\n",
    "        }\n",
    "        \n",
    "        # Assess whether outcomes met expectations\n",
    "        expected_outcomes = len(task_results)\n",
    "        actual_successful_outcomes = sum(1 for result in task_results if result.get('success', False))\n",
    "        \n",
    "        outcome_insights['expectation_alignment'] = actual_successful_outcomes / expected_outcomes\n",
    "        \n",
    "        return outcome_insights\n",
    "    \n",
    "    def reflect_on_learning(self, task_results):\n",
    "        \"\"\"Reflect on learning and knowledge acquisition\"\"\"\n",
    "        learning_insights = {\n",
    "            'new_knowledge_acquired': self.identify_new_knowledge(task_results),\n",
    "            'skills_developed': self.identify_skill_development(task_results),\n",
    "            'knowledge_gaps': self.identify_knowledge_gaps(task_results),\n",
    "            'learning_effectiveness': self.assess_learning_effectiveness(task_results)\n",
    "        }\n",
    "        \n",
    "        # Track learning progress\n",
    "        learning_insights['learning_trajectory'] = self.analyze_learning_trajectory()\n",
    "        \n",
    "        return learning_insights\n",
    "    \n",
    "    def categorize_performance(self, success_rate):\n",
    "        \"\"\"Categorize performance level\"\"\"\n",
    "        if success_rate >= 0.9:\n",
    "            return 'excellent'\n",
    "        elif success_rate >= 0.8:\n",
    "            return 'good'\n",
    "        elif success_rate >= 0.6:\n",
    "            return 'satisfactory'\n",
    "        elif success_rate >= 0.4:\n",
    "            return 'needs_improvement'\n",
    "        else:\n",
    "            return 'poor'\n",
    "    \n",
    "    def analyze_performance_trends(self):\n",
    "        \"\"\"Analyze performance trends over time\"\"\"\n",
    "        if len(self.reflection_history) < 2:\n",
    "            return {'trend': 'insufficient_data', 'direction': 'unknown'}\n",
    "        \n",
    "        recent_sessions = self.reflection_history[-3:]  # Last 3 sessions\n",
    "        success_rates = []\n",
    "        \n",
    "        for session in recent_sessions:\n",
    "            if 'performance' in session['insights']:\n",
    "                success_rates.append(session['insights']['performance']['success_rate'])\n",
    "        \n",
    "        if len(success_rates) >= 2:\n",
    "            if success_rates[-1] > success_rates[0]:\n",
    "                return {'trend': 'improving', 'direction': 'upward'}\n",
    "            elif success_rates[-1] < success_rates[0]:\n",
    "                return {'trend': 'declining', 'direction': 'downward'}\n",
    "            else:\n",
    "                return {'trend': 'stable', 'direction': 'flat'}\n",
    "        \n",
    "        return {'trend': 'unknown', 'direction': 'unknown'}\n",
    "    \n",
    "    def analyze_failures(self, failed_tasks):\n",
    "        \"\"\"Analyze patterns in failed tasks\"\"\"\n",
    "        failure_reasons = []\n",
    "        \n",
    "        for task in failed_tasks:\n",
    "            # Simulate failure reason analysis\n",
    "            possible_reasons = [\n",
    "                'insufficient_information',\n",
    "                'complexity_too_high',\n",
    "                'resource_constraints',\n",
    "                'external_dependencies',\n",
    "                'unclear_requirements'\n",
    "            ]\n",
    "            failure_reasons.append(random.choice(possible_reasons))\n",
    "        \n",
    "        # Count failure reason frequency\n",
    "        reason_counts = Counter(failure_reasons)\n",
    "        \n",
    "        return {\n",
    "            'total_failures': len(failed_tasks),\n",
    "            'common_reasons': dict(reason_counts.most_common(3)),\n",
    "            'failure_rate': len(failed_tasks) / (len(failed_tasks) + 10)  # Assuming some successful tasks\n",
    "        }\n",
    "    \n",
    "    def generate_action_items(self, insights):\n",
    "        \"\"\"Generate actionable improvement items based on insights\"\"\"\n",
    "        action_items = []\n",
    "        \n",
    "        # Performance-based actions\n",
    "        if 'performance' in insights:\n",
    "            perf = insights['performance']\n",
    "            if perf['success_rate'] < 0.7:\n",
    "                action_items.append({\n",
    "                    'category': 'performance',\n",
    "                    'action': 'Improve task completion strategies',\n",
    "                    'priority': 'high',\n",
    "                    'timeline': 'immediate'\n",
    "                })\n",
    "        \n",
    "        # Process-based actions\n",
    "        if 'process' in insights:\n",
    "            process = insights['process']\n",
    "            if process.get('consistency_score', 1.0) < 0.7:\n",
    "                action_items.append({\n",
    "                    'category': 'process',\n",
    "                    'action': 'Standardize task execution procedures',\n",
    "                    'priority': 'medium',\n",
    "                    'timeline': 'short_term'\n",
    "                })\n",
    "        \n",
    "        # Learning-based actions\n",
    "        if 'learning' in insights:\n",
    "            learning = insights['learning']\n",
    "            if learning.get('knowledge_gaps'):\n",
    "                action_items.append({\n",
    "                    'category': 'learning',\n",
    "                    'action': 'Address identified knowledge gaps',\n",
    "                    'priority': 'medium',\n",
    "                    'timeline': 'medium_term'\n",
    "                })\n",
    "        \n",
    "        return action_items\n",
    "    \n",
    "    def assess_reflection_confidence(self, reflection_session):\n",
    "        \"\"\"Assess confidence in the reflection analysis\"\"\"\n",
    "        # Simple confidence assessment based on data availability\n",
    "        data_points = len(reflection_session['task_results'])\n",
    "        insight_depth = len(reflection_session['insights'])\n",
    "        \n",
    "        confidence = min(1.0, (data_points * 0.1) + (insight_depth * 0.2))\n",
    "        return confidence\n",
    "    \n",
    "    # Simplified implementations for demonstration\n",
    "    def assess_process_efficiency(self, task_results):\n",
    "        return random.uniform(0.6, 0.9)\n",
    "    \n",
    "    def identify_bottlenecks(self, task_results):\n",
    "        return ['task_complexity', 'resource_availability']\n",
    "    \n",
    "    def assess_process_consistency(self, task_results):\n",
    "        return random.uniform(0.5, 0.9)\n",
    "    \n",
    "    def assess_goal_alignment(self, task_results):\n",
    "        return random.uniform(0.7, 0.95)\n",
    "    \n",
    "    def assess_outcome_quality(self, task_results):\n",
    "        return random.uniform(0.6, 0.9)\n",
    "    \n",
    "    def evaluate_impact(self, task_results):\n",
    "        return {'impact_level': 'moderate', 'stakeholder_satisfaction': 0.8}\n",
    "    \n",
    "    def identify_new_knowledge(self, task_results):\n",
    "        return ['domain_specific_insights', 'process_improvements']\n",
    "    \n",
    "    def identify_skill_development(self, task_results):\n",
    "        return ['problem_solving', 'analytical_thinking']\n",
    "    \n",
    "    def identify_knowledge_gaps(self, task_results):\n",
    "        return ['advanced_techniques', 'domain_expertise']\n",
    "    \n",
    "    def assess_learning_effectiveness(self, task_results):\n",
    "        return random.uniform(0.6, 0.9)\n",
    "    \n",
    "    def analyze_learning_trajectory(self):\n",
    "        return {'direction': 'positive', 'rate': 'steady'}\n",
    "\n",
    "# Initialize reflection system\n",
    "reflection_system = ReflectionSystem()\n",
    "print(\"Reflection system initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Reflection System\n",
    "\n",
    "Let's test the reflection system with simulated task results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate different task execution scenarios\n",
    "test_scenarios = [\n",
    "    {\n",
    "        'name': 'High Performance Scenario',\n",
    "        'task_results': [\n",
    "            {'success': True, 'message': 'Task 1 completed successfully'},\n",
    "            {'success': True, 'message': 'Task 2 completed successfully'},\n",
    "            {'success': True, 'message': 'Task 3 completed successfully'},\n",
    "            {'success': True, 'message': 'Task 4 completed successfully'},\n",
    "            {'success': False, 'message': 'Task 5 failed due to complexity'}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'name': 'Mixed Performance Scenario',\n",
    "        'task_results': [\n",
    "            {'success': True, 'message': 'Task 1 completed successfully'},\n",
    "            {'success': False, 'message': 'Task 2 failed - insufficient data'},\n",
    "            {'success': True, 'message': 'Task 3 completed successfully'},\n",
    "            {'success': False, 'message': 'Task 4 failed - resource constraints'},\n",
    "            {'success': True, 'message': 'Task 5 completed successfully'}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'name': 'Low Performance Scenario',\n",
    "        'task_results': [\n",
    "            {'success': False, 'message': 'Task 1 failed - unclear requirements'},\n",
    "            {'success': False, 'message': 'Task 2 failed - complexity too high'},\n",
    "            {'success': True, 'message': 'Task 3 completed successfully'},\n",
    "            {'success': False, 'message': 'Task 4 failed - external dependencies'},\n",
    "            {'success': False, 'message': 'Task 5 failed - insufficient information'}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "reflection_results = []\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"REFLECTION SESSION: {scenario['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Conduct reflection\n",
    "    reflection = reflection_system.conduct_reflection(\n",
    "        scenario['task_results'], \n",
    "        reflection_type='comprehensive'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nReflection completed with confidence: {reflection['confidence_assessment']:.2f}\")\n",
    "    \n",
    "    # Display performance insights\n",
    "    if 'performance' in reflection['insights']:\n",
    "        perf = reflection['insights']['performance']\n",
    "        print(f\"\\nPerformance Analysis:\")\n",
    "        print(f\"  Success Rate: {perf['success_rate']:.1%}\")\n",
    "        print(f\"  Performance Level: {perf['performance_level']}\")\n",
    "        print(f\"  Successful Tasks: {perf['successful_tasks']}/{perf['total_tasks']}\")\n",
    "        \n",
    "        if perf['strengths']:\n",
    "            print(f\"  Strengths: {', '.join(perf['strengths'])}\")\n",
    "        if perf['weaknesses']:\n",
    "            print(f\"  Weaknesses: {', '.join(perf['weaknesses'])}\")\n",
    "        \n",
    "        print(f\"  Trend: {perf['trends']['trend']} ({perf['trends']['direction']})\")\n",
    "    \n",
    "    # Display process insights\n",
    "    if 'process' in reflection['insights']:\n",
    "        process = reflection['insights']['process']\n",
    "        print(f\"\\nProcess Analysis:\")\n",
    "        print(f\"  Efficiency: {process['efficiency_assessment']:.2f}\")\n",
    "        print(f\"  Consistency: {process['consistency_score']:.2f}\")\n",
    "        print(f\"  Quality: {process['process_quality']}\")\n",
    "        \n",
    "        if process['optimization_opportunities']:\n",
    "            print(f\"  Optimization Opportunities:\")\n",
    "            for opp in process['optimization_opportunities']:\n",
    "                print(f\"    - {opp}\")\n",
    "    \n",
    "    # Display learning insights\n",
    "    if 'learning' in reflection['insights']:\n",
    "        learning = reflection['insights']['learning']\n",
    "        print(f\"\\nLearning Analysis:\")\n",
    "        print(f\"  Learning Effectiveness: {learning['learning_effectiveness']:.2f}\")\n",
    "        print(f\"  New Knowledge: {', '.join(learning['new_knowledge_acquired'])}\")\n",
    "        print(f\"  Skills Developed: {', '.join(learning['skills_developed'])}\")\n",
    "        print(f\"  Knowledge Gaps: {', '.join(learning['knowledge_gaps'])}\")\n",
    "    \n",
    "    # Display action items\n",
    "    print(f\"\\nAction Items ({len(reflection['action_items'])}):\") \n",
    "    for i, action in enumerate(reflection['action_items'], 1):\n",
    "        print(f\"  {i}. [{action['priority'].upper()}] {action['action']}\")\n",
    "        print(f\"     Category: {action['category']}, Timeline: {action['timeline']}\")\n",
    "    \n",
    "    # Store results for analysis\n",
    "    reflection_results.append({\n",
    "        'scenario': scenario['name'],\n",
    "        'success_rate': reflection['insights']['performance']['success_rate'],\n",
    "        'confidence': reflection['confidence_assessment'],\n",
    "        'action_items': len(reflection['action_items']),\n",
    "        'performance_level': reflection['insights']['performance']['performance_level']\n",
    "    })\n",
    "\n",
    "# Visualize reflection analysis\n",
    "df_reflections = pd.DataFrame(reflection_results)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Success rates by scenario\n",
    "plt.subplot(2, 3, 1)\n",
    "colors = ['green', 'orange', 'red']\n",
    "bars = plt.bar(range(len(df_reflections)), df_reflections['success_rate'], color=colors)\n",
    "plt.title('Success Rates by Scenario')\n",
    "plt.xlabel('Scenario')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.xticks(range(len(df_reflections)), ['High', 'Mixed', 'Low'], rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.1%}', ha='center', va='bottom')\n",
    "\n",
    "# Reflection confidence\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.bar(range(len(df_reflections)), df_reflections['confidence'], color=colors)\n",
    "plt.title('Reflection Confidence')\n",
    "plt.xlabel('Scenario')\n",
    "plt.ylabel('Confidence Score')\n",
    "plt.xticks(range(len(df_reflections)), ['High', 'Mixed', 'Low'], rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Action items generated\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.bar(range(len(df_reflections)), df_reflections['action_items'], color=colors)\n",
    "plt.title('Action Items Generated')\n",
    "plt.xlabel('Scenario')\n",
    "plt.ylabel('Number of Action Items')\n",
    "plt.xticks(range(len(df_reflections)), ['High', 'Mixed', 'Low'], rotation=45)\n",
    "\n",
    "# Performance level distribution\n",
    "plt.subplot(2, 3, 4)\n",
    "performance_levels = df_reflections['performance_level'].value_counts()\n",
    "plt.pie(performance_levels.values, labels=performance_levels.index, autopct='%1.0f%%')\n",
    "plt.title('Performance Level Distribution')\n",
    "\n",
    "# Correlation between success rate and confidence\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.scatter(df_reflections['success_rate'], df_reflections['confidence'], \n",
    "           c=colors, s=100, alpha=0.7)\n",
    "plt.xlabel('Success Rate')\n",
    "plt.ylabel('Reflection Confidence')\n",
    "plt.title('Success Rate vs Reflection Confidence')\n",
    "\n",
    "# Add scenario labels\n",
    "for i, row in df_reflections.iterrows():\n",
    "    plt.annotate(['High', 'Mixed', 'Low'][i], \n",
    "                (row['success_rate'], row['confidence']),\n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "# Reflection effectiveness over time\n",
    "plt.subplot(2, 3, 6)\n",
    "sessions = range(1, len(reflection_system.reflection_history) + 1)\n",
    "confidences = [session['confidence_assessment'] for session in reflection_system.reflection_history]\n",
    "plt.plot(sessions, confidences, marker='o', linewidth=2, markersize=6)\n",
    "plt.title('Reflection Confidence Over Time')\n",
    "plt.xlabel('Reflection Session')\n",
    "plt.ylabel('Confidence Score')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== REFLECTION SYSTEM ANALYSIS ===\")\n",
    "print(f\"Total reflection sessions: {len(reflection_system.reflection_history)}\")\n",
    "print(f\"Average reflection confidence: {np.mean([s['confidence_assessment'] for s in reflection_system.reflection_history]):.3f}\")\n",
    "print(f\"Average action items per session: {df_reflections['action_items'].mean():.1f}\")\n",
    "print(f\"Performance improvement correlation: {np.corrcoef(df_reflections['success_rate'], df_reflections['confidence'])[0,1]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hallucination and Reliability in Large Language Models\n",
    "\n",
    "## Overview\n",
    "\n",
    "Hallucination in LLMs refers to the generation of content that appears plausible but is factually incorrect, unsupported by training data, or inconsistent with provided context. This notebook covers:\n",
    "\n",
    "- **Hallucination Types**: Factual, contextual, and logical inconsistencies\n",
    "- **Detection Methods**: Pattern-based and ML-based detection systems\n",
    "- **Faithfulness Evaluation**: Measuring adherence to source information\n",
    "- **Reliability Assessment**: Comprehensive frameworks for trustworthiness\n",
    "\n",
    "Let's start by implementing practical hallucination detection systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Factual Hallucination Detection\n",
    "\n",
    "Let's implement a system to detect factual inconsistencies in generated text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactualHallucinationDetector:\n",
    "    \"\"\"Detect factual hallucinations in generated text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Simple knowledge base for demonstration\n",
    "        self.knowledge_base = {\n",
    "            'facts': {\n",
    "                'Paris is the capital of France': True,\n",
    "                'London is the capital of England': True,\n",
    "                'Tokyo is the capital of Japan': True,\n",
    "                'Berlin is the capital of Germany': True,\n",
    "                'Madrid is the capital of Spain': True,\n",
    "                'Rome is the capital of Italy': True,\n",
    "                'The Earth is flat': False,\n",
    "                'The moon is made of cheese': False,\n",
    "                'Water boils at 100°C at sea level': True,\n",
    "                'Humans have 10 fingers': True,\n",
    "                'There are 8 planets in our solar system': True,\n",
    "                'Shakespeare wrote Romeo and Juliet': True\n",
    "            },\n",
    "            'entities': {\n",
    "                'Paris': {'type': 'city', 'country': 'France'},\n",
    "                'London': {'type': 'city', 'country': 'England'},\n",
    "                'Tokyo': {'type': 'city', 'country': 'Japan'},\n",
    "                'Shakespeare': {'type': 'person', 'profession': 'playwright'},\n",
    "                'Earth': {'type': 'planet', 'system': 'Solar System'}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Factual claim patterns\n",
    "        self.claim_patterns = [\n",
    "            r'(.+) is the capital of (.+)',\n",
    "            r'(.+) wrote (.+)',\n",
    "            r'(.+) is (?:a|an) (.+)',\n",
    "            r'There are (\\d+) (.+)',\n",
    "            r'(.+) (?:boils|melts|freezes) at (.+)'\n",
    "        ]\n",
    "    \n",
    "    def detect_factual_errors(self, generated_text, context=None):\n",
    "        \"\"\"Detect factual inconsistencies in generated text\"\"\"\n",
    "        claims = self.extract_claims(generated_text)\n",
    "        errors = []\n",
    "        \n",
    "        for claim in claims:\n",
    "            # Check against knowledge base\n",
    "            kb_result = self.verify_against_kb(claim)\n",
    "            \n",
    "            if kb_result['status'] == 'FALSE':\n",
    "                errors.append({\n",
    "                    'claim': claim,\n",
    "                    'type': 'factual_contradiction',\n",
    "                    'confidence': kb_result['confidence'],\n",
    "                    'explanation': kb_result['explanation']\n",
    "                })\n",
    "            \n",
    "            # Cross-reference with context if provided\n",
    "            if context and not self.is_supported_by_context(claim, context):\n",
    "                errors.append({\n",
    "                    'claim': claim,\n",
    "                    'type': 'context_unsupported',\n",
    "                    'severity': 'high',\n",
    "                    'explanation': 'Claim not supported by provided context'\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            'total_claims': len(claims),\n",
    "            'factual_errors': errors,\n",
    "            'error_rate': len(errors) / len(claims) if claims else 0,\n",
    "            'reliability_score': 1 - (len(errors) / len(claims)) if claims else 1.0\n",
    "        }\n",
    "    \n",
    "    def extract_claims(self, text):\n",
    "        \"\"\"Extract verifiable claims from text\"\"\"\n",
    "        claims = []\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Check if sentence contains factual claims\n",
    "            if self.contains_factual_claim(sentence):\n",
    "                claims.append({\n",
    "                    'text': sentence.strip(),\n",
    "                    'entities': self.extract_entities(sentence),\n",
    "                    'claim_type': self.classify_claim_type(sentence)\n",
    "                })\n",
    "        \n",
    "        return claims\n",
    "    \n",
    "    def contains_factual_claim(self, sentence):\n",
    "        \"\"\"Check if sentence contains a factual claim\"\"\"\n",
    "        # Look for definitive statements\n",
    "        definitive_patterns = [\n",
    "            r'\\bis\\b', r'\\bare\\b', r'\\bwas\\b', r'\\bwere\\b',\n",
    "            r'\\bhas\\b', r'\\bhave\\b', r'\\bwrote\\b', r'\\binvented\\b',\n",
    "            r'\\bdiscovered\\b', r'\\bfounded\\b'\n",
    "        ]\n",
    "        \n",
    "        return any(re.search(pattern, sentence.lower()) for pattern in definitive_patterns)\n",
    "    \n",
    "    def extract_entities(self, sentence):\n",
    "        \"\"\"Extract entities from sentence (simplified)\"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        # Simple entity extraction based on capitalization\n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            clean_word = re.sub(r'[^\\w]', '', word)\n",
    "            if clean_word.istitle() and len(clean_word) > 1:\n",
    "                if clean_word in self.knowledge_base['entities']:\n",
    "                    entities.append({\n",
    "                        'text': clean_word,\n",
    "                        'info': self.knowledge_base['entities'][clean_word]\n",
    "                    })\n",
    "                else:\n",
    "                    entities.append({'text': clean_word, 'info': None})\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def classify_claim_type(self, sentence):\n",
    "        \"\"\"Classify the type of claim\"\"\"\n",
    "        sentence_lower = sentence.lower()\n",
    "        \n",
    "        if 'capital' in sentence_lower:\n",
    "            return 'geographical'\n",
    "        elif any(word in sentence_lower for word in ['wrote', 'invented', 'discovered']):\n",
    "            return 'attribution'\n",
    "        elif any(word in sentence_lower for word in ['is', 'are', 'was', 'were']):\n",
    "            return 'definitional'\n",
    "        else:\n",
    "            return 'general'\n",
    "    \n",
    "    def verify_against_kb(self, claim):\n",
    "        \"\"\"Verify claim against knowledge base\"\"\"\n",
    "        claim_text = claim['text'].strip('.')\n",
    "        \n",
    "        # Direct lookup\n",
    "        if claim_text in self.knowledge_base['facts']:\n",
    "            is_true = self.knowledge_base['facts'][claim_text]\n",
    "            return {\n",
    "                'status': 'TRUE' if is_true else 'FALSE',\n",
    "                'confidence': 0.95,\n",
    "                'explanation': f\"Direct match in knowledge base: {is_true}\"\n",
    "            }\n",
    "        \n",
    "        # Pattern-based verification\n",
    "        for pattern in self.claim_patterns:\n",
    "            match = re.search(pattern, claim_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                return self.verify_pattern_match(pattern, match, claim_text)\n",
    "        \n",
    "        # Unknown claim\n",
    "        return {\n",
    "            'status': 'UNKNOWN',\n",
    "            'confidence': 0.0,\n",
    "            'explanation': 'Claim not found in knowledge base'\n",
    "        }\n",
    "    \n",
    "    def verify_pattern_match(self, pattern, match, claim_text):\n",
    "        \"\"\"Verify pattern-based matches\"\"\"\n",
    "        if 'capital' in pattern:\n",
    "            city, country = match.groups()\n",
    "            # Check if this capital relationship is correct\n",
    "            correct_fact = f\"{city.strip()} is the capital of {country.strip()}\"\n",
    "            if correct_fact in self.knowledge_base['facts']:\n",
    "                is_correct = self.knowledge_base['facts'][correct_fact]\n",
    "                return {\n",
    "                    'status': 'TRUE' if is_correct else 'FALSE',\n",
    "                    'confidence': 0.9,\n",
    "                    'explanation': f\"Capital relationship verified: {is_correct}\"\n",
    "                }\n",
    "        \n",
    "        return {\n",
    "            'status': 'UNKNOWN',\n",
    "            'confidence': 0.3,\n",
    "            'explanation': 'Pattern matched but not verified'\n",
    "        }\n",
    "    \n",
    "    def is_supported_by_context(self, claim, context):\n",
    "        \"\"\"Check if claim is supported by context\"\"\"\n",
    "        claim_text = claim['text'].lower()\n",
    "        context_lower = context.lower()\n",
    "        \n",
    "        # Simple keyword overlap check\n",
    "        claim_words = set(re.findall(r'\\w+', claim_text))\n",
    "        context_words = set(re.findall(r'\\w+', context_lower))\n",
    "        \n",
    "        overlap = len(claim_words.intersection(context_words))\n",
    "        overlap_ratio = overlap / len(claim_words) if claim_words else 0\n",
    "        \n",
    "        return overlap_ratio > 0.5  # At least 50% word overlap\n",
    "\n",
    "# Initialize the detector\n",
    "hallucination_detector = FactualHallucinationDetector()\n",
    "print(\"Factual hallucination detector initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Factual Hallucination Detection\n",
    "\n",
    "Let's test our detector with various examples containing both correct and incorrect facts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases with factual and hallucinated content\n",
    "test_texts = [\n",
    "    {\n",
    "        'text': 'Paris is the capital of France. London is the capital of England. Shakespeare wrote Romeo and Juliet.',\n",
    "        'description': 'All correct facts',\n",
    "        'expected_errors': 0\n",
    "    },\n",
    "    {\n",
    "        'text': 'Paris is the capital of Germany. Tokyo is the capital of China. The Earth is flat.',\n",
    "        'description': 'Multiple factual errors',\n",
    "        'expected_errors': 3\n",
    "    },\n",
    "    {\n",
    "        'text': 'Berlin is the capital of Germany. Water boils at 100°C at sea level. The moon is made of cheese.',\n",
    "        'description': 'Mixed correct and incorrect facts',\n",
    "        'expected_errors': 1\n",
    "    },\n",
    "    {\n",
    "        'text': 'I think machine learning is interesting. This might be a good approach. Perhaps we should consider this.',\n",
    "        'description': 'Opinions and uncertain statements',\n",
    "        'expected_errors': 0\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Testing factual hallucination detection...\\n\")\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "for i, test_case in enumerate(test_texts, 1):\n",
    "    print(f\"Test {i}: {test_case['description']}\")\n",
    "    print(f\"Text: {test_case['text']}\")\n",
    "    \n",
    "    # Run detection\n",
    "    result = hallucination_detector.detect_factual_errors(test_case['text'])\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Total claims extracted: {result['total_claims']}\")\n",
    "    print(f\"  Factual errors detected: {len(result['factual_errors'])}\")\n",
    "    print(f\"  Error rate: {result['error_rate']:.2%}\")\n",
    "    print(f\"  Reliability score: {result['reliability_score']:.3f}\")\n",
    "    \n",
    "    # Show detected errors\n",
    "    if result['factual_errors']:\n",
    "        print(f\"\\n  Detected errors:\")\n",
    "        for j, error in enumerate(result['factual_errors'], 1):\n",
    "            print(f\"    {j}. {error['claim']['text']}\")\n",
    "            print(f\"       Type: {error['type']}\")\n",
    "            print(f\"       Confidence: {error['confidence']:.2f}\")\n",
    "            print(f\"       Explanation: {error['explanation']}\")\n",
    "    \n",
    "    # Check accuracy\n",
    "    detected_errors = len(result['factual_errors'])\n",
    "    expected_errors = test_case['expected_errors']\n",
    "    accuracy = \"✓\" if detected_errors == expected_errors else \"✗\"\n",
    "    \n",
    "    print(f\"\\n  Expected errors: {expected_errors}\")\n",
    "    print(f\"  Detection accuracy: {accuracy}\")\n",
    "    \n",
    "    results_summary.append({\n",
    "        'test': i,\n",
    "        'detected': detected_errors,\n",
    "        'expected': expected_errors,\n",
    "        'correct': detected_errors == expected_errors,\n",
    "        'reliability_score': result['reliability_score']\n",
    "    })\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# Summary statistics\n",
    "correct_detections = sum(1 for r in results_summary if r['correct'])\n",
    "total_tests = len(results_summary)\n",
    "avg_reliability = np.mean([r['reliability_score'] for r in results_summary])\n",
    "\n",
    "print(f\"\\n=== DETECTION SUMMARY ===\")\n",
    "print(f\"Correct detections: {correct_detections}/{total_tests} ({correct_detections/total_tests:.1%})\")\n",
    "print(f\"Average reliability score: {avg_reliability:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Contextual Consistency Checker\n",
    "\n",
    "Let's implement a system to check if generated text is consistent with provided context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualConsistencyChecker:\n",
    "    \"\"\"Check contextual consistency of generated text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "        \n",
    "        # Contradiction indicators\n",
    "        self.contradiction_patterns = [\n",
    "            r'\\b(?:not|never|no|none|neither)\\b',\n",
    "            r'\\b(?:but|however|although|despite)\\b',\n",
    "            r'\\b(?:opposite|contrary|different)\\b',\n",
    "            r'\\b(?:wrong|incorrect|false)\\b'\n",
    "        ]\n",
    "    \n",
    "    def check_contextual_consistency(self, context, generated_text):\n",
    "        \"\"\"Check if generated text is consistent with context\"\"\"\n",
    "        # Split generated text into sentences\n",
    "        gen_sentences = nltk.sent_tokenize(generated_text)\n",
    "        \n",
    "        inconsistencies = []\n",
    "        consistency_scores = []\n",
    "        \n",
    "        for sentence in gen_sentences:\n",
    "            # Calculate semantic similarity\n",
    "            similarity_score = self.calculate_semantic_similarity(context, sentence)\n",
    "            \n",
    "            # Check for explicit contradictions\n",
    "            contradiction_score = self.detect_contradictions(context, sentence)\n",
    "            \n",
    "            # Overall consistency score\n",
    "            consistency_score = similarity_score - contradiction_score\n",
    "            consistency_scores.append(consistency_score)\n",
    "            \n",
    "            # Flag inconsistencies\n",
    "            if consistency_score < 0.3:  # Low consistency threshold\n",
    "                inconsistencies.append({\n",
    "                    'sentence': sentence,\n",
    "                    'type': 'low_consistency',\n",
    "                    'similarity_score': similarity_score,\n",
    "                    'contradiction_score': contradiction_score,\n",
    "                    'consistency_score': consistency_score\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            'inconsistencies': inconsistencies,\n",
    "            'overall_consistency': np.mean(consistency_scores) if consistency_scores else 0,\n",
    "            'consistency_distribution': {\n",
    "                'high': sum(1 for s in consistency_scores if s > 0.7),\n",
    "                'medium': sum(1 for s in consistency_scores if 0.3 <= s <= 0.7),\n",
    "                'low': sum(1 for s in consistency_scores if s < 0.3)\n",
    "            },\n",
    "            'total_sentences': len(gen_sentences)\n",
    "        }\n",
    "    \n",
    "    def calculate_semantic_similarity(self, context, sentence):\n",
    "        \"\"\"Calculate semantic similarity using TF-IDF\"\"\"\n",
    "        try:\n",
    "            # Fit vectorizer on both texts\n",
    "            tfidf_matrix = self.vectorizer.fit_transform([context, sentence])\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "            \n",
    "            return similarity\n",
    "        except:\n",
    "            # Fallback to simple word overlap\n",
    "            context_words = set(re.findall(r'\\w+', context.lower()))\n",
    "            sentence_words = set(re.findall(r'\\w+', sentence.lower()))\n",
    "            \n",
    "            if not sentence_words:\n",
    "                return 0\n",
    "            \n",
    "            overlap = len(context_words.intersection(sentence_words))\n",
    "            return overlap / len(sentence_words)\n",
    "    \n",
    "    def detect_contradictions(self, context, sentence):\n",
    "        \"\"\"Detect explicit contradictions\"\"\"\n",
    "        contradiction_score = 0\n",
    "        \n",
    "        # Check for contradiction patterns in sentence\n",
    "        sentence_lower = sentence.lower()\n",
    "        for pattern in self.contradiction_patterns:\n",
    "            if re.search(pattern, sentence_lower):\n",
    "                contradiction_score += 0.2\n",
    "        \n",
    "        # Check for negation of context content\n",
    "        context_words = set(re.findall(r'\\w+', context.lower()))\n",
    "        \n",
    "        # Look for negated versions of context words\n",
    "        negation_patterns = [r'not ' + word for word in list(context_words)[:10]]  # Limit for performance\n",
    "        \n",
    "        for pattern in negation_patterns:\n",
    "            if re.search(pattern, sentence_lower):\n",
    "                contradiction_score += 0.3\n",
    "        \n",
    "        return min(contradiction_score, 1.0)  # Cap at 1.0\n",
    "    \n",
    "    def analyze_information_flow(self, context, generated_text):\n",
    "        \"\"\"Analyze how information flows from context to generated text\"\"\"\n",
    "        context_sentences = nltk.sent_tokenize(context)\n",
    "        gen_sentences = nltk.sent_tokenize(generated_text)\n",
    "        \n",
    "        # Track which context sentences are referenced\n",
    "        context_usage = []\n",
    "        \n",
    "        for i, ctx_sent in enumerate(context_sentences):\n",
    "            max_similarity = 0\n",
    "            best_match = None\n",
    "            \n",
    "            for j, gen_sent in enumerate(gen_sentences):\n",
    "                similarity = self.calculate_semantic_similarity(ctx_sent, gen_sent)\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    best_match = j\n",
    "            \n",
    "            context_usage.append({\n",
    "                'context_sentence_id': i,\n",
    "                'context_sentence': ctx_sent,\n",
    "                'best_match_id': best_match,\n",
    "                'best_match_sentence': gen_sentences[best_match] if best_match is not None else None,\n",
    "                'similarity_score': max_similarity,\n",
    "                'is_used': max_similarity > 0.3\n",
    "            })\n",
    "        \n",
    "        used_context = sum(1 for usage in context_usage if usage['is_used'])\n",
    "        context_coverage = used_context / len(context_sentences) if context_sentences else 0\n",
    "        \n",
    "        return {\n",
    "            'context_usage': context_usage,\n",
    "            'context_coverage': context_coverage,\n",
    "            'unused_context_sentences': len(context_sentences) - used_context\n",
    "        }\n",
    "\n",
    "# Initialize the consistency checker\n",
    "consistency_checker = ContextualConsistencyChecker()\n",
    "print(\"Contextual consistency checker initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Contextual Consistency\n",
    "\n",
    "Let's test the consistency checker with various context-generation pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for contextual consistency\n",
    "consistency_tests = [\n",
    "    {\n",
    "        'context': 'Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It uses algorithms to identify patterns and make predictions.',\n",
    "        'generated': 'Machine learning allows computers to learn from data and identify patterns. This technology is used for making predictions and is part of artificial intelligence.',\n",
    "        'description': 'Consistent with context',\n",
    "        'expected_consistency': 'high'\n",
    "    },\n",
    "    {\n",
    "        'context': 'Python is a high-level programming language known for its simplicity and readability. It was created by Guido van Rossum in 1991.',\n",
    "        'generated': 'Python is a low-level programming language that is very complex to use. It was invented by John Smith in 2005.',\n",
    "        'description': 'Contradicts context facts',\n",
    "        'expected_consistency': 'low'\n",
    "    },\n",
    "    {\n",
    "        'context': 'Climate change refers to long-term shifts in global temperatures and weather patterns. Human activities are the primary driver of recent climate change.',\n",
    "        'generated': 'Weather can vary from day to day. Some regions experience more rainfall than others. Temperature fluctuations are normal.',\n",
    "        'description': 'Related but not directly addressing context',\n",
    "        'expected_consistency': 'medium'\n",
    "    },\n",
    "    {\n",
    "        'context': 'The Renaissance was a period of cultural rebirth in Europe from the 14th to 17th centuries. It marked a transition from medieval to modern times.',\n",
    "        'generated': 'Quantum physics deals with the behavior of matter at the atomic level. Particles can exist in multiple states simultaneously.',\n",
    "        'description': 'Completely unrelated topic',\n",
    "        'expected_consistency': 'low'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Testing contextual consistency...\\n\")\n",
    "\n",
    "for i, test_case in enumerate(consistency_tests, 1):\n",
    "    print(f\"Test {i}: {test_case['description']}\")\n",
    "    print(f\"Context: {test_case['context'][:100]}...\")\n",
    "    print(f\"Generated: {test_case['generated'][:100]}...\")\n",
    "    \n",
    "    # Run consistency check\n",
    "    result = consistency_checker.check_contextual_consistency(\n",
    "        test_case['context'], \n",
    "        test_case['generated']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nConsistency Analysis:\")\n",
    "    print(f\"  Overall consistency: {result['overall_consistency']:.3f}\")\n",
    "    print(f\"  Total sentences: {result['total_sentences']}\")\n",
    "    print(f\"  Inconsistencies found: {len(result['inconsistencies'])}\")\n",
    "    \n",
    "    # Show consistency distribution\n",
    "    dist = result['consistency_distribution']\n",
    "    print(f\"  Consistency distribution:\")\n",
    "    print(f\"    High: {dist['high']} sentences\")\n",
    "    print(f\"    Medium: {dist['medium']} sentences\")\n",
    "    print(f\"    Low: {dist['low']} sentences\")\n",
    "    \n",
    "    # Show inconsistencies if any\n",
    "    if result['inconsistencies']:\n",
    "        print(f\"\\n  Detected inconsistencies:\")\n",
    "        for j, inconsistency in enumerate(result['inconsistencies'], 1):\n",
    "            print(f\"    {j}. {inconsistency['sentence'][:80]}...\")\n",
    "            print(f\"       Consistency score: {inconsistency['consistency_score']:.3f}\")\n",
    "    \n",
    "    # Analyze information flow\n",
    "    flow_analysis = consistency_checker.analyze_information_flow(\n",
    "        test_case['context'], \n",
    "        test_case['generated']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n  Information Flow:\")\n",
    "    print(f\"    Context coverage: {flow_analysis['context_coverage']:.2%}\")\n",
    "    print(f\"    Unused context sentences: {flow_analysis['unused_context_sentences']}\")\n",
    "    \n",
    "    # Determine consistency level\n",
    "    if result['overall_consistency'] > 0.7:\n",
    "        consistency_level = 'high'\n",
    "    elif result['overall_consistency'] > 0.3:\n",
    "        consistency_level = 'medium'\n",
    "    else:\n",
    "        consistency_level = 'low'\n",
    "    \n",
    "    expected = test_case['expected_consistency']\n",
    "    accuracy = \"✓\" if consistency_level == expected else \"✗\"\n",
    "    \n",
    "    print(f\"\\n  Expected: {expected}, Detected: {consistency_level} {accuracy}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comprehensive Reliability Assessment\n",
    "\n",
    "Let's create a comprehensive framework that combines multiple reliability measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReliabilityAssessmentFramework:\n",
    "    \"\"\"Comprehensive reliability assessment for LLM outputs\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.hallucination_detector = FactualHallucinationDetector()\n",
    "        self.consistency_checker = ContextualConsistencyChecker()\n",
    "        self.uncertainty_estimator = UncertaintyEstimator()\n",
    "        \n",
    "        # Reliability weights\n",
    "        self.weights = {\n",
    "            'factual_accuracy': 0.35,\n",
    "            'contextual_consistency': 0.25,\n",
    "            'logical_coherence': 0.20,\n",
    "            'uncertainty_calibration': 0.20\n",
    "        }\n",
    "    \n",
    "    def assess_reliability(self, generated_text, context=None, sources=None):\n",
    "        \"\"\"Comprehensive reliability assessment\"\"\"\n",
    "        assessment = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'text_length': len(generated_text),\n",
    "            'factual_analysis': {},\n",
    "            'consistency_analysis': {},\n",
    "            'coherence_analysis': {},\n",
    "            'uncertainty_analysis': {},\n",
    "            'overall_reliability': 0.0,\n",
    "            'reliability_grade': 'F',\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # 1. Factual Analysis\n",
    "        factual_result = self.hallucination_detector.detect_factual_errors(generated_text, context)\n",
    "        assessment['factual_analysis'] = factual_result\n",
    "        \n",
    "        # 2. Contextual Consistency (if context provided)\n",
    "        if context:\n",
    "            consistency_result = self.consistency_checker.check_contextual_consistency(context, generated_text)\n",
    "            assessment['consistency_analysis'] = consistency_result\n",
    "        else:\n",
    "            assessment['consistency_analysis'] = {'overall_consistency': 0.5}  # Neutral score\n",
    "        \n",
    "        # 3. Logical Coherence\n",
    "        coherence_result = self.analyze_logical_coherence(generated_text)\n",
    "        assessment['coherence_analysis'] = coherence_result\n",
    "        \n",
    "        # 4. Uncertainty Analysis\n",
    "        uncertainty_result = self.uncertainty_estimator.estimate_uncertainty(generated_text)\n",
    "        assessment['uncertainty_analysis'] = uncertainty_result\n",
    "        \n",
    "        # 5. Calculate Overall Reliability\n",
    "        assessment['overall_reliability'] = self.calculate_overall_reliability(assessment)\n",
    "        assessment['reliability_grade'] = self.get_reliability_grade(assessment['overall_reliability'])\n",
    "        \n",
    "        # 6. Generate Recommendations\n",
    "        assessment['recommendations'] = self.generate_recommendations(assessment)\n",
    "        \n",
    "        return assessment\n",
    "    \n",
    "    def analyze_logical_coherence(self, text):\n",
    "        \"\"\"Analyze logical coherence of the text\"\"\"\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        \n",
    "        coherence_scores = []\n",
    "        contradictions = []\n",
    "        \n",
    "        # Check sentence-to-sentence coherence\n",
    "        for i in range(len(sentences) - 1):\n",
    "            current_sent = sentences[i]\n",
    "            next_sent = sentences[i + 1]\n",
    "            \n",
    "            # Calculate semantic similarity between adjacent sentences\n",
    "            similarity = self.consistency_checker.calculate_semantic_similarity(current_sent, next_sent)\n",
    "            coherence_scores.append(similarity)\n",
    "            \n",
    "            # Check for contradictions\n",
    "            contradiction_score = self.consistency_checker.detect_contradictions(current_sent, next_sent)\n",
    "            if contradiction_score > 0.5:\n",
    "                contradictions.append({\n",
    "                    'sentence1': current_sent,\n",
    "                    'sentence2': next_sent,\n",
    "                    'contradiction_score': contradiction_score\n",
    "                })\n",
    "        \n",
    "        # Check for logical flow indicators\n",
    "        flow_indicators = self.count_flow_indicators(text)\n",
    "        \n",
    "        avg_coherence = np.mean(coherence_scores) if coherence_scores else 0.5\n",
    "        \n",
    "        return {\n",
    "            'average_coherence': avg_coherence,\n",
    "            'coherence_scores': coherence_scores,\n",
    "            'contradictions': contradictions,\n",
    "            'flow_indicators': flow_indicators,\n",
    "            'coherence_grade': self.grade_coherence(avg_coherence, len(contradictions))\n",
    "        }\n",
    "    \n",
    "    def count_flow_indicators(self, text):\n",
    "        \"\"\"Count logical flow indicators in text\"\"\"\n",
    "        flow_patterns = {\n",
    "            'sequence': [r'\\bfirst\\b', r'\\bsecond\\b', r'\\bthen\\b', r'\\bfinally\\b', r'\\bnext\\b'],\n",
    "            'causation': [r'\\bbecause\\b', r'\\btherefore\\b', r'\\bthus\\b', r'\\bso\\b', r'\\bas a result\\b'],\n",
    "            'contrast': [r'\\bhowever\\b', r'\\bbut\\b', r'\\balthough\\b', r'\\bnevertheless\\b'],\n",
    "            'addition': [r'\\bmoreover\\b', r'\\bfurthermore\\b', r'\\badditionally\\b', r'\\balso\\b']\n",
    "        }\n",
    "        \n",
    "        indicators = {}\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for category, patterns in flow_patterns.items():\n",
    "            count = sum(len(re.findall(pattern, text_lower)) for pattern in patterns)\n",
    "            indicators[category] = count\n",
    "        \n",
    "        indicators['total'] = sum(indicators.values())\n",
    "        return indicators\n",
    "    \n",
    "    def grade_coherence(self, avg_coherence, contradiction_count):\n",
    "        \"\"\"Grade logical coherence\"\"\"\n",
    "        # Penalize for contradictions\n",
    "        penalty = contradiction_count * 0.1\n",
    "        adjusted_score = max(0, avg_coherence - penalty)\n",
    "        \n",
    "        if adjusted_score > 0.8:\n",
    "            return 'A'\n",
    "        elif adjusted_score > 0.6:\n",
    "            return 'B'\n",
    "        elif adjusted_score > 0.4:\n",
    "            return 'C'\n",
    "        elif adjusted_score > 0.2:\n",
    "            return 'D'\n",
    "        else:\n",
    "            return 'F'\n",
    "    \n",
    "    def calculate_overall_reliability(self, assessment):\n",
    "        \"\"\"Calculate weighted overall reliability score\"\"\"\n",
    "        scores = {\n",
    "            'factual_accuracy': assessment['factual_analysis']['reliability_score'],\n",
    "            'contextual_consistency': assessment['consistency_analysis']['overall_consistency'],\n",
    "            'logical_coherence': assessment['coherence_analysis']['average_coherence'],\n",
    "            'uncertainty_calibration': 1.0 - assessment['uncertainty_analysis']['overall_uncertainty']\n",
    "        }\n",
    "        \n",
    "        overall_score = sum(scores[component] * self.weights[component] for component in scores)\n",
    "        return max(0, min(1, overall_score))\n",
    "    \n",
    "    def get_reliability_grade(self, score):\n",
    "        \"\"\"Convert reliability score to letter grade\"\"\"\n",
    "        if score >= 0.9:\n",
    "            return 'A+'\n",
    "        elif score >= 0.8:\n",
    "            return 'A'\n",
    "        elif score >= 0.7:\n",
    "            return 'B'\n",
    "        elif score >= 0.6:\n",
    "            return 'C'\n",
    "        elif score >= 0.5:\n",
    "            return 'D'\n",
    "        else:\n",
    "            return 'F'\n",
    "    \n",
    "    def generate_recommendations(self, assessment):\n",
    "        \"\"\"Generate improvement recommendations\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Factual accuracy recommendations\n",
    "        if assessment['factual_analysis']['reliability_score'] < 0.7:\n",
    "            recommendations.append(\"Improve factual accuracy by verifying claims against reliable sources\")\n",
    "        \n",
    "        # Consistency recommendations\n",
    "        if assessment['consistency_analysis']['overall_consistency'] < 0.6:\n",
    "            recommendations.append(\"Enhance contextual consistency by staying closer to provided information\")\n",
    "        \n",
    "        # Coherence recommendations\n",
    "        if assessment['coherence_analysis']['average_coherence'] < 0.5:\n",
    "            recommendations.append(\"Improve logical flow with better transitions and structure\")\n",
    "        \n",
    "        # Uncertainty recommendations\n",
    "        if assessment['uncertainty_analysis']['overall_uncertainty'] > 0.7:\n",
    "            recommendations.append(\"Reduce uncertainty by providing more confident and specific statements\")\n",
    "        \n",
    "        if not recommendations:\n",
    "            recommendations.append(\"Overall reliability is good - maintain current quality standards\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "class UncertaintyEstimator:\n",
    "    \"\"\"Estimate uncertainty in generated text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.uncertainty_indicators = [\n",
    "            r'\\b(?:maybe|perhaps|possibly|probably|likely|might|could|may)\\b',\n",
    "            r'\\b(?:i think|i believe|i guess|it seems|appears to be)\\b',\n",
    "            r'\\b(?:uncertain|unsure|unclear|ambiguous)\\b',\n",
    "            r'\\?',  # Question marks\n",
    "            r'\\b(?:approximately|roughly|about|around)\\b'\n",
    "        ]\n",
    "    \n",
    "    def estimate_uncertainty(self, text):\n",
    "        \"\"\"Estimate overall uncertainty in text\"\"\"\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        sentence_uncertainties = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            uncertainty_score = self.calculate_sentence_uncertainty(sentence)\n",
    "            sentence_uncertainties.append(uncertainty_score)\n",
    "        \n",
    "        overall_uncertainty = np.mean(sentence_uncertainties) if sentence_uncertainties else 0\n",
    "        \n",
    "        return {\n",
    "            'overall_uncertainty': overall_uncertainty,\n",
    "            'sentence_uncertainties': sentence_uncertainties,\n",
    "            'uncertainty_distribution': self.categorize_uncertainties(sentence_uncertainties),\n",
    "            'confidence_level': 1.0 - overall_uncertainty\n",
    "        }\n",
    "    \n",
    "    def calculate_sentence_uncertainty(self, sentence):\n",
    "        \"\"\"Calculate uncertainty score for a single sentence\"\"\"\n",
    "        uncertainty_count = 0\n",
    "        sentence_lower = sentence.lower()\n",
    "        \n",
    "        for pattern in self.uncertainty_indicators:\n",
    "            matches = len(re.findall(pattern, sentence_lower))\n",
    "            uncertainty_count += matches\n",
    "        \n",
    "        # Normalize by sentence length (word count)\n",
    "        word_count = len(sentence.split())\n",
    "        if word_count == 0:\n",
    "            return 0\n",
    "        \n",
    "        uncertainty_ratio = uncertainty_count / word_count\n",
    "        return min(uncertainty_ratio * 5, 1.0)  # Scale and cap at 1.0\n",
    "    \n",
    "    def categorize_uncertainties(self, uncertainties):\n",
    "        \"\"\"Categorize uncertainty levels\"\"\"\n",
    "        return {\n",
    "            'high': sum(1 for u in uncertainties if u > 0.7),\n",
    "            'medium': sum(1 for u in uncertainties if 0.3 <= u <= 0.7),\n",
    "            'low': sum(1 for u in uncertainties if u < 0.3)\n",
    "        }\n",
    "\n",
    "# Initialize the comprehensive framework\n",
    "reliability_framework = ReliabilityAssessmentFramework()\n",
    "print(\"Comprehensive reliability assessment framework initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Comprehensive Reliability Assessment\n",
    "\n",
    "Let's test our comprehensive framework with different types of generated text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for comprehensive reliability assessment\n",
    "reliability_tests = [\n",
    "    {\n",
    "        'context': 'Artificial intelligence is transforming healthcare through machine learning algorithms that can analyze medical images and assist in diagnosis.',\n",
    "        'generated': 'AI is revolutionizing healthcare by using machine learning to analyze medical images. These algorithms help doctors make more accurate diagnoses. This technology improves patient outcomes and reduces diagnostic errors.',\n",
    "        'description': 'High-quality, consistent response'\n",
    "    },\n",
    "    {\n",
    "        'context': 'Python is a programming language created by Guido van Rossum in 1991.',\n",
    "        'generated': 'Python was invented by John Smith in 2005. It is a very difficult language to learn. Maybe it is used for web development, but I am not sure. The syntax might be complex.',\n",
    "        'description': 'Factually incorrect and uncertain'\n",
    "    },\n",
    "    {\n",
    "        'context': 'Climate change is caused by greenhouse gas emissions from human activities.',\n",
    "        'generated': 'Climate change is a natural phenomenon. However, human activities also contribute to it. But then again, it might be entirely natural. Scientists are probably wrong about greenhouse gases.',\n",
    "        'description': 'Self-contradictory and inconsistent'\n",
    "    },\n",
    "    {\n",
    "        'context': 'The Renaissance was a period of cultural and artistic rebirth in Europe.',\n",
    "        'generated': 'The Renaissance marked a significant cultural revival in Europe. This period saw remarkable achievements in art, literature, and science. Artists like Leonardo da Vinci and Michelangelo created masterpieces that continue to inspire us today.',\n",
    "        'description': 'Coherent and well-structured'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Testing comprehensive reliability assessment...\\n\")\n",
    "\n",
    "assessment_results = []\n",
    "\n",
    "for i, test_case in enumerate(reliability_tests, 1):\n",
    "    print(f\"Test {i}: {test_case['description']}\")\n",
    "    print(f\"Context: {test_case['context'][:80]}...\")\n",
    "    print(f\"Generated: {test_case['generated'][:80]}...\")\n",
    "    \n",
    "    # Run comprehensive assessment\n",
    "    assessment = reliability_framework.assess_reliability(\n",
    "        test_case['generated'], \n",
    "        test_case['context']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== RELIABILITY ASSESSMENT ===\")\n",
    "    print(f\"Overall Reliability: {assessment['overall_reliability']:.3f} (Grade: {assessment['reliability_grade']})\")\n",
    "    \n",
    "    # Component scores\n",
    "    print(f\"\\nComponent Analysis:\")\n",
    "    print(f\"  Factual Accuracy: {assessment['factual_analysis']['reliability_score']:.3f}\")\n",
    "    print(f\"  Contextual Consistency: {assessment['consistency_analysis']['overall_consistency']:.3f}\")\n",
    "    print(f\"  Logical Coherence: {assessment['coherence_analysis']['average_coherence']:.3f} (Grade: {assessment['coherence_analysis']['coherence_grade']})\")\n",
    "    print(f\"  Confidence Level: {assessment['uncertainty_analysis']['confidence_level']:.3f}\")\n",
    "    \n",
    "    # Issues found\n",
    "    factual_errors = len(assessment['factual_analysis']['factual_errors'])\n",
    "    inconsistencies = len(assessment['consistency_analysis'].get('inconsistencies', []))\n",
    "    contradictions = len(assessment['coherence_analysis']['contradictions'])\n",
    "    \n",
    "    print(f\"\\nIssues Detected:\")\n",
    "    print(f\"  Factual errors: {factual_errors}\")\n",
    "    print(f\"  Contextual inconsistencies: {inconsistencies}\")\n",
    "    print(f\"  Logical contradictions: {contradictions}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nRecommendations:\")\n",
    "    for j, rec in enumerate(assessment['recommendations'], 1):\n",
    "        print(f\"  {j}. {rec}\")\n",
    "    \n",
    "    assessment_results.append({\n",
    "        'test': i,\n",
    "        'description': test_case['description'],\n",
    "        'reliability_score': assessment['overall_reliability'],\n",
    "        'grade': assessment['reliability_grade']\n",
    "    })\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Summary visualization\n",
    "df_results = pd.DataFrame(assessment_results)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Reliability scores\n",
    "plt.subplot(1, 2, 1)\n",
    "bars = plt.bar(range(len(df_results)), df_results['reliability_score'], \n",
    "               color=['green' if score > 0.7 else 'orange' if score > 0.5 else 'red' \n",
    "                     for score in df_results['reliability_score']])\n",
    "plt.xlabel('Test Case')\n",
    "plt.ylabel('Reliability Score')\n",
    "plt.title('Reliability Assessment Results')\n",
    "plt.xticks(range(len(df_results)), [f\"Test {i+1}\" for i in range(len(df_results))])\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add score labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Grade distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "grade_counts = df_results['grade'].value_counts()\n",
    "plt.pie(grade_counts.values, labels=grade_counts.index, autopct='%1.0f%%')\n",
    "plt.title('Grade Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== SUMMARY STATISTICS ===\")\n",
    "print(f\"Average reliability score: {df_results['reliability_score'].mean():.3f}\")\n",
    "print(f\"Best performing test: {df_results.loc[df_results['reliability_score'].idxmax(), 'description']}\")\n",
    "print(f\"Lowest performing test: {df_results.loc[df_results['reliability_score'].idxmin(), 'description']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
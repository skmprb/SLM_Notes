{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Context Architectures for LLMs\n",
    "\n",
    "## Overview\n",
    "\n",
    "Handling long sequences efficiently is crucial for many LLM applications. This notebook covers:\n",
    "\n",
    "- **Memory-Augmented Transformers**: External memory mechanisms\n",
    "- **Hierarchical Attention**: Multi-scale attention patterns\n",
    "- **Recurrent Memory**: Integrating recurrence with transformers\n",
    "- **Context Compression**: Efficient long-range modeling\n",
    "\n",
    "Let's implement architectures that can handle extended contexts efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Memory-Augmented Transformer\n",
    "\n",
    "External memory allows the model to store and retrieve information beyond the immediate context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryAugmentedTransformer(nn.Module):\n",
    "    \"\"\"Transformer with external memory for long contexts\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, memory_size=1024, memory_dim=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.memory_size = memory_size\n",
    "        self.memory_dim = memory_dim or d_model\n",
    "        \n",
    "        # External memory\n",
    "        self.memory = nn.Parameter(torch.randn(memory_size, self.memory_dim))\n",
    "        \n",
    "        # Memory access mechanisms\n",
    "        self.memory_query = nn.Linear(d_model, self.memory_dim)\n",
    "        self.memory_key = nn.Linear(self.memory_dim, self.memory_dim)\n",
    "        self.memory_value = nn.Linear(self.memory_dim, d_model)\n",
    "        \n",
    "        # Standard attention\n",
    "        self.self_attention = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
    "        \n",
    "        # Memory update mechanism\n",
    "        self.memory_update = nn.Linear(d_model, self.memory_dim)\n",
    "        self.memory_gate = nn.Linear(d_model + self.memory_dim, 1)\n",
    "        \n",
    "        # Layer norm and feedforward\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, memory_state=None):\n",
    "        B, T, D = x.shape\n",
    "        \n",
    "        # Use provided memory state or initialize\n",
    "        if memory_state is None:\n",
    "            current_memory = self.memory.unsqueeze(0).expand(B, -1, -1)\n",
    "        else:\n",
    "            current_memory = memory_state\n",
    "        \n",
    "        # Self-attention\n",
    "        attn_out, _ = self.self_attention(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        \n",
    "        # Memory attention\n",
    "        memory_out = self.memory_attention(x, current_memory)\n",
    "        x = self.norm2(x + memory_out)\n",
    "        \n",
    "        # Update memory\n",
    "        updated_memory = self.update_memory(x, current_memory)\n",
    "        \n",
    "        # Feedforward\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm3(x + ffn_out)\n",
    "        \n",
    "        return x, updated_memory\n",
    "    \n",
    "    def memory_attention(self, queries, memory):\n",
    "        \"\"\"Attend to external memory\"\"\"\n",
    "        B, T, D = queries.shape\n",
    "        M, MD = memory.shape[1:]\n",
    "        \n",
    "        # Project queries\n",
    "        q = self.memory_query(queries)  # [B, T, MD]\n",
    "        k = self.memory_key(memory)     # [B, M, MD]\n",
    "        v = self.memory_value(memory)   # [B, M, D]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(MD)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention\n",
    "        memory_out = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        return memory_out\n",
    "    \n",
    "    def update_memory(self, x, memory):\n",
    "        \"\"\"Update memory based on current input\"\"\"\n",
    "        B, T, D = x.shape\n",
    "        \n",
    "        # Compute memory updates\n",
    "        memory_updates = self.memory_update(x)  # [B, T, MD]\n",
    "        \n",
    "        # Compute attention weights for memory update\n",
    "        update_queries = memory_updates.mean(dim=1, keepdim=True)  # [B, 1, MD]\n",
    "        memory_keys = self.memory_key(memory)  # [B, M, MD]\n",
    "        \n",
    "        update_scores = torch.matmul(update_queries, memory_keys.transpose(-2, -1))\n",
    "        update_weights = F.softmax(update_scores / math.sqrt(self.memory_dim), dim=-1)\n",
    "        \n",
    "        # Compute gates for selective update\n",
    "        gate_input = torch.cat([update_queries.expand(-1, self.memory_size, -1), memory], dim=-1)\n",
    "        gates = torch.sigmoid(self.memory_gate(gate_input))  # [B, M, 1]\n",
    "        \n",
    "        # Update memory\n",
    "        memory_delta = update_weights.transpose(-2, -1) * memory_updates.mean(dim=1, keepdim=True)\n",
    "        updated_memory = memory + gates * memory_delta\n",
    "        \n",
    "        return updated_memory\n",
    "\n",
    "class HierarchicalAttention(nn.Module):\n",
    "    \"\"\"Multi-scale hierarchical attention mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, scales=[1, 4, 16]):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.scales = scales\n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        # Multi-scale projections\n",
    "        self.scale_projections = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'q': nn.Linear(d_model, d_model),\n",
    "                'k': nn.Linear(d_model, d_model),\n",
    "                'v': nn.Linear(d_model, d_model)\n",
    "            }) for _ in scales\n",
    "        ])\n",
    "        \n",
    "        # Scale fusion\n",
    "        self.scale_fusion = nn.Linear(len(scales) * d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        scale_outputs = []\n",
    "        \n",
    "        for scale_idx, scale in enumerate(self.scales):\n",
    "            # Downsample for this scale\n",
    "            if scale > 1:\n",
    "                # Average pooling for downsampling\n",
    "                x_scaled = F.avg_pool1d(\n",
    "                    x.transpose(1, 2), \n",
    "                    kernel_size=scale, \n",
    "                    stride=scale,\n",
    "                    padding=0\n",
    "                ).transpose(1, 2)\n",
    "            else:\n",
    "                x_scaled = x\n",
    "            \n",
    "            # Apply attention at this scale\n",
    "            scale_out = self.scale_attention(x_scaled, scale_idx)\n",
    "            \n",
    "            # Upsample back to original resolution\n",
    "            if scale > 1:\n",
    "                scale_out = F.interpolate(\n",
    "                    scale_out.transpose(1, 2),\n",
    "                    size=T,\n",
    "                    mode='linear',\n",
    "                    align_corners=False\n",
    "                ).transpose(1, 2)\n",
    "            \n",
    "            scale_outputs.append(scale_out)\n",
    "        \n",
    "        # Fuse multi-scale outputs\n",
    "        fused = torch.cat(scale_outputs, dim=-1)\n",
    "        output = self.scale_fusion(fused)\n",
    "        \n",
    "        return self.out_proj(output)\n",
    "    \n",
    "    def scale_attention(self, x, scale_idx):\n",
    "        \"\"\"Apply attention at specific scale\"\"\"\n",
    "        B, T, D = x.shape\n",
    "        \n",
    "        # Project Q, K, V\n",
    "        proj = self.scale_projections[scale_idx]\n",
    "        q = proj['q'](x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = proj['k'](x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = proj['v'](x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        \n",
    "        # Reshape output\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
    "        \n",
    "        return out\n",
    "\n",
    "print(\"Long context architecture modules implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Long Context Architectures\n",
    "\n",
    "Let's test memory retention and hierarchical processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_memory_retention(model, seq_length=2048, memory_test_positions=[100, 500, 1000, 1500]):\n",
    "    \"\"\"Test how well the model retains information across long sequences\"\"\"\n",
    "    d_model = model.d_model\n",
    "    \n",
    "    # Create test sequence with special tokens at specific positions\n",
    "    x = torch.randn(1, seq_length, d_model)\n",
    "    \n",
    "    # Insert distinctive patterns at test positions\n",
    "    for pos in memory_test_positions:\n",
    "        if pos < seq_length:\n",
    "            # Create a distinctive pattern\n",
    "            pattern = torch.ones(1, 1, d_model) * (pos / seq_length)\n",
    "            x[:, pos:pos+1, :] = pattern\n",
    "    \n",
    "    # Process through model\n",
    "    memory_state = None\n",
    "    chunk_size = 256\n",
    "    outputs = []\n",
    "    \n",
    "    for i in range(0, seq_length, chunk_size):\n",
    "        end_idx = min(i + chunk_size, seq_length)\n",
    "        chunk = x[:, i:end_idx, :]\n",
    "        \n",
    "        if hasattr(model, 'memory'):\n",
    "            output, memory_state = model(chunk, memory_state)\n",
    "        else:\n",
    "            output = model(chunk)\n",
    "        \n",
    "        outputs.append(output)\n",
    "    \n",
    "    # Analyze memory retention\n",
    "    full_output = torch.cat(outputs, dim=1)\n",
    "    retention_scores = []\n",
    "    \n",
    "    for pos in memory_test_positions:\n",
    "        if pos < seq_length - 100:  # Ensure we have space to test retention\n",
    "            # Compare pattern at insertion vs later positions\n",
    "            original_pattern = x[:, pos, :]\n",
    "            \n",
    "            # Test retention at positions after the pattern\n",
    "            test_positions = [pos + 50, pos + 100, pos + 200]\n",
    "            \n",
    "            for test_pos in test_positions:\n",
    "                if test_pos < seq_length:\n",
    "                    output_at_test = full_output[:, test_pos, :]\n",
    "                    \n",
    "                    # Measure similarity (simplified)\n",
    "                    similarity = F.cosine_similarity(\n",
    "                        original_pattern.unsqueeze(0), \n",
    "                        output_at_test.unsqueeze(0)\n",
    "                    ).item()\n",
    "                    \n",
    "                    retention_scores.append({\n",
    "                        'insert_pos': pos,\n",
    "                        'test_pos': test_pos,\n",
    "                        'distance': test_pos - pos,\n",
    "                        'similarity': similarity\n",
    "                    })\n",
    "    \n",
    "    return retention_scores\n",
    "\n",
    "def analyze_hierarchical_patterns(model, seq_length=1024):\n",
    "    \"\"\"Analyze attention patterns at different scales\"\"\"\n",
    "    if not hasattr(model, 'scales'):\n",
    "        return None\n",
    "    \n",
    "    d_model = model.d_model\n",
    "    x = torch.randn(1, seq_length, d_model)\n",
    "    \n",
    "    # Hook to capture attention weights\n",
    "    attention_weights = {}\n",
    "    \n",
    "    def hook_fn(name):\n",
    "        def hook(module, input, output):\n",
    "            if hasattr(module, 'last_attention_weights'):\n",
    "                attention_weights[name] = module.last_attention_weights\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks (simplified for demo)\n",
    "    with torch.no_grad():\n",
    "        output = model(x)\n",
    "    \n",
    "    return {\n",
    "        'output_shape': output.shape,\n",
    "        'scales_processed': model.scales,\n",
    "        'hierarchical_processing': True\n",
    "    }\n",
    "\n",
    "# Test models\n",
    "d_model, n_heads = 256, 8\n",
    "\n",
    "print(\"Testing Memory-Augmented Transformer...\")\n",
    "memory_model = MemoryAugmentedTransformer(d_model, n_heads, memory_size=512)\n",
    "memory_retention = test_memory_retention(memory_model, seq_length=1024)\n",
    "\n",
    "print(\"Testing Hierarchical Attention...\")\n",
    "hierarchical_model = HierarchicalAttention(d_model, n_heads, scales=[1, 2, 4, 8])\n",
    "hierarchical_analysis = analyze_hierarchical_patterns(hierarchical_model, seq_length=512)\n",
    "\n",
    "print(\"Testing Standard Transformer (baseline)...\")\n",
    "standard_model = nn.TransformerEncoderLayer(d_model, n_heads, batch_first=True)\n",
    "\n",
    "# Benchmark memory efficiency\n",
    "def benchmark_memory_efficiency(models, seq_lengths):\n",
    "    \"\"\"Benchmark memory usage across different sequence lengths\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        results[name] = []\n",
    "        \n",
    "        for seq_len in seq_lengths:\n",
    "            x = torch.randn(2, seq_len, d_model)\n",
    "            \n",
    "            # Measure memory before\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    if hasattr(model, 'memory'):\n",
    "                        output, _ = model(x)\n",
    "                    else:\n",
    "                        output = model(x)\n",
    "                \n",
    "                memory_used = torch.cuda.max_memory_allocated() if torch.cuda.is_available() else 0\n",
    "                \n",
    "                results[name].append({\n",
    "                    'seq_len': seq_len,\n",
    "                    'memory_mb': memory_used / (1024 * 1024),\n",
    "                    'success': True\n",
    "                })\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                results[name].append({\n",
    "                    'seq_len': seq_len,\n",
    "                    'memory_mb': float('inf'),\n",
    "                    'success': False,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "    \n",
    "    return results\n",
    "\n",
    "models = {\n",
    "    'Standard': standard_model,\n",
    "    'Memory-Augmented': memory_model,\n",
    "    'Hierarchical': hierarchical_model\n",
    "}\n",
    "\n",
    "seq_lengths = [128, 256, 512, 1024]\n",
    "memory_benchmark = benchmark_memory_efficiency(models, seq_lengths)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Memory retention analysis\n",
    "plt.subplot(2, 3, 1)\n",
    "if memory_retention:\n",
    "    distances = [r['distance'] for r in memory_retention]\n",
    "    similarities = [r['similarity'] for r in memory_retention]\n",
    "    plt.scatter(distances, similarities, alpha=0.6)\n",
    "    plt.xlabel('Distance from Original')\n",
    "    plt.ylabel('Similarity Score')\n",
    "    plt.title('Memory Retention Analysis')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Memory usage comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "for name, results in memory_benchmark.items():\n",
    "    seq_lens = [r['seq_len'] for r in results if r['success']]\n",
    "    memory_usage = [r['memory_mb'] for r in results if r['success']]\n",
    "    if seq_lens:\n",
    "        plt.plot(seq_lens, memory_usage, 'o-', label=name, linewidth=2, markersize=6)\n",
    "\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Memory Usage (MB)')\n",
    "plt.title('Memory Usage Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Hierarchical scale visualization\n",
    "plt.subplot(2, 3, 3)\n",
    "if hierarchical_analysis:\n",
    "    scales = hierarchical_model.scales\n",
    "    scale_complexities = [1/s for s in scales]  # Inverse complexity\n",
    "    plt.bar(range(len(scales)), scale_complexities, alpha=0.7)\n",
    "    plt.xlabel('Scale Index')\n",
    "    plt.ylabel('Relative Efficiency')\n",
    "    plt.title('Hierarchical Scale Efficiency')\n",
    "    plt.xticks(range(len(scales)), [f'Scale {s}' for s in scales])\n",
    "\n",
    "# Memory retention by distance\n",
    "plt.subplot(2, 3, 4)\n",
    "if memory_retention:\n",
    "    # Group by distance ranges\n",
    "    distance_ranges = [(0, 50), (50, 100), (100, 200), (200, 400)]\n",
    "    avg_similarities = []\n",
    "    \n",
    "    for min_dist, max_dist in distance_ranges:\n",
    "        range_similarities = [r['similarity'] for r in memory_retention \n",
    "                            if min_dist <= r['distance'] < max_dist]\n",
    "        avg_sim = np.mean(range_similarities) if range_similarities else 0\n",
    "        avg_similarities.append(avg_sim)\n",
    "    \n",
    "    range_labels = [f'{min_d}-{max_d}' for min_d, max_d in distance_ranges]\n",
    "    plt.bar(range_labels, avg_similarities, alpha=0.7, color='skyblue')\n",
    "    plt.xlabel('Distance Range')\n",
    "    plt.ylabel('Average Similarity')\n",
    "    plt.title('Memory Retention by Distance')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "# Complexity comparison\n",
    "plt.subplot(2, 3, 5)\n",
    "seq_lens = np.array(seq_lengths)\n",
    "standard_complexity = seq_lens ** 2  # O(n²)\n",
    "hierarchical_complexity = seq_lens * np.log(seq_lens)  # O(n log n)\n",
    "memory_complexity = seq_lens  # O(n) with fixed memory\n",
    "\n",
    "plt.plot(seq_lens, standard_complexity, 'o-', label='Standard O(n²)', linewidth=2)\n",
    "plt.plot(seq_lens, hierarchical_complexity, 's-', label='Hierarchical O(n log n)', linewidth=2)\n",
    "plt.plot(seq_lens, memory_complexity, '^-', label='Memory-Aug O(n)', linewidth=2)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Theoretical Complexity')\n",
    "plt.title('Computational Complexity')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Architecture comparison summary\n",
    "plt.subplot(2, 3, 6)\n",
    "architectures = ['Standard', 'Memory-Aug', 'Hierarchical']\n",
    "max_seq_lens = []\n",
    "\n",
    "for arch in architectures:\n",
    "    if arch in memory_benchmark:\n",
    "        successful_runs = [r for r in memory_benchmark[arch] if r['success']]\n",
    "        max_len = max([r['seq_len'] for r in successful_runs]) if successful_runs else 0\n",
    "        max_seq_lens.append(max_len)\n",
    "    else:\n",
    "        max_seq_lens.append(0)\n",
    "\n",
    "plt.bar(architectures, max_seq_lens, alpha=0.7, color=['red', 'green', 'blue'])\n",
    "plt.xlabel('Architecture')\n",
    "plt.ylabel('Max Sequence Length')\n",
    "plt.title('Maximum Supported Sequence Length')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== LONG CONTEXT ANALYSIS ===\")\n",
    "print(f\"Memory-Augmented Transformer:\")\n",
    "print(f\"  Memory size: {memory_model.memory_size}\")\n",
    "print(f\"  Memory dimension: {memory_model.memory_dim}\")\n",
    "\n",
    "print(f\"\\nHierarchical Attention:\")\n",
    "print(f\"  Scales: {hierarchical_model.scales}\")\n",
    "print(f\"  Multi-scale processing: {hierarchical_analysis['hierarchical_processing'] if hierarchical_analysis else 'N/A'}\")\n",
    "\n",
    "if memory_retention:\n",
    "    avg_retention = np.mean([r['similarity'] for r in memory_retention])\n",
    "    print(f\"\\nMemory Retention:\")\n",
    "    print(f\"  Average similarity: {avg_retention:.3f}\")\n",
    "    print(f\"  Total test points: {len(memory_retention)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
# LLM Abbreviations & Terminology Glossary

## A

**ALiBi** - Attention with Linear Biases  
**API** - Application Programming Interface  
**AR** - Autoregressive  
**BERT** - Bidirectional Encoder Representations from Transformers  

## B

**BLEU** - Bilingual Evaluation Understudy  
**BPE** - Byte Pair Encoding  
**BPTT** - Backpropagation Through Time  

## C

**CLM** - Causal Language Modeling  
**CoT** - Chain-of-Thought  
**CPU** - Central Processing Unit  
**CUDA** - Compute Unified Device Architecture  

## D

**DDP** - Distributed Data Parallel  
**DPO** - Direct Preference Optimization  

## E

**ELMo** - Embeddings from Language Models  
**EOS** - End of Sequence  

## F

**FFN** - Feed-Forward Network  
**FLAN** - Finetuned Language Net  
**FP16** - 16-bit Floating Point  
**FP32** - 32-bit Floating Point  

## G

**GAN** - Generative Adversarial Network  
**GeLU** - Gaussian Error Linear Unit  
**GLU** - Gated Linear Unit  
**GloVe** - Global Vectors for Word Representation  
**GPU** - Graphics Processing Unit  
**GPT** - Generative Pre-trained Transformer  
**GRU** - Gated Recurrent Unit  

## H

**HF** - Hugging Face  
**HHRLHF** - Helpful, Harmless, and Honest Reinforcement Learning from Human Feedback  

## I

**ICL** - In-Context Learning  
**IID** - Independent and Identically Distributed  

## K

**KL** - Kullback-Leibler (divergence)  
**KV** - Key-Value (cache)  

## L

**LLM** - Large Language Model  
**LM** - Language Model  
**LoRA** - Low-Rank Adaptation  
**LR** - Learning Rate  
**LSTM** - Long Short-Term Memory  

## M

**MHA** - Multi-Head Attention  
**MLE** - Maximum Likelihood Estimation  
**MLM** - Masked Language Modeling  
**MoE** - Mixture of Experts  
**MSE** - Mean Squared Error  

## N

**NLP** - Natural Language Processing  
**NLU** - Natural Language Understanding  
**NMT** - Neural Machine Translation  
**NSP** - Next Sentence Prediction  

## O

**OOV** - Out-of-Vocabulary  

## P

**PEFT** - Parameter-Efficient Fine-Tuning  
**PLM** - Pre-trained Language Model  
**PPO** - Proximal Policy Optimization  

## Q

**QA** - Question Answering  
**QKV** - Query, Key, Value  

## R

**RAG** - Retrieval-Augmented Generation  
**ReAct** - Reasoning and Acting  
**ReLU** - Rectified Linear Unit  
**RLHF** - Reinforcement Learning from Human Feedback  
**RMSNorm** - Root Mean Square Layer Normalization  
**RNN** - Recurrent Neural Network  
**RoPE** - Rotary Position Embedding  
**ROUGE** - Recall-Oriented Understudy for Gisting Evaluation  

## S

**SFT** - Supervised Fine-Tuning  
**SGD** - Stochastic Gradient Descent  
**SoTA** - State-of-the-Art  

## T

**TF-IDF** - Term Frequency-Inverse Document Frequency  
**TPU** - Tensor Processing Unit  

## V

**VAE** - Variational Autoencoder  

## W

**WER** - Word Error Rate  

---

## Key Terms & Definitions

### Attention
Mechanism allowing models to focus on relevant parts of input sequences.

### Autoregressive
Models that predict next token based on previous tokens.

### Beam Search
Decoding strategy that maintains multiple candidate sequences.

### Context Window
Maximum sequence length a model can process at once.

### Embedding
Dense vector representation of tokens or words.

### Fine-tuning
Training a pre-trained model on specific downstream tasks.

### Hallucination
When models generate plausible but incorrect information.

### In-Context Learning
Learning from examples provided in the input prompt.

### Perplexity
Measure of how well a model predicts text (lower is better).

### Prompt Engineering
Crafting effective input prompts to guide model behavior.

### Quantization
Reducing model precision to decrease memory usage.

### Temperature
Parameter controlling randomness in text generation.

### Tokenization
Process of splitting text into discrete units (tokens).

### Zero-shot
Performing tasks without specific training examples.
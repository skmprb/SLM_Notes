{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Representation Variants for LLMs\n",
    "\n",
    "## Overview\n",
    "\n",
    "Positional encodings are crucial for transformers to understand sequence order. This notebook covers:\n",
    "\n",
    "- **RoPE (Rotary Position Embedding)**: Rotation-based position encoding\n",
    "- **ALiBi (Attention with Linear Biases)**: Bias-based positional information\n",
    "- **Relative Position Encodings**: Distance-based position representations\n",
    "- **Performance Comparison**: Evaluating different positional schemes\n",
    "\n",
    "Let's implement and compare various positional encoding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RoPE (Rotary Position Embedding)\n",
    "\n",
    "RoPE encodes position by rotating query and key vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPEAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention with Rotary Position Embedding\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.qkv_proj = nn.Linear(d_model, 3 * d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Precompute rotation matrices\n",
    "        self.register_buffer('cos_cached', None)\n",
    "        self.register_buffer('sin_cached', None)\n",
    "        self._build_rope_cache(max_seq_len)\n",
    "    \n",
    "    def _build_rope_cache(self, seq_len):\n",
    "        \"\"\"Build rotation matrices for RoPE\"\"\"\n",
    "        # Frequency for each dimension pair\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.d_head, 2).float() / self.d_head))\n",
    "        \n",
    "        # Position indices\n",
    "        t = torch.arange(seq_len).type_as(inv_freq)\n",
    "        \n",
    "        # Compute frequencies\n",
    "        freqs = torch.einsum('i,j->ij', t, inv_freq)\n",
    "        \n",
    "        # Create rotation matrices\n",
    "        cos = freqs.cos()\n",
    "        sin = freqs.sin()\n",
    "        \n",
    "        self.cos_cached = cos\n",
    "        self.sin_cached = sin\n",
    "    \n",
    "    def apply_rope(self, x, cos, sin):\n",
    "        \"\"\"Apply rotary position embedding\"\"\"\n",
    "        # x shape: [batch, heads, seq_len, head_dim]\n",
    "        seq_len = x.shape[2]\n",
    "        \n",
    "        # Get rotation matrices for current sequence length\n",
    "        cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, head_dim//2]\n",
    "        sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Split into pairs for rotation\n",
    "        x1 = x[..., ::2]   # Even indices\n",
    "        x2 = x[..., 1::2]  # Odd indices\n",
    "        \n",
    "        # Apply rotation\n",
    "        rotated_x1 = x1 * cos - x2 * sin\n",
    "        rotated_x2 = x1 * sin + x2 * cos\n",
    "        \n",
    "        # Interleave back\n",
    "        rotated_x = torch.stack([rotated_x1, rotated_x2], dim=-1)\n",
    "        rotated_x = rotated_x.flatten(-2)\n",
    "        \n",
    "        return rotated_x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Extend cache if needed\n",
    "        if T > self.cos_cached.shape[0]:\n",
    "            self._build_rope_cache(T)\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        qkv = self.qkv_proj(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # Apply RoPE to queries and keys\n",
    "        q = self.apply_rope(q, self.cos_cached, self.sin_cached)\n",
    "        k = self.apply_rope(k, self.cos_cached, self.sin_cached)\n",
    "        \n",
    "        # Compute attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        \n",
    "        # Reshape and project output\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class ALiBiAttention(nn.Module):\n",
    "    \"\"\"Attention with Linear Biases (ALiBi)\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        self.qkv_proj = nn.Linear(d_model, 3 * d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # ALiBi slopes\n",
    "        self.register_buffer('slopes', self._get_alibi_slopes(n_heads))\n",
    "    \n",
    "    def _get_alibi_slopes(self, n_heads):\n",
    "        \"\"\"Compute ALiBi slopes for each head\"\"\"\n",
    "        def get_slopes_power_of_2(n):\n",
    "            start = (2**(-2**-(math.log2(n)-3)))\n",
    "            ratio = start\n",
    "            return [start*ratio**i for i in range(n)]\n",
    "        \n",
    "        if math.log2(n_heads).is_integer():\n",
    "            slopes = get_slopes_power_of_2(n_heads)\n",
    "        else:\n",
    "            closest_power_of_2 = 2**math.floor(math.log2(n_heads))\n",
    "            slopes = get_slopes_power_of_2(closest_power_of_2)\n",
    "            slopes.extend(get_slopes_power_of_2(2*closest_power_of_2)[0::2][:n_heads-closest_power_of_2])\n",
    "        \n",
    "        return torch.tensor(slopes).float()\n",
    "    \n",
    "    def _get_alibi_bias(self, seq_len):\n",
    "        \"\"\"Generate ALiBi bias matrix\"\"\"\n",
    "        # Create distance matrix\n",
    "        context_position = torch.arange(seq_len)[:, None]\n",
    "        memory_position = torch.arange(seq_len)[None, :]\n",
    "        relative_position = memory_position - context_position\n",
    "        \n",
    "        # Apply slopes to create bias\n",
    "        bias = relative_position[None, :, :] * self.slopes[:, None, None]\n",
    "        \n",
    "        return bias\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        qkv = self.qkv_proj(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
    "        \n",
    "        # Add ALiBi bias\n",
    "        alibi_bias = self._get_alibi_bias(T).to(scores.device)\n",
    "        scores = scores + alibi_bias\n",
    "        \n",
    "        # Apply attention\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        \n",
    "        # Reshape and project output\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "print(\"Positional encoding modules implemented!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
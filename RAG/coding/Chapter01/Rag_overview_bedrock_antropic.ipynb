{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e1c736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "def call_llm_with_full_text(itext):\n",
    "    text_input = '\\n'.join(itext)\n",
    "    prompt = f\"Please elaborate on the following content:\\n{text_input}\"\n",
    "    \n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps({\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": 4096,\n",
    "                \"temperature\": 0.1,\n",
    "                \"system\": \"You are an expert Natural Language Processing exercise expert. You can explain read the input and answer in detail\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response['body'].read())\n",
    "        return response_body['content'][0]['text'].strip()\n",
    "    except Exception as e:\n",
    "        return str(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be95c939",
   "metadata": {},
   "source": [
    "Formatted response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148c008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_formatted_response(response):\n",
    "    # Define the width for wrapping the text\n",
    "    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
    "    wrapped_text = wrapper.fill(text=response)\n",
    "\n",
    "    # Print the formatted response with a header and footer\n",
    "    print(\"Response:\")\n",
    "    print(\"---------------\")\n",
    "    print(wrapped_text)\n",
    "    print(\"---------------\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdb9daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_records = [\n",
    "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
    "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
    "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
    "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
    "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
    "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
    "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
    "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
    "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
    "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
    "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
    "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
    "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
    "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
    "    \"The retrieved documents are then fed into the language model.\",\n",
    "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
    "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
    "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
    "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
    "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
    "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
    "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
    "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
    "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
    "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
    "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
    "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\",\n",
    "    \"A RAG vector store is a database or dataset that contains vectorized data points.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f6f78eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
      "in the field of artificial intelligence, particularly within the realm of\n",
      "natural language processing (NLP). It innovatively combines the capabilities of\n",
      "neural network-based language models with retrieval systems to enhance the\n",
      "generation of text, making it more accurate, informative, and contextually\n",
      "relevant. This methodology leverages the strengths of both generative and\n",
      "retrieval architectures to tackle complex tasks that require not only linguistic\n",
      "fluency but also factual correctness and depth of knowledge. At the core of\n",
      "Retrieval Augmented Generation (RAG) is a generative model, typically a\n",
      "transformer-based neural network, similar to those used in models like GPT\n",
      "(Generative Pre-trained Transformer) or BERT (Bidirectional Encoder\n",
      "Representations from Transformers). This component is responsible for producing\n",
      "coherent and contextually appropriate language outputs based on a mixture of\n",
      "input prompts and additional information fetched by the retrieval component.\n",
      "Complementing the language model is the retrieval system, which is usually built\n",
      "on a database of documents or a corpus of texts. This system uses techniques\n",
      "from information retrieval to find and fetch documents that are relevant to the\n",
      "input query or prompt. The mechanism of relevance determination can range from\n",
      "simple keyword matching to more complex semantic search algorithms which\n",
      "interpret the meaning behind the query to find the best matches. This component\n",
      "merges the outputs from the language model and the retrieval system. It\n",
      "effectively synthesizes the raw data fetched by the retrieval system into the\n",
      "generative process of the language model. The integrator ensures that the\n",
      "information from the retrieval system is seamlessly incorporated into the final\n",
      "text output, enhancing the model's ability to generate responses that are not\n",
      "only fluent and grammatically correct but also rich in factual details and\n",
      "context-specific nuances. When a query or prompt is received, the system first\n",
      "processes it to understand the requirement or the context. Based on the\n",
      "processed query, the retrieval system searches through its database to find\n",
      "relevant documents or information snippets. This retrieval is guided by the\n",
      "similarity of content in the documents to the query, which can be determined\n",
      "through various techniques like vector embeddings or semantic similarity\n",
      "measures. The retrieved documents are then fed into the language model. In some\n",
      "implementations, this integration happens at the token level, where the model\n",
      "can access and incorporate specific pieces of information from the retrieved\n",
      "texts dynamically as it generates each part of the response. The language model,\n",
      "now augmented with direct access to retrieved information, generates a response.\n",
      "This response is not only influenced by the training of the model but also by\n",
      "the specific facts and details contained in the retrieved documents, making it\n",
      "more tailored and accurate. By directly incorporating information from external\n",
      "sources, Retrieval Augmented Generation (RAG) models can produce responses that\n",
      "are more factual and relevant to the given query. This is particularly useful in\n",
      "domains like medical advice, technical support, and other areas where precision\n",
      "and up-to-date knowledge are crucial. Retrieval Augmented Generation (RAG)\n",
      "systems can dynamically adapt to new information since they retrieve data in\n",
      "real-time from their databases. This allows them to remain current with the\n",
      "latest knowledge and trends without needing frequent retraining. With access to\n",
      "a wide range of documents, Retrieval Augmented Generation (RAG) systems can\n",
      "provide detailed and nuanced answers that a standalone language model might not\n",
      "be capable of generating based solely on its pre-trained knowledge. While\n",
      "Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes\n",
      "with its challenges. These include the complexity of integrating retrieval and\n",
      "generation systems, the computational overhead associated with real-time data\n",
      "retrieval, and the need for maintaining a large, up-to-date, and high-quality\n",
      "database of retrievable texts. Furthermore, ensuring the relevance and accuracy\n",
      "of the retrieved information remains a significant challenge, as does managing\n",
      "the potential for introducing biases or errors from the external sources. In\n",
      "summary, Retrieval Augmented Generation represents a significant advancement in\n",
      "the field of artificial intelligence, merging the best of retrieval-based and\n",
      "generative technologies to create systems that not only understand and generate\n",
      "natural language but also deeply comprehend and utilize the vast amounts of\n",
      "information available in textual form. A RAG vector store is a database or\n",
      "dataset that contains vectorized data points.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "paragraph = \" \".join(db_records)\n",
    "wrapper = textwrap.TextWrapper(width=80)\n",
    "wrapped_text = wrapper.fill(text = paragraph)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a0033da",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"define a rag store\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59c20609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "Certainly! I'll elaborate on the content you've provided, which appears to be a\n",
      "phrase or instruction that has been split into individual letters. Let's\n",
      "reconstruct and interpret it:  The phrase is: \"define a rag store\"  This phrase\n",
      "is likely asking for a definition or explanation of what a \"rag store\" is. Let's\n",
      "break it down:  1. \"Define\": This word indicates that we need to provide a clear\n",
      "and precise meaning or description of the concept that follows.  2. \"Rag store\":\n",
      "This is the term we need to define. A rag store is a type of retail\n",
      "establishment or business that specializes in selling rags, cloths, and other\n",
      "similar textile products. Here's a more detailed explanation:  A rag store is: -\n",
      "A shop or business that primarily sells rags, which are pieces of cloth used for\n",
      "cleaning, wiping, or other practical purposes. - These stores may offer a\n",
      "variety of rag types, including:   * Cotton rags   * Microfiber cloths   * Terry\n",
      "cloth rags   * Shop towels   * Cheesecloth   * Lint-free rags - Rag stores often\n",
      "cater to different industries and customers, such as:   * Automotive repair\n",
      "shops   * Industrial cleaning services   * Janitorial supply companies   *\n",
      "Painters and decorators   * Households for general cleaning purposes - Some rag\n",
      "stores might also sell related products like:   * Cleaning solutions   * Mops\n",
      "and brooms   * Buckets and other cleaning tools - In modern times, rag stores\n",
      "may operate as physical retail locations, online stores, or a combination of\n",
      "both. - They may source their products from textile manufacturers, recycling\n",
      "programs, or by repurposing discarded clothing and fabrics.  In summary, a rag\n",
      "store is a specialized retail establishment that focuses on selling various\n",
      "types of rags and related cleaning materials to meet the needs of both\n",
      "commercial and residential customers.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm_response = call_llm_with_full_text(query)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5537aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        use_idf=True,\n",
    "        norm='l2',\n",
    "        ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "        sublinear_tf=True,   # Apply sublinear TF scaling\n",
    "        analyzer='word'      # You could also experiment with 'char' or 'char_wb' for character-level features\n",
    "    )\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e7db2",
   "metadata": {},
   "source": [
    "Enhanced Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18d99d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    lemmatized_words = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        lemmatized_words.append(token.lemma_)\n",
    "    return lemmatized_words\n",
    "\n",
    "def expand_with_synonyms(words):\n",
    "    expanded_words = words.copy()\n",
    "    for word in words:\n",
    "        expanded_words.extend(get_synonyms(word))\n",
    "    return expanded_words\n",
    "\n",
    "def calculate_enhanced_similarity(text1, text2):\n",
    "    # Preprocess and tokenize texts\n",
    "    words1 = preprocess_text(text1)\n",
    "    words2 = preprocess_text(text2)\n",
    "\n",
    "    # Expand with synonyms\n",
    "    words1_expanded = expand_with_synonyms(words1)\n",
    "    words2_expanded = expand_with_synonyms(words2)\n",
    "\n",
    "    # Count word frequencies\n",
    "    freq1 = Counter(words1_expanded)\n",
    "    freq2 = Counter(words2_expanded)\n",
    "\n",
    "    # Create a set of all unique words\n",
    "    unique_words = set(freq1.keys()).union(set(freq2.keys()))\n",
    "\n",
    "    # Create frequency vectors\n",
    "    vector1 = [freq1[word] for word in unique_words]\n",
    "    vector2 = [freq2[word] for word in unique_words]\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    vector1 = np.array(vector1)\n",
    "    vector2 = np.array(vector2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca916476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 65.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 67.7 MB/s  0:00:00\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "021ae76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Keyword Score: 3\n",
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_best_match_keyword_search(query, db_records):\n",
    "    best_score = 0\n",
    "    best_record = None\n",
    "\n",
    "    # Split the query into individual keywords\n",
    "    query_keywords = set(query.lower().split())\n",
    "\n",
    "    # Iterate through each record in db_records\n",
    "    for record in db_records:\n",
    "        # Split the record into keywords\n",
    "        record_keywords = set(record.lower().split())\n",
    "\n",
    "        # Calculate the number of common keywords\n",
    "        common_keywords = query_keywords.intersection(record_keywords)\n",
    "        current_score = len(common_keywords)\n",
    "\n",
    "        # Update the best score and record if the current score is higher\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_record = record\n",
    "\n",
    "    return best_score, best_record\n",
    "\n",
    "# Assuming 'query' and 'db_records' are defined in previous cells in your Colab notebook\n",
    "best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)\n",
    "\n",
    "print(f\"Best Keyword Score: {best_keyword_score}\")\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfb7ca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.126\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "score = calculate_cosine_similarity(query, best_matching_record)\n",
    "print(f\"Best Cosine Similarity Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "371eb049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Enhanced Similarity Score: 0.642\n"
     ]
    }
   ],
   "source": [
    "enchanced_score = calculate_enhanced_similarity(query, best_matching_record)\n",
    "print(f\"Best Enhanced Similarity Score: {enchanced_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ac1af1",
   "metadata": {},
   "source": [
    "Agumented Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2259854",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_input=query+ \": \"+ best_matching_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a626f61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "define a rag store: A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcc8c83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response = call_llm_with_full_text(augmented_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04cdd79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "Certainly! I'd be happy to elaborate on the content you provided. Let's break it\n",
      "down and discuss the concept of a RAG store in more detail:  1. Definition of a\n",
      "RAG store: A RAG (Retrieval-Augmented Generation) vector store is a specialized\n",
      "database or dataset designed to store and manage vectorized data points. These\n",
      "data points are typically high-dimensional numerical representations of various\n",
      "types of information, such as text, images, or other forms of data.  2. Purpose:\n",
      "The primary purpose of a RAG vector store is to facilitate efficient storage,\n",
      "retrieval, and similarity search operations on large amounts of vectorized data.\n",
      "This is particularly useful in natural language processing, machine learning,\n",
      "and artificial intelligence applications.  3. Key components: - Vectors: The\n",
      "core elements stored in a RAG vector store are vectors, which are mathematical\n",
      "representations of data in a multi-dimensional space. - Indexing: RAG stores\n",
      "often use advanced indexing techniques to organize vectors for quick retrieval.\n",
      "- Similarity search: They typically support efficient algorithms for finding\n",
      "similar vectors based on various distance metrics (e.g., cosine similarity,\n",
      "Euclidean distance).  4. Applications: RAG vector stores are commonly used in: -\n",
      "Semantic search engines - Recommendation systems - Natural language\n",
      "understanding and generation - Image and audio processing - Anomaly detection\n",
      "5. Advantages: - Fast retrieval of relevant information - Ability to handle\n",
      "large-scale datasets - Support for complex queries and similarity searches -\n",
      "Integration with machine learning models for improved performance  6.\n",
      "Challenges: - Handling high-dimensional data efficiently - Maintaining accuracy\n",
      "as the dataset grows - Balancing between speed and precision in similarity\n",
      "searches  7. Popular implementations: Some well-known vector store solutions\n",
      "include: - Faiss (Facebook AI Similarity Search) - Annoy (Approximate Nearest\n",
      "Neighbors Oh Yeah) - Elasticsearch with vector search capabilities - Pinecone -\n",
      "Weaviate  In the context of Retrieval-Augmented Generation (RAG), these vector\n",
      "stores play a crucial role in enhancing the performance of language models by\n",
      "providing relevant contextual information during the generation process. This\n",
      "allows AI systems to combine the power of pre-trained language models with up-\n",
      "to-date, task-specific information stored in the vector database.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "856738bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_match(text_input, records):\n",
    "    best_score = 0\n",
    "    best_record = None\n",
    "    for record in records:\n",
    "        current_score = calculate_cosine_similarity(text_input, record)\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_record = record\n",
    "    return best_score, best_record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fb9acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_similarity_score, best_matching_record = find_best_match(query, db_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f9bc793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n",
      "0.12631460871586422\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(best_matching_record)\n",
    "print(best_similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55a04a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
      "Enhanced Similarity:, 0.642\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, best_matching_record)\n",
    "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b83b4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF Matrix: (28, 297)\n",
      "TfidfVectorizer()\n",
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def setup_vectorizer(records):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(records)\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "def find_best_match(query, vectorizer, tfidf_matrix):\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
    "    best_index = similarities.argmax()  # Get the index of the highest similarity score\n",
    "    best_score = similarities[0, best_index]\n",
    "    return best_score, best_index\n",
    "\n",
    "vectorizer, tfidf_matrix = setup_vectorizer(db_records)\n",
    "\n",
    "print(f\"Shape of TF-IDF Matrix: {tfidf_matrix.shape}\")\n",
    "print(vectorizer)\n",
    "\n",
    "best_similarity_score, best_index = find_best_match(query, vectorizer, tfidf_matrix)\n",
    "best_matching_record = db_records[best_index]\n",
    "\n",
    "print_formatted_response(best_matching_record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3321674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
      "Enhanced Similarity:, 0.642\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, response)\n",
    "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c29fad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ability    access  accuracy  ...      with    within   without\n",
      "0   0.000000  0.000000  0.000000  ...  0.000000  0.260582  0.000000\n",
      "1   0.000000  0.000000  0.000000  ...  0.160278  0.000000  0.000000\n",
      "2   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "3   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "4   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "5   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "6   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "7   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "8   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "9   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "10  0.186734  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "11  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "12  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "13  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "14  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "15  0.000000  0.172624  0.000000  ...  0.000000  0.000000  0.000000\n",
      "16  0.000000  0.317970  0.000000  ...  0.258278  0.000000  0.000000\n",
      "17  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "18  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "19  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "20  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "21  0.000000  0.000000  0.000000  ...  0.192110  0.000000  0.291503\n",
      "22  0.000000  0.174772  0.000000  ...  0.141963  0.000000  0.000000\n",
      "23  0.000000  0.000000  0.000000  ...  0.217033  0.000000  0.000000\n",
      "24  0.000000  0.000000  0.000000  ...  0.134513  0.000000  0.000000\n",
      "25  0.000000  0.000000  0.228743  ...  0.000000  0.000000  0.000000\n",
      "26  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "27  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
      "\n",
      "[28 rows x 297 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def setup_vectorizer(records):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(records)\n",
    "    \n",
    "\n",
    "    # Convert the TF-IDF matrix to a DataFrame for display purposes\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(tfidf_df)\n",
    "\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "vectorizer, tfidf_matrix = setup_vectorizer(db_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d2fb6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_input=query+\": \"+best_matching_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bf8dbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "define a rag store: A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63dfc119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "Certainly! Let me elaborate on the content you've provided:  The text appears to\n",
      "be asking for a definition of a RAG store, which stands for Retrieval-Augmented\n",
      "Generation store. Here's a detailed explanation:  A RAG vector store is a\n",
      "specialized database or dataset designed to store and manage vectorized data\n",
      "points. This type of storage system is crucial in the field of natural language\n",
      "processing (NLP) and machine learning, particularly for applications involving\n",
      "Retrieval-Augmented Generation (RAG) models.  Key aspects of a RAG vector store\n",
      "include:  1. Vectorized data: The store contains data that has been transformed\n",
      "into numerical vector representations. These vectors capture semantic meaning\n",
      "and relationships between different pieces of information.  2. Efficient\n",
      "retrieval: RAG stores are optimized for quick and accurate retrieval of relevant\n",
      "information based on vector similarity searches.  3. Scalability: They are\n",
      "designed to handle large amounts of data, often millions or billions of vectors.\n",
      "4. Integration with RAG models: These stores work seamlessly with RAG systems,\n",
      "which combine retrieval of relevant information with generative AI models to\n",
      "produce more accurate and contextually appropriate responses.  5. Indexing: RAG\n",
      "stores typically use advanced indexing techniques to speed up similarity\n",
      "searches and retrieval operations.  6. Dimensionality: The vectors stored are\n",
      "often high-dimensional, representing complex semantic relationships in the data.\n",
      "7. Updating capability: Many RAG stores allow for dynamic updates, enabling the\n",
      "addition of new information or modification of existing data.  8. Multimodal\n",
      "support: Advanced RAG stores can handle vectors derived from various data types,\n",
      "including text, images, and audio.  RAG vector stores are essential components\n",
      "in modern AI systems that require quick access to large amounts of structured\n",
      "and unstructured data, enabling more intelligent and context-aware responses in\n",
      "applications like chatbots, question-answering systems, and content\n",
      "recommendation engines.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2fe35e",
   "metadata": {},
   "source": [
    "Modular Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa508c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class RetrievalComponent:\n",
    "    def __init__(self, method='vector'):\n",
    "        self.method = method\n",
    "        if self.method == 'vector' or self.method == 'indexed':\n",
    "            self.vectorizer = TfidfVectorizer()\n",
    "            self.tfidf_matrix = None\n",
    "\n",
    "    def fit(self, records):\n",
    "        self.documents = records  # Initialize self.documents here\n",
    "        if self.method == 'vector' or self.method == 'indexed':\n",
    "            self.tfidf_matrix = self.vectorizer.fit_transform(records)\n",
    "\n",
    "    def retrieve(self, query):\n",
    "        if self.method == 'keyword':\n",
    "            return self.keyword_search(query)\n",
    "        elif self.method == 'vector':\n",
    "            return self.vector_search(query)\n",
    "        elif self.method == 'indexed':\n",
    "            return self.indexed_search(query)\n",
    "\n",
    "    def keyword_search(self, query):\n",
    "        best_score = 0\n",
    "        best_record = None\n",
    "        query_keywords = set(query.lower().split())\n",
    "        for index, doc in enumerate(self.documents):\n",
    "            doc_keywords = set(doc.lower().split())\n",
    "            common_keywords = query_keywords.intersection(doc_keywords)\n",
    "            score = len(common_keywords)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_record = self.documents[index]\n",
    "        return best_record\n",
    "\n",
    "    def vector_search(self, query):\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
    "        best_index = similarities.argmax()\n",
    "        return db_records[best_index]\n",
    "\n",
    "    def indexed_search(self, query):\n",
    "        # Assuming the tfidf_matrix is precomputed and stored\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
    "        best_index = similarities.argmax()\n",
    "        return db_records[best_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

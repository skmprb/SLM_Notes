{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Practical Examples - Linking Concepts Together\n",
    "\n",
    "This notebook demonstrates key RAG concepts with practical implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install numpy pandas scikit-learn sentence-transformers faiss-cpu matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Processing and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents for our RAG system\n",
    "documents = [\n",
    "    \"Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\",\n",
    "    \"Deep learning uses neural networks with multiple layers to model complex patterns in data.\",\n",
    "    \"Natural language processing enables computers to understand and generate human language.\",\n",
    "    \"Computer vision allows machines to interpret and understand visual information from images.\",\n",
    "    \"Reinforcement learning trains agents to make decisions through trial and error in an environment.\",\n",
    "    \"Supervised learning uses labeled data to train models for prediction tasks.\",\n",
    "    \"Unsupervised learning finds patterns in data without labeled examples.\",\n",
    "    \"Transfer learning leverages pre-trained models for new but related tasks.\"\n",
    "]\n",
    "\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"{i+1}. {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentChunker:\n",
    "    \"\"\"Demonstrates different chunking strategies\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fixed_chunking(text: str, chunk_size: int = 50) -> List[str]:\n",
    "        \"\"\"Fixed size chunking\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def sliding_window_chunking(text: str, chunk_size: int = 50, overlap: int = 10) -> List[str]:\n",
    "        \"\"\"Sliding window with overlap\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        stride = chunk_size - overlap\n",
    "        \n",
    "        for i in range(0, len(words), stride):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            if len(chunk.split()) > overlap:  # Avoid tiny chunks\n",
    "                chunks.append(chunk)\n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def sentence_chunking(text: str) -> List[str]:\n",
    "        \"\"\"Semantic chunking by sentences\"\"\"\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "# Demonstrate chunking strategies\n",
    "sample_text = \" \".join(documents)\n",
    "chunker = DocumentChunker()\n",
    "\n",
    "print(\"Original text length:\", len(sample_text.split()), \"words\")\n",
    "print(\"\\nChunking Results:\")\n",
    "\n",
    "fixed_chunks = chunker.fixed_chunking(sample_text, chunk_size=20)\n",
    "print(f\"Fixed chunking (20 words): {len(fixed_chunks)} chunks\")\n",
    "\n",
    "sliding_chunks = chunker.sliding_window_chunking(sample_text, chunk_size=20, overlap=5)\n",
    "print(f\"Sliding window (20 words, 5 overlap): {len(sliding_chunks)} chunks\")\n",
    "\n",
    "sentence_chunks = chunker.sentence_chunking(sample_text)\n",
    "print(f\"Sentence chunking: {len(sentence_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding Generation and Vector Mathematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for documents\n",
    "document_embeddings = embedding_model.encode(documents)\n",
    "\n",
    "print(f\"Embedding shape: {document_embeddings.shape}\")\n",
    "print(f\"Each document is represented as a {document_embeddings.shape[1]}-dimensional vector\")\n",
    "\n",
    "# Visualize embedding statistics\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(document_embeddings.flatten(), bins=50, alpha=0.7)\n",
    "plt.title('Distribution of Embedding Values')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "norms = np.linalg.norm(document_embeddings, axis=1)\n",
    "plt.bar(range(len(norms)), norms)\n",
    "plt.title('Vector Magnitudes')\n",
    "plt.xlabel('Document Index')\n",
    "plt.ylabel('L2 Norm')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "similarity_matrix = cosine_similarity(document_embeddings)\n",
    "sns.heatmap(similarity_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Document Similarity Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distance Metrics Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceMetrics:\n",
    "    \"\"\"Implementation of various distance metrics used in RAG\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "        \"\"\"Cosine similarity between two vectors\"\"\"\n",
    "        dot_product = np.dot(v1, v2)\n",
    "        norm_v1 = np.linalg.norm(v1)\n",
    "        norm_v2 = np.linalg.norm(v2)\n",
    "        return dot_product / (norm_v1 * norm_v2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def euclidean_distance(v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "        \"\"\"Euclidean distance between two vectors\"\"\"\n",
    "        return np.sqrt(np.sum((v1 - v2) ** 2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def manhattan_distance(v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "        \"\"\"Manhattan distance between two vectors\"\"\"\n",
    "        return np.sum(np.abs(v1 - v2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def dot_product(v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "        \"\"\"Dot product similarity\"\"\"\n",
    "        return np.dot(v1, v2)\n",
    "\n",
    "# Compare different distance metrics\n",
    "metrics = DistanceMetrics()\n",
    "v1, v2 = document_embeddings[0], document_embeddings[1]\n",
    "\n",
    "print(\"Comparing first two documents:\")\n",
    "print(f\"Document 1: {documents[0][:50]}...\")\n",
    "print(f\"Document 2: {documents[1][:50]}...\")\n",
    "print()\n",
    "print(f\"Cosine Similarity: {metrics.cosine_similarity(v1, v2):.4f}\")\n",
    "print(f\"Euclidean Distance: {metrics.euclidean_distance(v1, v2):.4f}\")\n",
    "print(f\"Manhattan Distance: {metrics.manhattan_distance(v1, v2):.4f}\")\n",
    "print(f\"Dot Product: {metrics.dot_product(v1, v2):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vector Database and Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorDB:\n",
    "    \"\"\"Simple vector database implementation using FAISS\"\"\"\n",
    "    \n",
    "    def __init__(self, dimension: int):\n",
    "        self.dimension = dimension\n",
    "        self.index = faiss.IndexFlatIP(dimension)  # Inner Product (for cosine similarity)\n",
    "        self.documents = []\n",
    "        \n",
    "    def add_documents(self, embeddings: np.ndarray, documents: List[str]):\n",
    "        \"\"\"Add documents and their embeddings to the database\"\"\"\n",
    "        # Normalize embeddings for cosine similarity\n",
    "        normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        self.index.add(normalized_embeddings.astype('float32'))\n",
    "        self.documents.extend(documents)\n",
    "        \n",
    "    def search(self, query_embedding: np.ndarray, k: int = 3) -> Tuple[List[float], List[str]]:\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        # Normalize query embedding\n",
    "        query_normalized = query_embedding / np.linalg.norm(query_embedding)\n",
    "        query_normalized = query_normalized.reshape(1, -1).astype('float32')\n",
    "        \n",
    "        # Search\n",
    "        scores, indices = self.index.search(query_normalized, k)\n",
    "        \n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx < len(self.documents):\n",
    "                results.append((scores[0][i], self.documents[idx]))\n",
    "                \n",
    "        return results\n",
    "\n",
    "# Create and populate vector database\n",
    "vector_db = SimpleVectorDB(document_embeddings.shape[1])\n",
    "vector_db.add_documents(document_embeddings, documents)\n",
    "\n",
    "print(f\"Vector database created with {len(documents)} documents\")\n",
    "print(f\"Index dimension: {vector_db.dimension}\")\n",
    "print(f\"Total vectors in index: {vector_db.index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Query Processing and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"RAG retrieval system combining different approaches\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_db: SimpleVectorDB, embedding_model, documents: List[str]):\n",
    "        self.vector_db = vector_db\n",
    "        self.embedding_model = embedding_model\n",
    "        self.documents = documents\n",
    "        \n",
    "        # Create TF-IDF vectorizer for sparse retrieval\n",
    "        self.tfidf = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "        self.tfidf_matrix = self.tfidf.fit_transform(documents)\n",
    "        \n",
    "    def dense_retrieval(self, query: str, k: int = 3) -> List[Tuple[float, str]]:\n",
    "        \"\"\"Dense retrieval using embeddings\"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query])[0]\n",
    "        return self.vector_db.search(query_embedding, k)\n",
    "    \n",
    "    def sparse_retrieval(self, query: str, k: int = 3) -> List[Tuple[float, str]]:\n",
    "        \"\"\"Sparse retrieval using TF-IDF\"\"\"\n",
    "        query_vector = self.tfidf.transform([query])\n",
    "        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
    "        \n",
    "        # Get top-k results\n",
    "        top_indices = similarities.argsort()[-k:][::-1]\n",
    "        results = [(similarities[i], self.documents[i]) for i in top_indices]\n",
    "        return results\n",
    "    \n",
    "    def hybrid_retrieval(self, query: str, k: int = 3, alpha: float = 0.7) -> List[Tuple[float, str]]:\n",
    "        \"\"\"Hybrid retrieval combining dense and sparse methods\"\"\"\n",
    "        dense_results = self.dense_retrieval(query, k*2)\n",
    "        sparse_results = self.sparse_retrieval(query, k*2)\n",
    "        \n",
    "        # Combine scores\n",
    "        combined_scores = {}\n",
    "        \n",
    "        for score, doc in dense_results:\n",
    "            combined_scores[doc] = alpha * score\n",
    "            \n",
    "        for score, doc in sparse_results:\n",
    "            if doc in combined_scores:\n",
    "                combined_scores[doc] += (1 - alpha) * score\n",
    "            else:\n",
    "                combined_scores[doc] = (1 - alpha) * score\n",
    "        \n",
    "        # Sort and return top-k\n",
    "        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [(score, doc) for doc, score in sorted_results[:k]]\n",
    "\n",
    "# Create retriever\n",
    "retriever = RAGRetriever(vector_db, embedding_model, documents)\n",
    "\n",
    "# Test different retrieval methods\n",
    "test_query = \"What is neural network learning?\"\n",
    "\n",
    "print(f\"Query: '{test_query}'\\n\")\n",
    "\n",
    "print(\"=== Dense Retrieval Results ===\")\n",
    "dense_results = retriever.dense_retrieval(test_query, k=3)\n",
    "for i, (score, doc) in enumerate(dense_results, 1):\n",
    "    print(f\"{i}. Score: {score:.4f}\")\n",
    "    print(f\"   Document: {doc}\\n\")\n",
    "\n",
    "print(\"=== Sparse Retrieval Results ===\")\n",
    "sparse_results = retriever.sparse_retrieval(test_query, k=3)\n",
    "for i, (score, doc) in enumerate(sparse_results, 1):\n",
    "    print(f\"{i}. Score: {score:.4f}\")\n",
    "    print(f\"   Document: {doc}\\n\")\n",
    "\n",
    "print(\"=== Hybrid Retrieval Results ===\")\n",
    "hybrid_results = retriever.hybrid_retrieval(test_query, k=3)\n",
    "for i, (score, doc) in enumerate(hybrid_results, 1):\n",
    "    print(f\"{i}. Score: {score:.4f}\")\n",
    "    print(f\"   Document: {doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    \"\"\"Evaluation metrics for RAG systems\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def precision_at_k(retrieved: List[str], relevant: List[str], k: int) -> float:\n",
    "        \"\"\"Precision@K metric\"\"\"\n",
    "        retrieved_k = retrieved[:k]\n",
    "        relevant_retrieved = len(set(retrieved_k) & set(relevant))\n",
    "        return relevant_retrieved / len(retrieved_k) if retrieved_k else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def recall_at_k(retrieved: List[str], relevant: List[str], k: int) -> float:\n",
    "        \"\"\"Recall@K metric\"\"\"\n",
    "        retrieved_k = retrieved[:k]\n",
    "        relevant_retrieved = len(set(retrieved_k) & set(relevant))\n",
    "        return relevant_retrieved / len(relevant) if relevant else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_reciprocal_rank(retrieved_lists: List[List[str]], relevant_lists: List[List[str]]) -> float:\n",
    "        \"\"\"Mean Reciprocal Rank (MRR)\"\"\"\n",
    "        reciprocal_ranks = []\n",
    "        \n",
    "        for retrieved, relevant in zip(retrieved_lists, relevant_lists):\n",
    "            for i, doc in enumerate(retrieved, 1):\n",
    "                if doc in relevant:\n",
    "                    reciprocal_ranks.append(1.0 / i)\n",
    "                    break\n",
    "            else:\n",
    "                reciprocal_ranks.append(0.0)\n",
    "        \n",
    "        return sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ndcg_at_k(retrieved: List[str], relevant: List[str], k: int) -> float:\n",
    "        \"\"\"Normalized Discounted Cumulative Gain@K\"\"\"\n",
    "        def dcg(relevances: List[int]) -> float:\n",
    "            return sum(rel / np.log2(i + 2) for i, rel in enumerate(relevances))\n",
    "        \n",
    "        retrieved_k = retrieved[:k]\n",
    "        relevances = [1 if doc in relevant else 0 for doc in retrieved_k]\n",
    "        \n",
    "        dcg_score = dcg(relevances)\n",
    "        ideal_relevances = sorted(relevances, reverse=True)\n",
    "        idcg_score = dcg(ideal_relevances)\n",
    "        \n",
    "        return dcg_score / idcg_score if idcg_score > 0 else 0.0\n",
    "\n",
    "# Example evaluation\n",
    "evaluator = RAGEvaluator()\n",
    "\n",
    "# Simulate evaluation scenario\n",
    "query = \"machine learning algorithms\"\n",
    "retrieved_docs = [doc for _, doc in retriever.dense_retrieval(query, k=5)]\n",
    "relevant_docs = [documents[0], documents[5]]  # Manually defined relevant docs\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Retrieved documents: {len(retrieved_docs)}\")\n",
    "print(f\"Relevant documents: {len(relevant_docs)}\")\n",
    "print()\n",
    "\n",
    "# Calculate metrics\n",
    "precision_3 = evaluator.precision_at_k(retrieved_docs, relevant_docs, 3)\n",
    "recall_3 = evaluator.recall_at_k(retrieved_docs, relevant_docs, 3)\n",
    "ndcg_3 = evaluator.ndcg_at_k(retrieved_docs, relevant_docs, 3)\n",
    "\n",
    "print(f\"Precision@3: {precision_3:.4f}\")\n",
    "print(f\"Recall@3: {recall_3:.4f}\")\n",
    "print(f\"NDCG@3: {ndcg_3:.4f}\")\n",
    "\n",
    "# F1 Score\n",
    "f1_score = 2 * (precision_3 * recall_3) / (precision_3 + recall_3) if (precision_3 + recall_3) > 0 else 0\n",
    "print(f\"F1@3: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Analysis and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceAnalyzer:\n",
    "    \"\"\"Analyze RAG system performance\"\"\"\n",
    "    \n",
    "    def __init__(self, retriever: RAGRetriever):\n",
    "        self.retriever = retriever\n",
    "        \n",
    "    def benchmark_retrieval_methods(self, queries: List[str], k: int = 3) -> dict:\n",
    "        \"\"\"Benchmark different retrieval methods\"\"\"\n",
    "        results = {\n",
    "            'dense': {'times': [], 'results': []},\n",
    "            'sparse': {'times': [], 'results': []},\n",
    "            'hybrid': {'times': [], 'results': []}\n",
    "        }\n",
    "        \n",
    "        for query in queries:\n",
    "            # Dense retrieval\n",
    "            start_time = time.time()\n",
    "            dense_result = self.retriever.dense_retrieval(query, k)\n",
    "            dense_time = time.time() - start_time\n",
    "            results['dense']['times'].append(dense_time)\n",
    "            results['dense']['results'].append(dense_result)\n",
    "            \n",
    "            # Sparse retrieval\n",
    "            start_time = time.time()\n",
    "            sparse_result = self.retriever.sparse_retrieval(query, k)\n",
    "            sparse_time = time.time() - start_time\n",
    "            results['sparse']['times'].append(sparse_time)\n",
    "            results['sparse']['results'].append(sparse_result)\n",
    "            \n",
    "            # Hybrid retrieval\n",
    "            start_time = time.time()\n",
    "            hybrid_result = self.retriever.hybrid_retrieval(query, k)\n",
    "            hybrid_time = time.time() - start_time\n",
    "            results['hybrid']['times'].append(hybrid_time)\n",
    "            results['hybrid']['results'].append(hybrid_result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_chunk_size_impact(self, text: str, chunk_sizes: List[int]) -> dict:\n",
    "        \"\"\"Analyze impact of different chunk sizes\"\"\"\n",
    "        chunker = DocumentChunker()\n",
    "        results = {}\n",
    "        \n",
    "        for size in chunk_sizes:\n",
    "            chunks = chunker.fixed_chunking(text, size)\n",
    "            embeddings = self.retriever.embedding_model.encode(chunks)\n",
    "            \n",
    "            results[size] = {\n",
    "                'num_chunks': len(chunks),\n",
    "                'avg_chunk_length': np.mean([len(chunk.split()) for chunk in chunks]),\n",
    "                'embedding_variance': np.var(embeddings.flatten()),\n",
    "                'memory_usage': embeddings.nbytes / (1024 * 1024)  # MB\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Performance analysis\n",
    "analyzer = PerformanceAnalyzer(retriever)\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"machine learning algorithms\",\n",
    "    \"neural networks\",\n",
    "    \"computer vision\",\n",
    "    \"natural language processing\"\n",
    "]\n",
    "\n",
    "# Benchmark retrieval methods\n",
    "benchmark_results = analyzer.benchmark_retrieval_methods(test_queries)\n",
    "\n",
    "print(\"=== Retrieval Method Performance ===\")\n",
    "for method, data in benchmark_results.items():\n",
    "    avg_time = np.mean(data['times'])\n",
    "    std_time = np.std(data['times'])\n",
    "    print(f\"{method.capitalize()} Retrieval:\")\n",
    "    print(f\"  Average time: {avg_time:.4f}s (Â±{std_time:.4f}s)\")\n",
    "    print()\n",
    "\n",
    "# Visualize performance\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "methods = list(benchmark_results.keys())\n",
    "avg_times = [np.mean(benchmark_results[method]['times']) for method in methods]\n",
    "plt.bar(methods, avg_times)\n",
    "plt.title('Average Retrieval Time by Method')\n",
    "plt.ylabel('Time (seconds)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "chunk_analysis = analyzer.analyze_chunk_size_impact(\" \".join(documents), [10, 20, 30, 50])\n",
    "sizes = list(chunk_analysis.keys())\n",
    "num_chunks = [chunk_analysis[size]['num_chunks'] for size in sizes]\n",
    "plt.plot(sizes, num_chunks, marker='o')\n",
    "plt.title('Number of Chunks vs Chunk Size')\n",
    "plt.xlabel('Chunk Size (words)')\n",
    "plt.ylabel('Number of Chunks')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Chunk Size Analysis ===\")\n",
    "for size, metrics in chunk_analysis.items():\n",
    "    print(f\"Chunk size {size} words:\")\n",
    "    print(f\"  Number of chunks: {metrics['num_chunks']}\")\n",
    "    print(f\"  Average chunk length: {metrics['avg_chunk_length']:.1f} words\")\n",
    "    print(f\"  Memory usage: {metrics['memory_usage']:.2f} MB\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Complete RAG Pipeline Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAGPipeline:\n",
    "    \"\"\"Complete RAG pipeline implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, documents: List[str], embedding_model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.documents = documents\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        \n",
    "        # Process documents\n",
    "        self.embeddings = self.embedding_model.encode(documents)\n",
    "        \n",
    "        # Create vector database\n",
    "        self.vector_db = SimpleVectorDB(self.embeddings.shape[1])\n",
    "        self.vector_db.add_documents(self.embeddings, documents)\n",
    "        \n",
    "        # Create retriever\n",
    "        self.retriever = RAGRetriever(self.vector_db, self.embedding_model, documents)\n",
    "        \n",
    "    def query(self, question: str, k: int = 3, method: str = 'hybrid') -> dict:\n",
    "        \"\"\"Process a query through the complete RAG pipeline\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Retrieve relevant documents\n",
    "        if method == 'dense':\n",
    "            results = self.retriever.dense_retrieval(question, k)\n",
    "        elif method == 'sparse':\n",
    "            results = self.retriever.sparse_retrieval(question, k)\n",
    "        else:  # hybrid\n",
    "            results = self.retriever.hybrid_retrieval(question, k)\n",
    "        \n",
    "        retrieval_time = time.time() - start_time\n",
    "        \n",
    "        # Prepare context for generation\n",
    "        context = \"\\n\".join([doc for _, doc in results])\n",
    "        \n",
    "        # Simulate response generation (in real implementation, this would use an LLM)\n",
    "        prompt = f\"\"\"Based on the following context, answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Simulated response (replace with actual LLM call)\n",
    "        response = f\"Based on the retrieved documents, here's what I found about '{question}': {context[:200]}...\"\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'retrieved_documents': [doc for _, doc in results],\n",
    "            'retrieval_scores': [score for score, _ in results],\n",
    "            'context': context,\n",
    "            'response': response,\n",
    "            'retrieval_time': retrieval_time,\n",
    "            'total_time': total_time,\n",
    "            'method_used': method\n",
    "        }\n",
    "\n",
    "# Create and test complete RAG pipeline\n",
    "rag_pipeline = SimpleRAGPipeline(documents)\n",
    "\n",
    "# Test the pipeline\n",
    "test_questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How does deep learning work?\",\n",
    "    \"What are the types of learning in AI?\"\n",
    "]\n",
    "\n",
    "print(\"=== Complete RAG Pipeline Results ===\")\n",
    "for question in test_questions:\n",
    "    result = rag_pipeline.query(question, k=2, method='hybrid')\n",
    "    \n",
    "    print(f\"\\nQuestion: {result['question']}\")\n",
    "    print(f\"Method: {result['method_used']}\")\n",
    "    print(f\"Retrieval time: {result['retrieval_time']:.4f}s\")\n",
    "    print(f\"Total time: {result['total_time']:.4f}s\")\n",
    "    print(\"\\nRetrieved documents:\")\n",
    "    for i, (doc, score) in enumerate(zip(result['retrieved_documents'], result['retrieval_scores']), 1):\n",
    "        print(f\"  {i}. (Score: {score:.4f}) {doc}\")\n",
    "    print(f\"\\nResponse: {result['response'][:150]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of RAG Concepts Demonstrated:\n",
    "\n",
    "1. **Document Processing**: Different chunking strategies affect retrieval quality\n",
    "2. **Embeddings**: Vector representations capture semantic meaning\n",
    "3. **Distance Metrics**: Cosine similarity is most common for text similarity\n",
    "4. **Vector Databases**: Enable efficient similarity search at scale\n",
    "5. **Retrieval Methods**: Dense, sparse, and hybrid approaches each have strengths\n",
    "6. **Evaluation**: Multiple metrics needed to assess system performance\n",
    "7. **Performance**: Trade-offs between accuracy, speed, and resource usage\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- **Chunk Size**: Balance between context preservation and precision\n",
    "- **Embedding Models**: Choose based on domain and performance requirements\n",
    "- **Retrieval Strategy**: Hybrid approaches often perform best\n",
    "- **Evaluation**: Use multiple metrics and real-world testing\n",
    "- **Optimization**: Monitor latency, throughput, and resource usage\n",
    "- **Continuous Improvement**: Implement feedback loops for system enhancement\n",
    "\n",
    "### Mathematical Foundations:\n",
    "\n",
    "- Vector mathematics enables semantic search\n",
    "- Distance metrics quantify similarity\n",
    "- Optimization algorithms improve performance\n",
    "- Statistical measures ensure quality\n",
    "\n",
    "This notebook demonstrates how all RAG components work together to create an effective information retrieval and generation system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
